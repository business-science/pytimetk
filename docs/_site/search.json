[
  {
    "objectID": "tutorials/05_clustering.html",
    "href": "tutorials/05_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "tutorials/03_demand_forecasting.html",
    "href": "tutorials/03_demand_forecasting.html",
    "title": "Demand Forecasting",
    "section": "",
    "text": "Timetk enables you to generate features from the time column of your data very easily. This tutorial showcases how easy it is to perform time series forecasting with pytimetk. The specific methods we will be using are:\n\ntk.augment_timeseries_signature(): Add 29 time series features to a DataFrame.\ntk.plot_timeseries(): Creates time series plots using different plotting engines such as Plotnine, Matplotlib, and Plotly.\ntk.future_frame(): Extend a DataFrame or GroupBy object with future dates.\n\nLoad the following packages before proceeding with this tutorial.\n\n\nCode\nimport pandas as pd\nimport pytimetk as tk\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\n\n\nThe tutorial is divided into three parts: We will first have a look at the Walmart dataset and perform some preprocessing. Secondly, we will create models based on different features, and see how the time features can be useful. Finally, we will solve the task of time series forecasting, using the features from augment_timeseries_signature only, to predict future sales.\n\n1 Preprocessing the dataset\nThe first thing we want to do is to load the dataset. It is a subset of the Walmart sales prediction Kaggle competition. You can get more insights about the dataset by following this link: walmart_sales_weekly. The most important thing to know about the dataset is that you are provided with some features like the fuel price or whether the week contains holidays and you are expected to predict the weekly sales column for 7 different departments of a given store. Of course, you also have the date for each week, and that is what we can leverage to create additional features.\nLet us start by loading the dataset and cleaning it. Note that we also remove markdown columns as they are not very useful for the tutorial.\n\n\nCode\n# We start by loading the dataset\n# /walmart_sales_weekly.html\ndset = tk.load_dataset('walmart_sales_weekly', parse_dates = ['Date'])\n\ndset = dset.drop(columns=[\n    'id', # This column can be removed as it is equivalent to 'Dept'\n    'Store', # This column has only one possible value\n    'Type', # This column has only one possible value\n    'Size', # This column has only one possible value\n    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'])\n\ndset.head()\n\n\n\n\n\n\n\n\n\nDept\nDate\nWeekly_Sales\nIsHoliday\nTemperature\nFuel_Price\nCPI\nUnemployment\n\n\n\n\n0\n1\n2010-02-05\n24924.50\nFalse\n42.31\n2.572\n211.096358\n8.106\n\n\n1\n1\n2010-02-12\n46039.49\nTrue\n38.51\n2.548\n211.242170\n8.106\n\n\n2\n1\n2010-02-19\n41595.55\nFalse\n39.93\n2.514\n211.289143\n8.106\n\n\n3\n1\n2010-02-26\n19403.54\nFalse\n46.63\n2.561\n211.319643\n8.106\n\n\n4\n1\n2010-03-05\n21827.90\nFalse\n46.50\n2.625\n211.350143\n8.106\n\n\n\n\n\n\n\nWe can plot the values of one department to get an idea of how the data looks like, using the plot_timeseries method:\n\n\nCode\nsales_df = dset\nfig = sales_df[sales_df['Dept']==1].plot_timeseries(\n    date_column='Date',\n    value_column='Weekly_Sales',\n    facet_ncol = 1,\n    x_axis_date_labels = \"%Y\",\n    engine = 'plotly')\nfig\n\n\n\n                                                \n\n\nLet us now reshape the DataFrame so that it has one column per Dept. This DataFrame represents our target variable y. We call it Y as it is a matrix, a stack of target variables, one for each Dept.\n\n\nCode\nY = sales_df[['Dept', 'Date', 'Weekly_Sales']].set_index(['Dept', 'Date']).sort_index().unstack(['Dept'])\nY.head()\n\n\n\n\n\n\n\n\n\nWeekly_Sales\n\n\nDept\n1\n3\n8\n13\n38\n93\n95\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2010-02-05\n24924.50\n13740.12\n40129.01\n41969.29\n115564.35\n64494.87\n106690.06\n\n\n2010-02-12\n46039.49\n10887.84\n37334.83\n36476.40\n94136.35\n70202.02\n111390.36\n\n\n2010-02-19\n41595.55\n11523.47\n38717.60\n37857.68\n98672.59\n62581.64\n107952.07\n\n\n2010-02-26\n19403.54\n11135.17\n35318.20\n37467.32\n92755.59\n57630.02\n103652.58\n\n\n2010-03-05\n21827.90\n12275.58\n38776.09\n40423.95\n108282.86\n63550.08\n112807.75\n\n\n\n\n\n\n\nNow that we have our target, we want to produce the features that will help us predict the target. We will create two sets of features, to show the differences between time features and the original features provided with the dataset.\nX contains the features originally in the dataset:\n\n\nCode\nX = sales_df.drop_duplicates(subset=['Date']).drop(columns=['Dept', 'Date', 'Weekly_Sales'])\nX.head()\n\n\n\n\n\n\n\n\n\nIsHoliday\nTemperature\nFuel_Price\nCPI\nUnemployment\n\n\n\n\n0\nFalse\n42.31\n2.572\n211.096358\n8.106\n\n\n1\nTrue\n38.51\n2.548\n211.242170\n8.106\n\n\n2\nFalse\n39.93\n2.514\n211.289143\n8.106\n\n\n3\nFalse\n46.63\n2.561\n211.319643\n8.106\n\n\n4\nFalse\n46.50\n2.625\n211.350143\n8.106\n\n\n\n\n\n\n\nX_time contains the time features. To build it we first apply the augment_timeseries_signature method on the Date column. Then, we create dummy variables from the categorical features that have been created, so that they can be fed into a machine learning algorithm.\n\n\nCode\ndef dummify(X_time: pd.DataFrame, date_col = 'Date'):\n  \"\"\" Creates dummy variables from the categorical date features that have been created. \"\"\"\n\n  X_time = pd.get_dummies(X_time, columns=[\n      f'{date_col}_year',\n      f'{date_col}_year_iso',\n      f'{date_col}_quarteryear',\n      f'{date_col}_month_lbl',\n      f'{date_col}_wday_lbl',\n      f'{date_col}_am_pm'], drop_first=True)\n  return X_time\n\ndate_col = 'Date'\nX_time = sales_df[['Date']].drop_duplicates(subset=[date_col]).augment_timeseries_signature(date_column = date_col).drop(columns=[date_col])\nX_time = dummify(X_time, date_col=date_col)\nX_time\n\n\n\n\n\n\n\n\n\nDate_index_num\nDate_yearstart\nDate_yearend\nDate_leapyear\nDate_half\nDate_quarter\nDate_quarterstart\nDate_quarterend\nDate_month\nDate_monthstart\n...\nDate_month_lbl_December\nDate_month_lbl_February\nDate_month_lbl_January\nDate_month_lbl_July\nDate_month_lbl_June\nDate_month_lbl_March\nDate_month_lbl_May\nDate_month_lbl_November\nDate_month_lbl_October\nDate_month_lbl_September\n\n\n\n\n0\n1265328000\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n1265932800\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n1266537600\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n1267142400\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n1267747200\n0\n0\n0\n1\n1\n0\n0\n3\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n138\n1348790400\n0\n0\n1\n2\n3\n0\n0\n9\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n139\n1349395200\n0\n0\n1\n2\n4\n0\n0\n10\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n140\n1350000000\n0\n0\n1\n2\n4\n0\n0\n10\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n141\n1350604800\n0\n0\n1\n2\n4\n0\n0\n10\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n142\n1351209600\n0\n0\n1\n2\n4\n0\n0\n10\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n143 rows × 49 columns\n\n\n\nLet us explain a little bit more what happened here. We select only the Date column with\n\n\nCode\nsales_df[['Date']]\n\n\n\n\n\n\n\n\n\nDate\n\n\n\n\n0\n2010-02-05\n\n\n1\n2010-02-12\n\n\n2\n2010-02-19\n\n\n3\n2010-02-26\n\n\n4\n2010-03-05\n\n\n...\n...\n\n\n996\n2012-09-28\n\n\n997\n2012-10-05\n\n\n998\n2012-10-12\n\n\n999\n2012-10-19\n\n\n1000\n2012-10-26\n\n\n\n\n1001 rows × 1 columns\n\n\n\nWe then drop the duplicates, as the Date column contains all the dates 7 times (one for each Dept):\n\n\nCode\nsales_df.drop_duplicates(subset=[date_col])\n\n\n\n\n\n\n\n\n\nDept\nDate\nWeekly_Sales\nIsHoliday\nTemperature\nFuel_Price\nCPI\nUnemployment\n\n\n\n\n0\n1\n2010-02-05\n24924.50\nFalse\n42.31\n2.572\n211.096358\n8.106\n\n\n1\n1\n2010-02-12\n46039.49\nTrue\n38.51\n2.548\n211.242170\n8.106\n\n\n2\n1\n2010-02-19\n41595.55\nFalse\n39.93\n2.514\n211.289143\n8.106\n\n\n3\n1\n2010-02-26\n19403.54\nFalse\n46.63\n2.561\n211.319643\n8.106\n\n\n4\n1\n2010-03-05\n21827.90\nFalse\n46.50\n2.625\n211.350143\n8.106\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n138\n1\n2012-09-28\n18947.81\nFalse\n76.08\n3.666\n222.981658\n6.908\n\n\n139\n1\n2012-10-05\n21904.47\nFalse\n68.55\n3.617\n223.181477\n6.573\n\n\n140\n1\n2012-10-12\n22764.01\nFalse\n62.99\n3.601\n223.381296\n6.573\n\n\n141\n1\n2012-10-19\n24185.27\nFalse\n67.97\n3.594\n223.425723\n6.573\n\n\n142\n1\n2012-10-26\n27390.81\nFalse\n69.16\n3.506\n223.444251\n6.573\n\n\n\n\n143 rows × 8 columns\n\n\n\nWe can now augment the data using tk.augment_timeseries_signature, and drop the original Date column:\n\n\nCode\nsales_df.augment_timeseries_signature(date_column = date_col).drop(columns=[date_col])\n\n\n\n\n\n\n\n\n\nDept\nWeekly_Sales\nIsHoliday\nTemperature\nFuel_Price\nCPI\nUnemployment\nDate_index_num\nDate_year\nDate_year_iso\n...\nDate_mday\nDate_qday\nDate_yday\nDate_weekend\nDate_hour\nDate_minute\nDate_second\nDate_msecond\nDate_nsecond\nDate_am_pm\n\n\n\n\n0\n1\n24924.50\nFalse\n42.31\n2.572\n211.096358\n8.106\n1265328000\n2010\n2010\n...\n5\n36\n36\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1\n46039.49\nTrue\n38.51\n2.548\n211.242170\n8.106\n1265932800\n2010\n2010\n...\n12\n43\n43\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1\n41595.55\nFalse\n39.93\n2.514\n211.289143\n8.106\n1266537600\n2010\n2010\n...\n19\n50\n50\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1\n19403.54\nFalse\n46.63\n2.561\n211.319643\n8.106\n1267142400\n2010\n2010\n...\n26\n57\n57\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1\n21827.90\nFalse\n46.50\n2.625\n211.350143\n8.106\n1267747200\n2010\n2010\n...\n5\n64\n64\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n996\n95\n115326.47\nFalse\n76.08\n3.666\n222.981658\n6.908\n1348790400\n2012\n2012\n...\n28\n90\n272\n0\n0\n0\n0\n0\n0\nam\n\n\n997\n95\n127009.22\nFalse\n68.55\n3.617\n223.181477\n6.573\n1349395200\n2012\n2012\n...\n5\n5\n279\n0\n0\n0\n0\n0\n0\nam\n\n\n998\n95\n124559.93\nFalse\n62.99\n3.601\n223.381296\n6.573\n1350000000\n2012\n2012\n...\n12\n12\n286\n0\n0\n0\n0\n0\n0\nam\n\n\n999\n95\n123346.24\nFalse\n67.97\n3.594\n223.425723\n6.573\n1350604800\n2012\n2012\n...\n19\n19\n293\n0\n0\n0\n0\n0\n0\nam\n\n\n1000\n95\n117375.38\nFalse\n69.16\n3.506\n223.444251\n6.573\n1351209600\n2012\n2012\n...\n26\n26\n300\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n1001 rows × 36 columns\n\n\n\n\n\n2 Modeling\nSo far, we defined our target variables Y, and two different sets of features: X and X_time. We can now train a sales forecasting model. For this tutorial, we will be using the RandomForestsRegressor, as it is a simple yet powerful model, that can handle multiple types of data. We build a train function that takes the features and the targets as input, and is composed of several steps:\n\nWe divide the data into a train set and a test set. train_size is the percentage of the data that you want to keep for the train set, the rest will be used for the test set.\nWe scale numerical features if any, so that the model learns better. The RobustScaler allows for better performances, as it scales the data using statistics that are robust to outliers.\nWe added the k option so that we can select the k best features of our dataset using mutual information, hence reducing the noise of irrelevant features.\nWe train and test the RandomForests model, measuring its performance with the R2 score function.\n\nThe resulting training function is as follows:\n\n\nCode\ndef train(X, Y, k=None):\n  \"\"\" Trains a RandomForests model on the input data. \"\"\"\n\n  Y = Y.fillna(method='ffill').fillna(method='bfill')\n  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, shuffle=False, train_size=.5)\n\n  # scale numerical features\n  features_to_scale = [ c for c in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment'] if c in X.columns]\n  if len(features_to_scale):\n    scaler = RobustScaler()\n    X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n    X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n\n  # select best features to remove noise\n  if k is not None:\n    selector = SelectKBest(mutual_info_regression, k=k)\n    X_train = selector.fit_transform(X_train, Y_train.iloc[:,1])\n    X_test = selector.transform(X_test)\n\n  # train the model\n  model = RandomForestRegressor(random_state=123456, n_estimators=300)\n  model = model.fit(X_train, Y_train)\n  preds_train = model.predict(X_train)\n\n  # test the model\n  preds_test = model.predict(X_test)\n  print(f'R2 score: {r2_score(Y_test, preds_test)}')\n\n  return Y_train, Y_test, preds_train, preds_test  # returns data useful for the plot_result function below\n\n\nIn addition, we define a plot function based on tk.plot_timeseries that will enable us to compare the ground truth data to the model’s predictions, for a given department.\n\n\nCode\ndef plot_result(dept_idx, Y, Y_train, Y_test, preds_train, preds_test):\n  \"\"\" Plots the predictions for a given Department. \"\"\"\n  import numpy as np\n\n  data = pd.DataFrame({\n      'Weekly_Sales': pd.concat([\n          Y.iloc[:, dept_idx],\n          pd.Series(preds_train[:,dept_idx], index=Y_train.index),\n          pd.Series(preds_test[:,dept_idx], index=Y_test.index)])\n  })\n  data['Labels'] = \"\"\n  data['Labels'].iloc[:len(Y)] = 'Ground truth'\n  data['Labels'].iloc[len(Y):len(Y)+len(Y_train)] = 'Predictions on train set'\n  data['Labels'].iloc[len(Y)+len(Y_train):] = 'Predictions on test set'\n\n  fig = data.reset_index().plot_timeseries(\n    date_column='Date',\n    value_column='Weekly_Sales',\n    color_column='Labels',\n    facet_ncol = 1,\n    smooth=False,\n    x_axis_date_labels = \"%Y\",\n    engine = 'plotly')\n  fig.show()\n\n\nIf we train using the X matrix of features (the features of the dataset) we get a poor result with an R2 score of -0.16.\n\n\nCode\nY_train, Y_test, preds_train, preds_test = train(X, Y)\nplot_result(1, Y, Y_train, Y_test, preds_train, preds_test)  # We inspect the predictions for the first department\n\n\nR2 score: -0.16021915207983892\n\n\n\n                                                \n\n\nComputing the mutual information score on these features, we realize that only two features are really useful. However, using them alone does not improve the results.\n\n\nCode\nfrom sklearn.feature_selection import mutual_info_regression\n\ndef compute_MI(X, Y):\n  Y = Y.fillna(method='ffill').fillna(method='bfill')\n  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, shuffle=False, train_size=.5)\n  features_to_scale = [ c for c in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment'] if c in X.columns]\n  scaler = RobustScaler()\n  X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n\n  print(pd.DataFrame({'feature': X_train.columns, 'MI': mutual_info_regression(X_train, Y_train.iloc[:,0])}).sort_values(by='MI', ascending=False))\n\ncompute_MI(X, Y)\n\n\n        feature        MI\n1   Temperature  0.277781\n4  Unemployment  0.213103\n2    Fuel_Price  0.080510\n3           CPI  0.004586\n0     IsHoliday  0.004462\n\n\nNow if we create a model using our date-based features, the results are a lot better, with an R2 score of 0.23!\n\n\nCode\nY_train, Y_test, preds_train, preds_test = train(X_time, Y)\nplot_result(1, Y, Y_train, Y_test, preds_train, preds_test)\n\n\nR2 score: 0.23031249715453514\n\n\n\n                                                \n\n\nConcatenating all the features together, we can get a little bit of improvement, but not a significant one (R2 score: 0.239).\n\n\nCode\nX_concat = pd.concat([X_time, X[['Temperature', 'Unemployment']]], axis=1)\nY_train, Y_test, preds_train, preds_test = train(X_concat, Y, k=30)\nplot_result(1, Y, Y_train, Y_test, preds_train, preds_test)\n\n\nR2 score: 0.23974542401432444\n\n\n\n                                                \n\n\nThis section showed the relevance of the time features in time series forecasting. Now let us build our final forecasting engine, using only the time-based features.\n\n\n3 Forecasting\nWe want to use all of our data to create the final model. To that aim, we will use a very simplified version of our previous training function:\n\n\nCode\ndef train_final(X, Y):\n  \"\"\" Trains a RandomForests model on the input data. \"\"\"\n\n  Y = Y.fillna(method='ffill').fillna(method='bfill')\n  X_train, Y_train = X, Y\n  model = RandomForestRegressor(random_state=123456, n_estimators=300)\n  model = model.fit(X_train, Y_train)\n\n  return model\n\n\nTo perform the forecasting, we need time-based features for dates that are not in our dataset. Let us use the tk.future_frame method to add future dates to our dataset. We then apply the augment_timeseries_signature method on the resulting DataFrame, hence creating the time-based features for past and future dates:\n\n\nCode\ndate_col= 'Date'\n\nX_time_future = sales_df.drop_duplicates(subset=['Date'])[['Date']]\nX_time_future = X_time_future.future_frame(\n    date_column = date_col,\n    length_out  = 60\n).augment_timeseries_signature(date_column = date_col).drop(columns=[date_col])\nX_time_future = dummify(X_time_future, date_col='Date')\n\nX_time_future.head()\n\n\n\n\n\n\n\n\n\nDate_index_num\nDate_yearstart\nDate_yearend\nDate_leapyear\nDate_half\nDate_quarter\nDate_quarterstart\nDate_quarterend\nDate_month\nDate_monthstart\n...\nDate_month_lbl_December\nDate_month_lbl_February\nDate_month_lbl_January\nDate_month_lbl_July\nDate_month_lbl_June\nDate_month_lbl_March\nDate_month_lbl_May\nDate_month_lbl_November\nDate_month_lbl_October\nDate_month_lbl_September\n\n\n\n\n0\n1265328000\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n1265932800\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n1266537600\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n1267142400\n0\n0\n0\n1\n1\n0\n0\n2\n0\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n1267747200\n0\n0\n0\n1\n1\n0\n0\n3\n0\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 55 columns\n\n\n\nIn the same way, we augment our target DataFrame by 60 weeks. A we don’t know the sales in the future, the additional rows will be filled with nans. We will replace the nans with our predictions in following the tutorial.\n\n\nCode\nY_future = sales_df[['Dept', 'Date', 'Weekly_Sales']]\nY_future = Y_future.groupby('Dept').future_frame(\n    date_column = date_col,\n    length_out  = 60\n)\nY_future.head()\n\n\n\n\n\n\n\n\n\nDept\nDate\nWeekly_Sales\n\n\n\n\n0\n1\n2010-02-05\n24924.50\n\n\n1\n1\n2010-02-12\n46039.49\n\n\n2\n1\n2010-02-19\n41595.55\n\n\n3\n1\n2010-02-26\n19403.54\n\n\n4\n1\n2010-03-05\n21827.90\n\n\n\n\n\n\n\nWe train the model and store its predictions on the future dates:\n\n\nCode\nmodel = train_final(X_time_future.iloc[:len(Y)], Y)\npredictions = model.predict(X_time_future.iloc[len(Y):])\n\n\nWe store the predictions in the Y DataFrame, and tag the prediction entries with the label ‘Predictions’. Original entries are tagged with the label History.\n\n\nCode\nY_future.loc[Y_future.Date &gt; Y.index[-1], 'Weekly_Sales'] = predictions.T.ravel()\nY_future['Label'] = 'History'\nY_future.loc[Y_future.Date &gt; Y.index[-1], 'Label'] = 'Predictions'\n\n\nWe can now plot the result very easily using the plot_timeseries method. Note how you can easily select a subset of the data (the data of a given department) using the query method:\n\n\nCode\nY_future.query(\"Dept == 1\").plot_timeseries(date_column='Date', value_column='Weekly_Sales', color_column='Label', smooth=False)\n\n\n\n                                                \n\n\n\n\n4 Conclusion\nIn this tutorial, we showed how the tk.augment_timeseries_signature() function can be used to effortlessly extract useful features from the date index and perform forecasting."
  },
  {
    "objectID": "tutorials/01_sales_crm.html",
    "href": "tutorials/01_sales_crm.html",
    "title": "Sales Customer Analysis",
    "section": "",
    "text": "In this tutorial, we will use pytimetk and its powerful functions to perform a time series analysis on a dataset representing bike sales. Our goal is to understand the patterns in the data and forecast future sales. You will:"
  },
  {
    "objectID": "tutorials/01_sales_crm.html#load-packages.",
    "href": "tutorials/01_sales_crm.html#load-packages.",
    "title": "Sales Customer Analysis",
    "section": "1.1 Load Packages.",
    "text": "1.1 Load Packages.\nIf you do not have pytimetk installed, you can install by using\npip install pytimetk\nor for the latest features and functionality, you can install the development version.\npip install git+https://github.com/business-science/pytimetk.git\n\n\nCode\nimport pytimetk as tk\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "tutorials/01_sales_crm.html#load-inspect-dataset",
    "href": "tutorials/01_sales_crm.html#load-inspect-dataset",
    "title": "Sales Customer Analysis",
    "section": "1.2 Load & inspect dataset",
    "text": "1.2 Load & inspect dataset\nTo kick off our analysis, we’ll begin by importing essential libraries and accessing the ‘bike_sales’ dataset available within pytimetk’s suite of built-in datasets.\nThe Bike Sales dataset exemplifies what one might find in a CRM (Customer Relationship Management) system. CRM systems are pivotal for businesses, offering vital insights by tracking sales throughout the entire sales funnel. Such datasets are rich with transaction-level data, encompassing elements like order numbers, individual order lines, customer details, product information, and specific transaction data.\nTransactional data, such as this, inherently holds the essential components for time series analysis:\n\nTime Stamps\nAssociated Values\nDistinct Groups or Categories\n\nGiven these attributes, the Bike Sales dataset emerges as an ideal candidate for analysis using pytimetk."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#preliminary-data-exploration",
    "href": "tutorials/01_sales_crm.html#preliminary-data-exploration",
    "title": "Sales CRM Database Analysis",
    "section": "0.3 Preliminary Data Exploration",
    "text": "0.3 Preliminary Data Exploration\nCRM data is often bustling with activity, reflecting the myriad of transactions happening daily. Due to this high volume, the data can sometimes seem overwhelming or noisy. To derive meaningful insights, it’s essential to aggregate this data over specific time intervals. This is where tk.summarize_by_time() comes into play.\nThe tk.summarize_by_time() function offers a streamlined approach to time-based data aggregation. By defining a desired frequency and an aggregation method, this function seamlessly organizes your data. The beauty of it is its versatility; from a broad array of built-in aggregation methods and frequencies to the flexibility of integrating a custom function, it caters to a range of requirements.\nCurious about the various options it provides? Dive into its documentation with:\nhelp(tk.summarize_by_time)\nAnd explore the plethora of possibilities!\n\n\nCode\nweekly_totals = df.summarize_by_time(\n    date_column  = 'order_date',\n    value_column = 'total_price',\n    agg_func     = ['sum'],\n    freq         = 'W'\n)\n\nweekly_totals.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\n\n\n\n\n0\n2011-01-09\n12040\n\n\n1\n2011-01-16\n151460\n\n\n2\n2011-01-23\n143850\n\n\n3\n2011-01-30\n175665\n\n\n4\n2011-02-06\n105210\n\n\n5\n2011-02-13\n250390\n\n\n6\n2011-02-20\n410595\n\n\n7\n2011-02-27\n254045\n\n\n8\n2011-03-06\n308420\n\n\n9\n2011-03-13\n45450\n\n\n\n\n\n\n\nTo better understand your data, you might want to add groups to this summary. We can include a groupby before the summarize_by_time and then aggregate our data.\n\n\nCode\n sales_by_week = df \\\n    .groupby('category_2') \\\n    .summarize_by_time(\n        date_column = 'order_date',\n        value_column = 'total_price',\n        agg_func = ['sum'],\n        freq = 'W'\n    )\n\nsales_by_week.head(10)\n\n\n\n\n\n\n\n\n\ncategory_2\norder_date\ntotal_price_sum\n\n\n\n\n0\nCross Country Race\n2011-01-16\n61750\n\n\n1\nCross Country Race\n2011-01-23\n25050\n\n\n2\nCross Country Race\n2011-01-30\n56860\n\n\n3\nCross Country Race\n2011-02-06\n8740\n\n\n4\nCross Country Race\n2011-02-13\n78070\n\n\n5\nCross Country Race\n2011-02-20\n115010\n\n\n6\nCross Country Race\n2011-02-27\n64290\n\n\n7\nCross Country Race\n2011-03-06\n95070\n\n\n8\nCross Country Race\n2011-03-13\n3200\n\n\n9\nCross Country Race\n2011-03-20\n21170\n\n\n\n\n\n\n\nThis long format can make it a little hard to compare the different group values visually, so instead of long-format you might want to pivot wide to view the data.\n\n\nCode\nsales_by_week_wide = df \\\n    .groupby('category_2') \\\n    .summarize_by_time(\n        date_column = 'order_date',\n        value_column = 'total_price',\n        agg_func = ['sum'],\n        freq = 'W',\n        wide_format = True\n    )\n\nsales_by_week_wide.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum_Cross Country Race\ntotal_price_sum_Cyclocross\ntotal_price_sum_Elite Road\ntotal_price_sum_Endurance Road\ntotal_price_sum_Fat Bike\ntotal_price_sum_Over Mountain\ntotal_price_sum_Sport\ntotal_price_sum_Trail\ntotal_price_sum_Triathalon\n\n\n\n\n0\n2011-01-09\n0.0\n0.0\n0.0\n0.0\n0.0\n12040.0\n0.0\n0.0\n0.0\n\n\n1\n2011-01-16\n61750.0\n1960.0\n49540.0\n11110.0\n0.0\n9170.0\n4030.0\n7450.0\n6450.0\n\n\n2\n2011-01-23\n25050.0\n3500.0\n51330.0\n47930.0\n0.0\n3840.0\n0.0\n0.0\n12200.0\n\n\n3\n2011-01-30\n56860.0\n2450.0\n43895.0\n24160.0\n0.0\n10880.0\n3720.0\n26700.0\n7000.0\n\n\n4\n2011-02-06\n8740.0\n7000.0\n35640.0\n22680.0\n3730.0\n14270.0\n980.0\n10220.0\n1950.0\n\n\n5\n2011-02-13\n78070.0\n0.0\n83780.0\n24820.0\n2130.0\n17160.0\n6810.0\n17120.0\n20500.0\n\n\n6\n2011-02-20\n115010.0\n7910.0\n79770.0\n27650.0\n26100.0\n37830.0\n10925.0\n96250.0\n9150.0\n\n\n7\n2011-02-27\n64290.0\n6650.0\n86900.0\n31900.0\n5860.0\n22070.0\n6165.0\n16410.0\n13800.0\n\n\n8\n2011-03-06\n95070.0\n2450.0\n31990.0\n47660.0\n5860.0\n82060.0\n9340.0\n26790.0\n7200.0\n\n\n9\n2011-03-13\n3200.0\n4200.0\n23110.0\n7260.0\n0.0\n5970.0\n1710.0\n0.0\n0.0\n\n\n\n\n\n\n\nYou can now observe the total sales for each product side by side. This streamlined view facilitates easy comparison between product sales."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#plot-your-data",
    "href": "tutorials/01_sales_crm.html#plot-your-data",
    "title": "Sales CRM Database Analysis",
    "section": "2.2 Plot your data",
    "text": "2.2 Plot your data\nYou can now visualize the summarized data to gain a clearer insight into the prevailing trends.\n\nPlotlyPlotnine\n\n\n\n\nCode\nsales_by_week \\\n    .groupby('category_2') \\\n    .plot_timeseries(\n        date_column = 'order_date',  \n        value_column = 'total_price_sum',\n        title = 'Bike Sales by Category',\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        y_lab = 'Total Sales', \n        engine = 'plotly'\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nsales_by_week \\\n    .groupby('category_2') \\\n    .plot_timeseries(\n        date_column = 'order_date',  \n        value_column = 'total_price_sum',\n        title = 'Bike Sales by Category',\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        line_size = 0.35,\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        y_lab = 'Total Sales', \n        engine = 'plotnine'\n    )\n\n\n\n\n\n&lt;Figure Size: (1000 x 800)&gt;\n\n\n\n\n\nThe graph showcases a pronounced uptick in sales for most of the different bike products during the summer. It’s a natural trend, aligning with our understanding that people gravitate towards biking during the balmy summer days. Conversely, as the chill of winter sets in at the year’s start and end, we observe a corresponding dip in sales.\nIt’s worth highlighting the elegance of the plot_timeseries function. Beyond just plotting raw data, it introduces a smoother, accentuating underlying trends and making them more discernible. This enhancement ensures we can effortlessly capture and comprehend the cyclical nature of bike sales throughout the year."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#create-forecast-model",
    "href": "tutorials/01_sales_crm.html#create-forecast-model",
    "title": "Sales CRM Database Analysis",
    "section": "2.3 Create Forecast Model",
    "text": "2.3 Create Forecast Model\nForecasting future sales for bikes requires meticulous data preparation, and pytimetk streamlines this process for us. When crafting a Machine Learning model tailored for time series analysis, there are several pivotal steps to follow:\n\nTime Padding for Comprehensive Historical Data: It’s essential to ensure that our data captures every week, even those that witnessed zero sales. This padding becomes especially pertinent when integrating time series features, like lags.\nCrafting the Future Frame: This step involves setting up a structure that accommodates the test data features, which will eventually be fed into our prediction function.\nInfusing Time Series Lag Features: These features are critical for capturing patterns in time series data, and they need to be integrated into our future frame.\nFeature / Date Augmentation: This step can involve adding contextual features that might influence sales, such as date features, holidays, promotional events, etc.\nModel Training: Once the data is prepped, it’s time to train our Machine Learning model, refining it to understand historical patterns and predict future trends.\nMaking Predictions: After training, the model is ready to forecast sales for future periods based on the features of the new data.\n\nKicking off our journey, we’ll utilize pytimetk’s tk.pad_by_time() function. For this, grouping by the ‘category_1’ variable is recommended. Moreover, it’s prudent to establish a definitive end date. This ensures that all groups are equipped with training data up to the most recent date, accommodating scenarios where certain categories might have seen no sales in the final training week. By doing so, we create a representative observation for every group, capturing the nuances of each category’s sales pattern.\n\n5.1 Time Padding\n\n\nCode\nsales_padded = sales_by_week \\\n    .groupby('category_2') \\\n    .pad_by_time(\n        date_column = 'order_date',\n        freq        = 'W',\n        end_date    = sales_by_week.order_date.max()\n    )\nsales_padded\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\n\n\n...\n...\n...\n...\n\n\n452\n2011-12-04\nTriathalon\n3200.0\n\n\n453\n2011-12-11\nTriathalon\n28350.0\n\n\n454\n2011-12-18\nTriathalon\n2700.0\n\n\n455\n2011-12-25\nTriathalon\n3900.0\n\n\n456\n2012-01-01\nTriathalon\nNaN\n\n\n\n\n457 rows × 3 columns\n\n\n\n\n\n5.2 Future Frame\nMoving on, let’s set up the future frame, which will serve as our test dataset. To achieve this, employ the tk.future_frame() method. This function allows for the specification of a grouping column and a forecast horizon.\nUpon invoking tk.future_frame(), you’ll observe that placeholders (null values) are added for each group, extending 12 weeks into the future.\n\n\nCode\ndf_with_futureframe = sales_padded \\\n    .groupby('category_2') \\\n    .future_frame(\n        date_column = 'order_date',\n        length_out  = 12\n    )\ndf_with_futureframe\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\n\n\n...\n...\n...\n...\n\n\n58\n2012-02-26\nTriathalon\nNaN\n\n\n59\n2012-03-04\nTriathalon\nNaN\n\n\n60\n2012-03-11\nTriathalon\nNaN\n\n\n61\n2012-03-18\nTriathalon\nNaN\n\n\n62\n2012-03-25\nTriathalon\nNaN\n\n\n\n\n565 rows × 3 columns\n\n\n\n\n\n5.3 Lag Values\nCrafting features from time series data can be intricate, but thanks to the suite of feature engineering tools in pytimetk, the process is streamlined and intuitive.\nIn this guide, we’ll focus on the basics: introducing a few lag variables and incorporating some date-related features.\nFirstly, let’s dive into creating lag features.\nGiven our forecasting objective of a 12-week horizon, to ensure we have lag data available for every future point, we should utilize a lag of 12 or more. The beauty of the toolkit is that it supports the addition of multiple lags simultaneously.\nLag features play a pivotal role in machine learning for time series. Often, recent data offers valuable insights into future trends. To capture this recency effect, it’s crucial to integrate lag values. For this purpose, tk.augment_lags() comes in handy.\n\n\nCode\ndf_with_lags = df_with_futureframe \\\n    .groupby('category_2') \\\n    .augment_lags(\n        date_column  = 'order_date',\n        value_column = 'total_price_sum',\n        lags         = [12,24]\n\n    )\ndf_with_lags.head(25)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\nNaN\nNaN\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\nNaN\nNaN\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\nNaN\nNaN\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\nNaN\nNaN\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\nNaN\nNaN\n\n\n5\n2011-02-20\nCross Country Race\n115010.0\nNaN\nNaN\n\n\n6\n2011-02-27\nCross Country Race\n64290.0\nNaN\nNaN\n\n\n7\n2011-03-06\nCross Country Race\n95070.0\nNaN\nNaN\n\n\n8\n2011-03-13\nCross Country Race\n3200.0\nNaN\nNaN\n\n\n9\n2011-03-20\nCross Country Race\n21170.0\nNaN\nNaN\n\n\n10\n2011-03-27\nCross Country Race\n28990.0\nNaN\nNaN\n\n\n11\n2011-04-03\nCross Country Race\n51860.0\nNaN\nNaN\n\n\n12\n2011-04-10\nCross Country Race\n85910.0\n61750.0\nNaN\n\n\n13\n2011-04-17\nCross Country Race\n138230.0\n25050.0\nNaN\n\n\n14\n2011-04-24\nCross Country Race\n138350.0\n56860.0\nNaN\n\n\n15\n2011-05-01\nCross Country Race\n136090.0\n8740.0\nNaN\n\n\n16\n2011-05-08\nCross Country Race\n32110.0\n78070.0\nNaN\n\n\n17\n2011-05-15\nCross Country Race\n139010.0\n115010.0\nNaN\n\n\n18\n2011-05-22\nCross Country Race\n2060.0\n64290.0\nNaN\n\n\n19\n2011-05-29\nCross Country Race\n26130.0\n95070.0\nNaN\n\n\n20\n2011-06-05\nCross Country Race\n30360.0\n3200.0\nNaN\n\n\n21\n2011-06-12\nCross Country Race\n88280.0\n21170.0\nNaN\n\n\n22\n2011-06-19\nCross Country Race\n109470.0\n28990.0\nNaN\n\n\n23\n2011-06-26\nCross Country Race\n107280.0\n51860.0\nNaN\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n\n\n\n\n\n\n\nObserve that lag values of 12 and 24 introduce missing entries at the dataset’s outset. This occurs because there isn’t available data from 12 or 24 weeks prior. To address these gaps, you can adopt one of two strategies:\n\nDiscard the Affected Rows: This is a recommended approach if your dataset is sufficiently large. Removing a few initial rows might not significantly impact the training process.\nBackfill Missing Values: In situations with limited data, you might consider backfilling these nulls using the first available values from lag 12 and 24. However, the appropriateness of this technique hinges on your specific context and objectives.\n\nFor the scope of this tutorial, we’ll opt to remove these rows. However, it’s worth pointing out that our dataset is actually quite small with limited historical data, so this might impact our model.\n\n\nCode\nlag_columns = [col for col in df_with_lags.columns if 'lag' in col]\ndf_no_nas = df_with_lags \\\n    .dropna(subset=lag_columns, inplace=False)\n\ndf_no_nas.head()\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n\n\n\n\n\n\n\n\n\n5.4 Date Features\nNow, let’s enrich our dataset with date-related features.\nWith the function tk.augment_timeseries_signature(), you can effortlessly append 29 date attributes to a timestamp. Given that our dataset captures weekly intervals, certain attributes like ‘hour’ may not be pertinent. Thus, it’s prudent to refine our columns, retaining only those that truly matter to our analysis.\n\n\nCode\ndf_with_datefeatures = df_no_nas \\\n    .augment_timeseries_signature(date_column='order_date')\n\ndf_with_datefeatures.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\n...\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n1309651200\n2011\n2011\n0\n0\n...\n3\n3\n184\n1\n0\n0\n0\n0\n0\nam\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n1310256000\n2011\n2011\n0\n0\n...\n10\n10\n191\n1\n0\n0\n0\n0\n0\nam\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n1310860800\n2011\n2011\n0\n0\n...\n17\n17\n198\n1\n0\n0\n0\n0\n0\nam\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n1311465600\n2011\n2011\n0\n0\n...\n24\n24\n205\n1\n0\n0\n0\n0\n0\nam\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n1312070400\n2011\n2011\n0\n0\n...\n31\n31\n212\n1\n0\n0\n0\n0\n0\nam\n\n\n29\n2011-08-07\nCross Country Race\n11620.0\n139010.0\n115010.0\n1312675200\n2011\n2011\n0\n0\n...\n7\n38\n219\n1\n0\n0\n0\n0\n0\nam\n\n\n30\n2011-08-14\nCross Country Race\n9730.0\n2060.0\n64290.0\n1313280000\n2011\n2011\n0\n0\n...\n14\n45\n226\n1\n0\n0\n0\n0\n0\nam\n\n\n31\n2011-08-21\nCross Country Race\n22780.0\n26130.0\n95070.0\n1313884800\n2011\n2011\n0\n0\n...\n21\n52\n233\n1\n0\n0\n0\n0\n0\nam\n\n\n32\n2011-08-28\nCross Country Race\n53680.0\n30360.0\n3200.0\n1314489600\n2011\n2011\n0\n0\n...\n28\n59\n240\n1\n0\n0\n0\n0\n0\nam\n\n\n33\n2011-09-04\nCross Country Race\n38360.0\n88280.0\n21170.0\n1315094400\n2011\n2011\n0\n0\n...\n4\n66\n247\n1\n0\n0\n0\n0\n0\nam\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\nCode\ndf_with_datefeatures.columns\n\n\nIndex(['order_date', 'category_2', 'total_price_sum', 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_index_num', 'order_date_year',\n       'order_date_year_iso', 'order_date_yearstart', 'order_date_yearend',\n       'order_date_leapyear', 'order_date_half', 'order_date_quarter',\n       'order_date_quarteryear', 'order_date_quarterstart',\n       'order_date_quarterend', 'order_date_month', 'order_date_month_lbl',\n       'order_date_monthstart', 'order_date_monthend', 'order_date_yweek',\n       'order_date_mweek', 'order_date_wday', 'order_date_wday_lbl',\n       'order_date_mday', 'order_date_qday', 'order_date_yday',\n       'order_date_weekend', 'order_date_hour', 'order_date_minute',\n       'order_date_second', 'order_date_msecond', 'order_date_nsecond',\n       'order_date_am_pm'],\n      dtype='object')\n\n\nLet’s subset to just a few of the relevant date features.\n\n\nCode\ndf_with_datefeatures_narrom = df_with_datefeatures[[\n    'order_date', \n    'category_2', \n    'total_price_sum',\n    'total_price_sum_lag_12',\n    'total_price_sum_lag_24',\n    'order_date_year',  \n    'order_date_half', \n    'order_date_quarter',      \n    'order_date_month',\n    'order_date_yweek'\n]]\n\ndf_with_datefeatures_narrom.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\n\n\n29\n2011-08-07\nCross Country Race\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\n\n\n30\n2011-08-14\nCross Country Race\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\n\n\n31\n2011-08-21\nCross Country Race\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\n\n\n32\n2011-08-28\nCross Country Race\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\n\n\n33\n2011-09-04\nCross Country Race\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\n\n\n\n\n\n\n\nThe final phase in our feature engineering journey is one-hot encoding our categorical variables. While certain machine learning models like CatBoost can natively handle categorical data, many cannot. Enter one-hot encoding, a technique that transforms each category within a column into its separate column, marking its presence with a ‘1’ or absence with a ‘0’.\nFor this transformation, the handy pd.get_dummies() function from pandas comes to the rescue.\n\n\nCode\ndf_encoded = pd.get_dummies(df_with_datefeatures_narrom, columns=['category_2'])\n\ndf_encoded.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nPytimetk offers an extensive array of feature engineering tools and augmentation functions, giving you a broad spectrum of possibilities. However, for the purposes of this tutorial, let’s shift our focus to modeling.\nLet’s proceed by segmenting our dataframe into training and future sets.\n\n\nCode\nfuture = df_encoded[df_encoded.total_price_sum.isnull()]\ntrain = df_encoded[df_encoded.total_price_sum.notnull()]\n\n\nLet’s focus on the columns essential for training. You’ll observe that we’ve excluded the ‘order_date’ column. This is because numerous machine learning models struggle with date data types. This is precisely why we utilized the tk.augment_timeseries_signature earlier—to transform date features into a format that’s compatible with ML models.\n\n\nCode\ntrain.columns\n\n\nIndex(['order_date', 'total_price_sum', 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_year', 'order_date_half',\n       'order_date_quarter', 'order_date_month', 'order_date_yweek',\n       'category_2_Cross Country Race', 'category_2_Cyclocross',\n       'category_2_Elite Road', 'category_2_Endurance Road',\n       'category_2_Fat Bike', 'category_2_Over Mountain', 'category_2_Sport',\n       'category_2_Trail', 'category_2_Triathalon'],\n      dtype='object')\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ntrain_columns = [ 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_year', 'order_date_half',\n       'order_date_quarter', 'order_date_month', 'order_date_yweek','category_2_Cross Country Race', 'category_2_Cyclocross',\n       'category_2_Elite Road', 'category_2_Endurance Road',\n       'category_2_Fat Bike', 'category_2_Over Mountain', 'category_2_Sport',\n       'category_2_Trail', 'category_2_Triathalon']\nX = train[train_columns]\ny = train[['total_price_sum']]\n\nmodel = RandomForestClassifier()\nmodel = model.fit(X, y)\n\n\nWe now have a fitted model, and can use this to predict sales from our future frame.\n\n\nCode\npredicted_values = model.predict(future[train_columns])\nfuture['y_pred'] = predicted_values\n\nfuture.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\ny_pred\n\n\n\n\n51\n2012-01-08\nNaN\n51820.0\n75720.0\n2012\n1\n1\n1\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n52\n2012-01-15\nNaN\n62940.0\n21240.0\n2012\n1\n1\n1\n2\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n61480.0\n\n\n53\n2012-01-22\nNaN\n9060.0\n11620.0\n2012\n1\n1\n1\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n61480.0\n\n\n54\n2012-01-29\nNaN\n15980.0\n9730.0\n2012\n1\n1\n1\n4\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n61480.0\n\n\n55\n2012-02-05\nNaN\n59180.0\n22780.0\n2012\n1\n1\n2\n5\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n56\n2012-02-12\nNaN\n132550.0\n53680.0\n2012\n1\n1\n2\n6\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n57\n2012-02-19\nNaN\n68430.0\n38360.0\n2012\n1\n1\n2\n7\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n58\n2012-02-26\nNaN\n29470.0\n90290.0\n2012\n1\n1\n2\n8\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n59\n2012-03-04\nNaN\n71080.0\n7380.0\n2012\n1\n1\n3\n9\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n61480.0\n\n\n60\n2012-03-11\nNaN\n9800.0\n0.0\n2012\n1\n1\n3\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n6045.0\n\n\n\n\n\n\n\nNow let us do a little cleanup. For ease in plotting later, let’s add a column to track the actuals vs. the predicted values.\n\n\nCode\ntrain['type'] = 'actuals'\nfuture['type'] = 'prediction'\n\nfull_df = pd.concat([train, future])\n\nfull_df.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\ntype\ny_pred\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n\n\n\n\n\nYou can get the grouping category back from the one-hot encoding for easier plotting.\nFor simplicity, we will search for any column with ‘category’ in its name.\n\n\nCode\n# Extract dummy columns\ndummy_cols = [col for col in full_df.columns if 'category' in col.lower() ]\nfull_df_reverted = full_df.copy()\n\n# Convert dummy columns back to categorical column\nfull_df_reverted['category'] = full_df_reverted[dummy_cols].idxmax(axis=1).str.replace(\"A_\", \"\")\n\n# Drop dummy columns\nfull_df_reverted = full_df_reverted.drop(columns=dummy_cols)\n\nfull_df_reverted.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ntype\ny_pred\ncategory\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n\n\n\n\n\nBefore we proceed to visualization, let’s streamline our dataset by aligning our predicted values with the actuals. This approach will simplify the plotting process. Given that our DataFrame columns are already labeled as ‘actuals’ and ‘predictions’, a brief conditional check will allow us to consolidate the necessary values.\n\n\nCode\nfull_df_reverted['total_price_sum'] = np.where(full_df_reverted.type =='actuals', full_df_reverted.total_price_sum, full_df_reverted.y_pred)\n\nfull_df_reverted.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ntype\ny_pred\ncategory\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n\n\n\n\n\n\nPlotlyPlotnine\n\n\n\n\nCode\nfull_df_reverted \\\n    .groupby('category') \\\n    .plot_timeseries(\n        date_column = 'order_date',\n        value_column = 'total_price_sum',\n        color_column = 'type',\n        smooth = False,\n        smooth_alpha = 0,\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 800,\n        height = 600,\n        engine = 'plotly'\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nfull_df_reverted \\\n    .groupby('category') \\\n    .plot_timeseries(\n        date_column = 'order_date',\n        value_column = 'total_price_sum',\n        color_column = 'type',\n        smooth = False,\n        smooth_alpha = 0,\n        facet_ncol = 2,    \n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        engine = 'plotnine'\n    )\n\n\n\n\n\n&lt;Figure Size: (1000 x 800)&gt;\n\n\n\n\n\nUpon examining the graph, our models look alright given the length of time for training. Important points:\n\nFor effective time series forecasting, having multiple years of data is pivotal. This provides the model ample opportunities to recognize and adapt to seasonal variations.\nGiven our dataset spanned less than a year, the model lacked the depth of historical context to discern such patterns.\nAlthough our feature engineering was kept basic to introduce various pytimetk capabilities, there’s room for enhancement.\nFor a more refined analysis, consider experimenting with different machine learning models and diving deeper into feature engineering.\nPytimetk’s tk.augment_fourier() might assist in discerning seasonal trends, but with the dataset’s limited historical scope, capturing intricate patterns could remain a challenge."
  },
  {
    "objectID": "reference/ts_summary.html",
    "href": "reference/ts_summary.html",
    "title": "ts_summary",
    "section": "",
    "text": "ts_summary(data, date_column)\nComputes summary statistics for a time series data, either for the entire dataset or grouped by a specific column."
  },
  {
    "objectID": "reference/ts_summary.html#parameters",
    "href": "reference/ts_summary.html#parameters",
    "title": "ts_summary",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter can be either a Pandas DataFrame or a Pandas DataFrameGroupBy object. It represents the data that you want to summarize.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to compute summary statistics for the time series data.\nrequired"
  },
  {
    "objectID": "reference/ts_summary.html#returns",
    "href": "reference/ts_summary.html#returns",
    "title": "ts_summary",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe ts_summary function returns a summary of time series data. The summary includes the following statistics: - If grouped data is provided, the returned data will contain the grouping columns first. - date_n: The number of observations in the time series. - date_tz: The time zone of the time series. - date_start: The first date in the time series. - date_end: The last date in the time series. - freq_inferred_unit: The inferred frequency of the time series from pandas. - freq_median_timedelta: The median time difference between consecutive observations in the time series. - freq_median_scale: The median time difference between consecutive observations in the time series, scaled to a common unit. - freq_median_unit: The unit of the median time difference between consecutive observations in the time series. - diff_min: The minimum time difference between consecutive observations in the time series as a timedelta. - diff_q25: The 25th percentile of the time difference between consecutive observations in the time series as a timedelta. - diff_median: The median time difference between consecutive observations in the time series as a timedelta. - diff_mean: The mean time difference between consecutive observations in the time series as a timedelta. - diff_q75: The 75th percentile of the time difference between consecutive observations in the time series as a timedelta. - diff_max: The maximum time difference between consecutive observations in the time series as a timedelta. - diff_min_seconds: The minimum time difference between consecutive observations in the time series in seconds. - diff_q25_seconds: The 25th percentile of the time difference between consecutive observations in the time series in seconds. - diff_median_seconds: The median time difference between consecutive observations in the time series in seconds. - diff_mean_seconds: The mean time difference between consecutive observations in the time series in seconds. - diff_q75_seconds: The 75th percentile of the time difference between consecutive observations in the time series in seconds. - diff_max_seconds: The maximum time difference between consecutive observations in the time series in seconds."
  },
  {
    "objectID": "reference/ts_summary.html#examples",
    "href": "reference/ts_summary.html#examples",
    "title": "ts_summary",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\ndates = pd.to_datetime([\"2023-10-02\", \"2023-10-03\", \"2023-10-04\", \"2023-10-05\", \"2023-10-06\", \"2023-10-09\", \"2023-10-10\"])\ndf = pd.DataFrame(dates, columns = [\"date\"])\n\ndf.ts_summary(date_column = 'date')\n\n\n\n\n\n\n\n\ndate_n\ndate_tz\ndate_start\ndate_end\nfreq_inferred_unit\nfreq_median_timedelta\nfreq_median_scale\nfreq_median_unit\ndiff_min\ndiff_q25\ndiff_median\ndiff_mean\ndiff_q75\ndiff_max\ndiff_min_seconds\ndiff_q25_seconds\ndiff_median_seconds\ndiff_mean_seconds\ndiff_q75_seconds\ndiff_max_seconds\n\n\n\n\n0\n7\nNone\n2023-10-02\n2023-10-10\nB\n1 days\n1.0\nD\n1 days\n1 days\n1 days\n1 days 08:00:00\n1 days\n3 days\n86400.0\n86400.0\n86400.0\n115200.0\n86400.0\n259200.0\n\n\n\n\n\n\n\n\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\n \ndf.groupby('symbol').ts_summary(date_column = 'date') \n\n\n\n\n\n\n\n\nsymbol\ndate_n\ndate_tz\ndate_start\ndate_end\nfreq_inferred_unit\nfreq_median_timedelta\nfreq_median_scale\nfreq_median_unit\ndiff_min\n...\ndiff_median\ndiff_mean\ndiff_q75\ndiff_max\ndiff_min_seconds\ndiff_q25_seconds\ndiff_median_seconds\ndiff_mean_seconds\ndiff_q75_seconds\ndiff_max_seconds\n\n\n\n\n0\nAAPL\n2699\nNone\n2013-01-02\n2023-09-21\nB\n1 days\n1.0\nD\n1 days\n...\n1 days\n1 days 10:49:00.845070422\n1 days\n4 days\n86400.0\n86400.0\n86400.0\n125340.84507\n86400.0\n345600.0\n\n\n1\nAMZN\n2699\nNone\n2013-01-02\n2023-09-21\nB\n1 days\n1.0\nD\n1 days\n...\n1 days\n1 days 10:49:00.845070422\n1 days\n4 days\n86400.0\n86400.0\n86400.0\n125340.84507\n86400.0\n345600.0\n\n\n2\nGOOG\n2699\nNone\n2013-01-02\n2023-09-21\nB\n1 days\n1.0\nD\n1 days\n...\n1 days\n1 days 10:49:00.845070422\n1 days\n4 days\n86400.0\n86400.0\n86400.0\n125340.84507\n86400.0\n345600.0\n\n\n3\nMETA\n2699\nNone\n2013-01-02\n2023-09-21\nB\n1 days\n1.0\nD\n1 days\n...\n1 days\n1 days 10:49:00.845070422\n1 days\n4 days\n86400.0\n86400.0\n86400.0\n125340.84507\n86400.0\n345600.0\n\n\n4\nNFLX\n2699\nNone\n2013-01-02\n2023-09-21\nB\n1 days\n1.0\nD\n1 days\n...\n1 days\n1 days 10:49:00.845070422\n1 days\n4 days\n86400.0\n86400.0\n86400.0\n125340.84507\n86400.0\n345600.0\n\n\n5\nNVDA\n2699\nNone\n2013-01-02\n2023-09-21\nB\n1 days\n1.0\nD\n1 days\n...\n1 days\n1 days 10:49:00.845070422\n1 days\n4 days\n86400.0\n86400.0\n86400.0\n125340.84507\n86400.0\n345600.0\n\n\n\n\n6 rows × 21 columns\n\n\n\n\n# See also:\ntk.get_date_summary(df['date'])\n\ntk.get_frequency_summary(df['date'])\n\ntk.get_diff_summary(df['date'])\n\ntk.get_diff_summary(df['date'], numeric = True)\n\n\n\n\n\n\n\n\ndiff_min_seconds\ndiff_q25_seconds\ndiff_median_seconds\ndiff_mean_seconds\ndiff_q75_seconds\ndiff_max_seconds\n\n\n\n\n0\n-338169600.0\n86400.0\n86400.0\n20883.690484\n86400.0\n345600.0"
  },
  {
    "objectID": "reference/timeseries_unit_frequency_table.html",
    "href": "reference/timeseries_unit_frequency_table.html",
    "title": "timeseries_unit_frequency_table",
    "section": "",
    "text": "timeseries_unit_frequency_table(wide_format=False)\nThe function timeseries_unit_frequency_table returns a pandas DataFrame with units of time and their corresponding frequencies in seconds.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\na pandas DataFrame that contains two columns: “unit” and “freq”. The “unit” column contains the units of time (seconds, minutes, hours, etc.), and the “freq” column contains the corresponding frequencies in seconds for each unit."
  },
  {
    "objectID": "reference/timeseries_unit_frequency_table.html#returns",
    "href": "reference/timeseries_unit_frequency_table.html#returns",
    "title": "timeseries_unit_frequency_table",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\npd.DataFrame\na pandas DataFrame that contains two columns: “unit” and “freq”. The “unit” column contains the units of time (seconds, minutes, hours, etc.), and the “freq” column contains the corresponding frequencies in seconds for each unit."
  },
  {
    "objectID": "reference/summarize_by_time.html",
    "href": "reference/summarize_by_time.html",
    "title": "summarize_by_time",
    "section": "",
    "text": "summarize_by_time(data, date_column, value_column, freq='D', agg_func='sum', kind='timestamp', wide_format=False, fillna=0, *args, **kwargs)\nSummarize a DataFrame or GroupBy object by time.\nThe summarize_by_time function aggregates data by a specified time period and one or more numeric columns, allowing for grouping and customization of the time-based aggregation."
  },
  {
    "objectID": "reference/summarize_by_time.html#parameters",
    "href": "reference/summarize_by_time.html#parameters",
    "title": "summarize_by_time",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nA pandas DataFrame or a pandas GroupBy object. This is the data that you want to summarize by time.\nrequired\n\n\ndate_column\nstr\nThe name of the column in the data frame that contains the dates or timestamps to be aggregated by. This column must be of type datetime64.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the name of one or more columns in the DataFrame that you want to aggregate by. It can be either a string representing a single column name, or a list of strings representing multiple column names.\nrequired\n\n\nfreq\nstr\nThe freq parameter specifies the frequency at which the data should be aggregated. It accepts a string representing a pandas frequency offset, such as “D” for daily or “MS” for month start. The default value is “D”, which means the data will be aggregated on a daily basis. Some common frequency aliases include: - S: secondly frequency - min: minute frequency - H: hourly frequency - D: daily frequency - W: weekly frequency - M: month end frequency - MS: month start frequency - Q: quarter end frequency - QS: quarter start frequency - Y: year end frequency - YS: year start frequency\n'D'\n\n\nagg_func\nlist\nThe agg_func parameter is used to specify one or more aggregating functions to apply to the value column(s) during the summarization process. It can be a single function or a list of functions. The default value is \"sum\", which represents the sum function. Some common aggregating functions include: - “sum”: Sum of values - “mean”: Mean of values - “median”: Median of values - “min”: Minimum of values - “max”: Maximum of values - “std”: Standard deviation of values - “var”: Variance of values - “first”: First value in group - “last”: Last value in group - “count”: Count of values - “nunique”: Number of unique values - “corr”: Correlation between values Custom lambda aggregating functions can be used too. Here are several common examples: - (“q25”, lambda x: x.quantile(0.25)): 25th percentile of values - (“q75”, lambda x: x.quantile(0.75)): 75th percentile of values - (“iqr”, lambda x: x.quantile(0.75) - x.quantile(0.25)): Interquartile range of values - (“range”, lambda x: x.max() - x.min()): Range of values\n'sum'\n\n\nwide_format\nbool\nA boolean parameter that determines whether the output should be in “wide” or “long” format. If set to True, the output will be in wide format, where each group is represented by a separate column. If set to False, the output will be in long format, where each group is represented by a separate row. The default value is False.\nFalse\n\n\nfillna\nint\nThe fillna parameter is used to specify the value to fill missing data with. By default, it is set to 0. If you want to keep missing values as NaN, you can use np.nan as the value for fillna.\n0"
  },
  {
    "objectID": "reference/summarize_by_time.html#returns",
    "href": "reference/summarize_by_time.html#returns",
    "title": "summarize_by_time",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame that is summarized by time."
  },
  {
    "objectID": "reference/summarize_by_time.html#examples",
    "href": "reference/summarize_by_time.html#examples",
    "title": "summarize_by_time",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Summarize by time with a DataFrame object\n( \n    df \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price',\n            freq         = \"MS\",\n            agg_func     = ['mean', 'sum']\n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_mean\ntotal_price_sum\n\n\n\n\n0\n2011-01-01\n4600.142857\n483015\n\n\n1\n2011-02-01\n4611.408730\n1162075\n\n\n2\n2011-03-01\n5196.653543\n659975\n\n\n3\n2011-04-01\n4533.846154\n1827140\n\n\n4\n2011-05-01\n4097.912621\n844170\n\n\n5\n2011-06-01\n4544.839228\n1413445\n\n\n6\n2011-07-01\n4976.791667\n1194430\n\n\n7\n2011-08-01\n4961.970803\n679790\n\n\n8\n2011-09-01\n4682.298851\n814720\n\n\n9\n2011-10-01\n3930.053476\n734920\n\n\n10\n2011-11-01\n4768.175355\n1006085\n\n\n11\n2011-12-01\n4186.902655\n473120\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Long Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = False, \n        )\n)\n\n\n\n\n\n\n\n\ncategory_1\norder_date\ntotal_price\n\n\n\n\n0\nMountain\n2011-01-01\n221490\n\n\n1\nMountain\n2011-02-01\n660555\n\n\n2\nMountain\n2011-03-01\n358855\n\n\n3\nMountain\n2011-04-01\n1075975\n\n\n4\nMountain\n2011-05-01\n450440\n\n\n5\nMountain\n2011-06-01\n723040\n\n\n6\nMountain\n2011-07-01\n767740\n\n\n7\nMountain\n2011-08-01\n361255\n\n\n8\nMountain\n2011-09-01\n401125\n\n\n9\nMountain\n2011-10-01\n377335\n\n\n10\nMountain\n2011-11-01\n549345\n\n\n11\nMountain\n2011-12-01\n276055\n\n\n12\nRoad\n2011-01-01\n261525\n\n\n13\nRoad\n2011-02-01\n501520\n\n\n14\nRoad\n2011-03-01\n301120\n\n\n15\nRoad\n2011-04-01\n751165\n\n\n16\nRoad\n2011-05-01\n393730\n\n\n17\nRoad\n2011-06-01\n690405\n\n\n18\nRoad\n2011-07-01\n426690\n\n\n19\nRoad\n2011-08-01\n318535\n\n\n20\nRoad\n2011-09-01\n413595\n\n\n21\nRoad\n2011-10-01\n357585\n\n\n22\nRoad\n2011-11-01\n456740\n\n\n23\nRoad\n2011-12-01\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object and multiple summaries (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = ['sum', 'mean', ('q25', lambda x: x.quantile(0.25)), ('q75', lambda x: x.quantile(0.75))],\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum_Mountain\ntotal_price_sum_Road\ntotal_price_mean_Mountain\ntotal_price_mean_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n4922.000000\n4358.750000\n2060.0\n1950.0\n6070.0\n5605.0\n\n\n1\n2011-02-01\n660555\n501520\n4374.536424\n4965.544554\n2060.0\n1950.0\n5330.0\n5860.0\n\n\n2\n2011-03-01\n358855\n301120\n5882.868852\n4562.424242\n2130.0\n2240.0\n6390.0\n5875.0\n\n\n3\n2011-04-01\n1075975\n751165\n4890.795455\n4104.726776\n2060.0\n1950.0\n5970.0\n4800.0\n\n\n4\n2011-05-01\n450440\n393730\n4549.898990\n3679.719626\n2010.0\n1570.0\n6020.0\n3500.0\n\n\n5\n2011-06-01\n723040\n690405\n5021.111111\n4134.161677\n1950.0\n1840.0\n5647.5\n4500.0\n\n\n6\n2011-07-01\n767740\n426690\n5444.964539\n4310.000000\n2130.0\n1895.0\n6400.0\n5330.0\n\n\n7\n2011-08-01\n361255\n318535\n5734.206349\n4304.527027\n2235.0\n1950.0\n6400.0\n4987.5\n\n\n8\n2011-09-01\n401125\n413595\n5077.531646\n4353.631579\n1620.0\n1950.0\n6390.0\n5330.0\n\n\n9\n2011-10-01\n377335\n357585\n4439.235294\n3505.735294\n2160.0\n1750.0\n6070.0\n4260.0\n\n\n10\n2011-11-01\n549345\n456740\n5282.163462\n4268.598131\n2340.0\n1950.0\n7460.0\n4370.0\n\n\n11\n2011-12-01\n276055\n197065\n5208.584906\n3284.416667\n2060.0\n1652.5\n6400.0\n3200.0"
  },
  {
    "objectID": "reference/palette_timetk.html",
    "href": "reference/palette_timetk.html",
    "title": "palette_timetk",
    "section": "",
    "text": "palette_timetk()\nThe function palette_timetk returns a dictionary of color codes for various colors in the timetk theme."
  },
  {
    "objectID": "reference/palette_timetk.html#returns",
    "href": "reference/palette_timetk.html#returns",
    "title": "palette_timetk",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function palette_timetk returns a dictionary containing color names as keys and their corresponding hexadecimal color codes as values:"
  },
  {
    "objectID": "reference/palette_timetk.html#examples",
    "href": "reference/palette_timetk.html#examples",
    "title": "palette_timetk",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\n\ntk.palette_timetk()\n\n{'blue': '#2c3e50',\n 'red': '#e31a1c',\n 'green': '#18BC9C',\n 'yellow': '#CCBE93',\n 'steel_blue': '#a6cee3',\n 'navy_blue': '#1f78b4',\n 'light_green': '#b2df8a',\n 'pink': '#fb9a99',\n 'light_orange': '#fdbf6f',\n 'orange': '#ff7f00',\n 'light_purple': '#cab2d6',\n 'purple': '#6a3d9a'}"
  },
  {
    "objectID": "reference/make_weekend_sequence.html",
    "href": "reference/make_weekend_sequence.html",
    "title": "make_weekend_sequence",
    "section": "",
    "text": "make_weekend_sequence(start_date, end_date, friday_saturday=False, remove_holidays=False, country=None)\nGenerate a sequence of weekend dates within a specified date range, optionally excluding holidays."
  },
  {
    "objectID": "reference/make_weekend_sequence.html#parameters",
    "href": "reference/make_weekend_sequence.html#parameters",
    "title": "make_weekend_sequence",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nstr or datetime or pd.DatetimeIndex\nThe start date of the date range.\nrequired\n\n\nend_date\nstr or datetime or pd.DatetimeIndex\nThe end date of the date range.\nrequired\n\n\nfriday_saturday\nbool\nIf True, generates a sequence with Friday and Saturday as weekends.If False (default), generates a sequence with Saturday and Sunday as weekends.\nFalse\n\n\nremove_holidays\nbool\nIf True, excludes holidays (based on the specified country) from the generated sequence. If False (default), includes holidays in the sequence.\nFalse\n\n\ncountry\nstr\nThe name of the country for which to generate holiday-specific sequences. Defaults to None, which uses the United States as the default country.\nNone"
  },
  {
    "objectID": "reference/make_weekend_sequence.html#returns",
    "href": "reference/make_weekend_sequence.html#returns",
    "title": "make_weekend_sequence",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA Series containing the generated weekday dates."
  },
  {
    "objectID": "reference/make_weekend_sequence.html#examples",
    "href": "reference/make_weekend_sequence.html#examples",
    "title": "make_weekend_sequence",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\n# United States has Saturday and Sunday as weekends\ntk.make_weekend_sequence(\"2023-01-01\", \"2023-01-31\", friday_saturday=False, remove_holidays=True, country='UnitedStates')\n\n0   2023-01-07\n1   2023-01-08\n2   2023-01-14\n3   2023-01-15\n4   2023-01-21\n5   2023-01-22\n6   2023-01-28\n7   2023-01-29\nName: Weekend Dates, dtype: datetime64[ns]\n\n\n\n# Saudi Arabia has Friday and Saturday as weekends\ntk.make_weekend_sequence(\"2023-01-01\", \"2023-01-31\", friday_saturday=True, remove_holidays=True, country='SaudiArabia')\n\n0   2023-01-06\n1   2023-01-07\n2   2023-01-13\n3   2023-01-14\n4   2023-01-20\n5   2023-01-21\n6   2023-01-27\n7   2023-01-28\nName: Weekend Dates, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/make_future_timeseries.html",
    "href": "reference/make_future_timeseries.html",
    "title": "make_future_timeseries",
    "section": "",
    "text": "make_future_timeseries(idx, length_out, force_regular=False)\nMake future dates for a time series.\nThe function make_future_timeseries takes a pandas Series or DateTimeIndex and generates a future sequence of dates based on the frequency of the input series."
  },
  {
    "objectID": "reference/make_future_timeseries.html#parameters",
    "href": "reference/make_future_timeseries.html#parameters",
    "title": "make_future_timeseries",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe idx parameter is the input time series data. It can be either a pandas Series or a pandas DateTimeIndex. It represents the existing dates in the time series.\nrequired\n\n\nlength_out\nint\nThe parameter length_out is an integer that represents the number of future dates to generate for the time series.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether the frequency of the future dates should be forced to be regular. If force_regular is set to True, the frequency of the future dates will be forced to be regular. If force_regular is set to False, the frequency of the future dates will be inferred from the input data (e.g. business calendars might be used). The default value is False.\nFalse"
  },
  {
    "objectID": "reference/make_future_timeseries.html#returns",
    "href": "reference/make_future_timeseries.html#returns",
    "title": "make_future_timeseries",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA pandas Series object containing future dates."
  },
  {
    "objectID": "reference/make_future_timeseries.html#examples",
    "href": "reference/make_future_timeseries.html#examples",
    "title": "make_future_timeseries",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\ndates = pd.Series(pd.to_datetime(['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04']))\ndates\n\n0   2022-01-01\n1   2022-01-02\n2   2022-01-03\n3   2022-01-04\ndtype: datetime64[ns]\n\n\n\n# DateTimeIndex: Generate 5 future dates\nfuture_dates_dt = tk.make_future_timeseries(dates, 5)\nfuture_dates_dt\n\n0   2022-01-05\n1   2022-01-06\n2   2022-01-07\n3   2022-01-08\n4   2022-01-09\ndtype: datetime64[ns]\n\n\n\n# Series: Generate 5 future dates\npd.Series(future_dates_dt).make_future_timeseries(5)\n\n0   2022-01-10\n1   2022-01-11\n2   2022-01-12\n3   2022-01-13\n4   2022-01-14\ndtype: datetime64[ns]\n\n\n\ntimestamps = [\"2023-01-01 01:00\", \"2023-01-01 02:00\", \"2023-01-01 03:00\", \"2023-01-01 04:00\", \"2023-01-01 05:00\"]\n\ndates = pd.to_datetime(timestamps)\n\ntk.make_future_timeseries(dates, 5)\n\n0   2023-01-01 06:00:00\n1   2023-01-01 07:00:00\n2   2023-01-01 08:00:00\n3   2023-01-01 09:00:00\n4   2023-01-01 10:00:00\ndtype: datetime64[ns]\n\n\n\n# Monthly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-02-01\", \"2021-03-01\", \"2021-04-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2021-05-01\n1   2021-06-01\n2   2021-07-01\n3   2021-08-01\ndtype: datetime64[ns]\n\n\n\n# Quarterly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-04-01\", \"2021-07-01\", \"2021-10-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2022-01-01\n1   2022-04-01\n2   2022-07-01\n3   2022-10-01\ndtype: datetime64[ns]\n\n\n\n# Irregular Dates: Business Days\ndates = pd.to_datetime([\"2021-01-01\", \"2021-01-04\", \"2021-01-05\", \"2021-01-06\"])\n\ntk.get_pandas_frequency(dates)\n\ntk.make_future_timeseries(dates, 4)\n\n0   2021-01-07\n1   2021-01-08\n2   2021-01-11\n3   2021-01-12\ndtype: datetime64[ns]\n\n\n\n# Irregular Dates: Business Days (Force Regular)    \ntk.make_future_timeseries(dates, 4, force_regular=True)\n\n0   2021-01-07\n1   2021-01-08\n2   2021-01-09\n3   2021-01-10\ndtype: datetime64[ns]"
  },
  {
    "objectID": "reference/is_holiday.html",
    "href": "reference/is_holiday.html",
    "title": "is_holiday",
    "section": "",
    "text": "is_holiday(idx, country_name='UnitedStates', country=None)\nCheck if a given list of dates are holidays for a specified country.\nNote: This function requires the holidays package to be installed."
  },
  {
    "objectID": "reference/is_holiday.html#parameters",
    "href": "reference/is_holiday.html#parameters",
    "title": "is_holiday",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\nUnion[str, datetime, List[Union[str, datetime]], pd.DatetimeIndex, pd.Series]\nThe dates to check for holiday status.\nrequired\n\n\ncountry_name\nstr\nThe name of the country for which to check the holiday status. Defaults to ‘UnitedStates’ if not specified.\n'UnitedStates'\n\n\ncountry\nstr\nAn alternative parameter to specify the country for holiday checking, overriding country_name.\nNone"
  },
  {
    "objectID": "reference/is_holiday.html#returns",
    "href": "reference/is_holiday.html#returns",
    "title": "is_holiday",
    "section": "Returns:",
    "text": "Returns:\npd.Series: Series containing True if the date is a holiday, False otherwise."
  },
  {
    "objectID": "reference/is_holiday.html#raises",
    "href": "reference/is_holiday.html#raises",
    "title": "is_holiday",
    "section": "Raises:",
    "text": "Raises:\nValueError: If the specified country is not found in the holidays package."
  },
  {
    "objectID": "reference/is_holiday.html#examples",
    "href": "reference/is_holiday.html#examples",
    "title": "is_holiday",
    "section": "Examples:",
    "text": "Examples:\n\nimport pandas as pd\nimport pytimetk as tk\n\ntk.is_holiday('2023-01-01', country_name='UnitedStates')\n\n0    True\nName: is_holiday, dtype: bool\n\n\n\n# List of dates\ntk.is_holiday(['2023-01-01', '2023-01-02', '2023-01-03'], country_name='UnitedStates')\n\n0     True\n1     True\n2    False\nName: is_holiday, dtype: bool\n\n\n\n# DatetimeIndex\ntk.is_holiday(pd.date_range(\"2023-01-01\", \"2023-01-03\"), country_name='UnitedStates')\n\n0     True\n1     True\n2    False\nName: is_holiday, dtype: bool\n\n\n\n# Pandas Series Method\n( \n    pd.Series(pd.date_range(\"2023-01-01\", \"2023-01-03\"))\n        .is_holiday(country_name='UnitedStates')\n)\n\n0     True\n1     True\n2    False\nName: is_holiday, dtype: bool"
  },
  {
    "objectID": "reference/glimpse.html",
    "href": "reference/glimpse.html",
    "title": "glimpse",
    "section": "",
    "text": "glimpse(data, max_width=76)\nTakes a pandas DataFrame and prints a summary of its dimensions, column names, data types, and the first few values of each column."
  },
  {
    "objectID": "reference/glimpse.html#parameters",
    "href": "reference/glimpse.html#parameters",
    "title": "glimpse",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe data parameter is a pandas DataFrame that contains the data you want to glimpse at. It is the main input to the glimpse function.\nrequired\n\n\nmax_width\nint\nThe max_width parameter is an optional parameter that specifies the maximum width of each line when printing the glimpse of the DataFrame. If not provided, the default value is set to 76.\n76"
  },
  {
    "objectID": "reference/glimpse.html#examples",
    "href": "reference/glimpse.html#examples",
    "title": "glimpse",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('walmart_sales_weekly', parse_dates=['Date'])\n\ndf.glimpse()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;: 1001 rows of 17 columns\nid:            object           ['1_1', '1_1', '1_1', '1_1', '1_1', '1_1 ...\nStore:         int64            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  ...\nDept:          int64            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  ...\nDate:          datetime64[ns]   [Timestamp('2010-02-05 00:00:00'), Times ...\nWeekly_Sales:  float64          [24924.5, 46039.49, 41595.55, 19403.54,  ...\nIsHoliday:     bool             [False, True, False, False, False, False ...\nType:          object           ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', ...\nSize:          int64            [151315, 151315, 151315, 151315, 151315, ...\nTemperature:   float64          [42.31, 38.51, 39.93, 46.63, 46.5, 57.79 ...\nFuel_Price:    float64          [2.572, 2.548, 2.514, 2.561, 2.625, 2.66 ...\nMarkDown1:     float64          [nan, nan, nan, nan, nan, nan, nan, nan, ...\nMarkDown2:     float64          [nan, nan, nan, nan, nan, nan, nan, nan, ...\nMarkDown3:     float64          [nan, nan, nan, nan, nan, nan, nan, nan, ...\nMarkDown4:     float64          [nan, nan, nan, nan, nan, nan, nan, nan, ...\nMarkDown5:     float64          [nan, nan, nan, nan, nan, nan, nan, nan, ...\nCPI:           float64          [211.0963582, 211.2421698, 211.2891429,  ...\nUnemployment:  float64          [8.106, 8.106, 8.106, 8.106, 8.106, 8.10 ..."
  },
  {
    "objectID": "reference/get_pandas_frequency.html",
    "href": "reference/get_pandas_frequency.html",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "get_pandas_frequency(idx, force_regular=False)\nGet the frequency of a pandas Series or DatetimeIndex.\nThe function get_pandas_frequency takes a Pandas Series or DatetimeIndex as input and returns the inferred frequency of the index, with an option to force regular frequency.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/get_pandas_frequency.html#parameters",
    "href": "reference/get_pandas_frequency.html#parameters",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse"
  },
  {
    "objectID": "reference/get_pandas_frequency.html#returns",
    "href": "reference/get_pandas_frequency.html#returns",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/get_frequency_summary.html",
    "href": "reference/get_frequency_summary.html",
    "title": "get_frequency_summary",
    "section": "",
    "text": "get_frequency_summary(idx)\nMore robust version of pandas inferred frequency."
  },
  {
    "objectID": "reference/get_frequency_summary.html#parameters",
    "href": "reference/get_frequency_summary.html#parameters",
    "title": "get_frequency_summary",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe idx parameter is either a pd.Series or a pd.DateTimeIndex. It represents the index of a pandas DataFrame or Series, which contains datetime values.\nrequired"
  },
  {
    "objectID": "reference/get_frequency_summary.html#returns",
    "href": "reference/get_frequency_summary.html#returns",
    "title": "get_frequency_summary",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA pandas DataFrame with the following columns: - freq_inferred_unit: The inferred frequency of the time series from pandas. - freq_median_timedelta: The median time difference between consecutive observations in the time series. - freq_median_scale: The median time difference between consecutive observations in the time series, scaled to a common unit. - freq_median_unit: The unit of the median time difference between consecutive observations in the time series."
  },
  {
    "objectID": "reference/get_frequency_summary.html#examples",
    "href": "reference/get_frequency_summary.html#examples",
    "title": "get_frequency_summary",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\ndates = pd.date_range(start = '2020-01-01', end = '2020-01-10', freq = 'D')\n\ntk.get_frequency(dates)\n\n'D'\n\n\n\n# pandas inferred frequency fails\ndates = pd.to_datetime([\"2021-01-01\", \"2021-02-01\"])\n\n# Returns None\ntk.get_pandas_frequency(dates)\n\n# Returns '1MS'\ntk.get_frequency(dates)\n\n'1MS'"
  },
  {
    "objectID": "reference/get_diff_summary.html",
    "href": "reference/get_diff_summary.html",
    "title": "get_diff_summary",
    "section": "",
    "text": "get_diff_summary(idx, numeric=False)\nCalculates summary statistics of the time differences between consecutive values in a datetime index.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe idx parameter can be either a pandas Series or a pandas DateTimeIndex. It represents the index values for which you want to calculate the difference summary.\nrequired\n\n\nnumeric\nbool\nThe numeric parameter is a boolean flag that indicates whether the input index should be treated as numeric or not. - If numeric is set to True, the index values are converted to integers representing the number of seconds since the Unix epoch (January 1, 1970). - If numeric is set to False, the index values are treated as datetime values. The default value of numeric is False.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe function get_diff_summary returns a pandas DataFrame containing summary statistics including: If numeric is set to False, the column names are: - diff_min: The minimum time difference between consecutive observations in the time series as a timedelta. - diff_q25: The 25th percentile of the time difference between consecutive observations in the time series as a timedelta. - diff_median: The median time difference between consecutive observations in the time series as a timedelta. - diff_mean: The mean time difference between consecutive observations in the time series as a timedelta. - diff_q75: The 75th percentile of the time difference between consecutive observations in the time series as a timedelta. - diff_max: The maximum time difference between consecutive observations in the time series as a timedelta. If numeric is set to True, the column names are: - diff_min_seconds: The minimum time difference between consecutive observations in the time series in seconds. - diff_q25_seconds: The 25th percentile of the time difference between consecutive observations in the time series in seconds. - diff_median_seconds: The median time difference between consecutive observations in the time series in seconds. - diff_mean_seconds: The mean time difference between consecutive observations in the time series in seconds. - diff_q75_seconds: The 75th percentile of the time difference between consecutive observations in the time series in seconds. - diff_max_seconds: The maximum time difference between consecutive observations in the time series in seconds."
  },
  {
    "objectID": "reference/get_diff_summary.html#parameters",
    "href": "reference/get_diff_summary.html#parameters",
    "title": "get_diff_summary",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe idx parameter can be either a pandas Series or a pandas DateTimeIndex. It represents the index values for which you want to calculate the difference summary.\nrequired\n\n\nnumeric\nbool\nThe numeric parameter is a boolean flag that indicates whether the input index should be treated as numeric or not. - If numeric is set to True, the index values are converted to integers representing the number of seconds since the Unix epoch (January 1, 1970). - If numeric is set to False, the index values are treated as datetime values. The default value of numeric is False.\nFalse"
  },
  {
    "objectID": "reference/get_diff_summary.html#returns",
    "href": "reference/get_diff_summary.html#returns",
    "title": "get_diff_summary",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\npd.DataFrame\nThe function get_diff_summary returns a pandas DataFrame containing summary statistics including: If numeric is set to False, the column names are: - diff_min: The minimum time difference between consecutive observations in the time series as a timedelta. - diff_q25: The 25th percentile of the time difference between consecutive observations in the time series as a timedelta. - diff_median: The median time difference between consecutive observations in the time series as a timedelta. - diff_mean: The mean time difference between consecutive observations in the time series as a timedelta. - diff_q75: The 75th percentile of the time difference between consecutive observations in the time series as a timedelta. - diff_max: The maximum time difference between consecutive observations in the time series as a timedelta. If numeric is set to True, the column names are: - diff_min_seconds: The minimum time difference between consecutive observations in the time series in seconds. - diff_q25_seconds: The 25th percentile of the time difference between consecutive observations in the time series in seconds. - diff_median_seconds: The median time difference between consecutive observations in the time series in seconds. - diff_mean_seconds: The mean time difference between consecutive observations in the time series in seconds. - diff_q75_seconds: The 75th percentile of the time difference between consecutive observations in the time series in seconds. - diff_max_seconds: The maximum time difference between consecutive observations in the time series in seconds."
  },
  {
    "objectID": "reference/get_available_datasets.html",
    "href": "reference/get_available_datasets.html",
    "title": "get_available_datasets",
    "section": "",
    "text": "get_available_datasets()\nGet a list of 12 datasets that can be loaded with pytimetk.load_dataset.\nThe get_available_datasets function returns a sorted list of available dataset names from the pytimetk.datasets module. The available datasets are:"
  },
  {
    "objectID": "reference/get_available_datasets.html#returns",
    "href": "reference/get_available_datasets.html#returns",
    "title": "get_available_datasets",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nThe function get_available_datasets returns a sorted list of available dataset names from the pytimetk.datasets module."
  },
  {
    "objectID": "reference/get_available_datasets.html#examples",
    "href": "reference/get_available_datasets.html#examples",
    "title": "get_available_datasets",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\n\ntk.get_available_datasets()\n\n['bike_sales_sample',\n 'bike_sharing_daily',\n 'expedia',\n 'm4_daily',\n 'm4_hourly',\n 'm4_monthly',\n 'm4_quarterly',\n 'm4_weekly',\n 'm4_yearly',\n 'stocks_daily',\n 'taylor_30_min',\n 'walmart_sales_weekly',\n 'wikipedia_traffic_daily']"
  },
  {
    "objectID": "reference/floor_date.html",
    "href": "reference/floor_date.html",
    "title": "floor_date",
    "section": "",
    "text": "floor_date(idx, unit='D')\nRound a date down to the specified unit (e.g. Flooring).\nThe floor_date function takes a pandas Series of dates and returns a new Series with the dates rounded down to the specified unit."
  },
  {
    "objectID": "reference/floor_date.html#parameters",
    "href": "reference/floor_date.html#parameters",
    "title": "floor_date",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter is a pandas Series or pandas DatetimeIndex object that contains datetime values. It represents the dates that you want to round down.\nrequired\n\n\nunit\nstr\nThe unit parameter in the floor_date function is a string that specifies the time unit to which the dates in the idx series should be rounded down. It has a default value of “D”, which stands for day. Other possible values for the unit parameter could be\n'D'"
  },
  {
    "objectID": "reference/floor_date.html#returns",
    "href": "reference/floor_date.html#returns",
    "title": "floor_date",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe floor_date function returns a pandas Series object containing datetime64[ns] values."
  },
  {
    "objectID": "reference/floor_date.html#examples",
    "href": "reference/floor_date.html#examples",
    "title": "floor_date",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-01-10\", freq=\"1H\")\ndates\n\nDatetimeIndex(['2020-01-01 00:00:00', '2020-01-01 01:00:00',\n               '2020-01-01 02:00:00', '2020-01-01 03:00:00',\n               '2020-01-01 04:00:00', '2020-01-01 05:00:00',\n               '2020-01-01 06:00:00', '2020-01-01 07:00:00',\n               '2020-01-01 08:00:00', '2020-01-01 09:00:00',\n               ...\n               '2020-01-09 15:00:00', '2020-01-09 16:00:00',\n               '2020-01-09 17:00:00', '2020-01-09 18:00:00',\n               '2020-01-09 19:00:00', '2020-01-09 20:00:00',\n               '2020-01-09 21:00:00', '2020-01-09 22:00:00',\n               '2020-01-09 23:00:00', '2020-01-10 00:00:00'],\n              dtype='datetime64[ns]', length=217, freq='H')\n\n\n\n# Works on DateTimeIndex\ntk.floor_date(dates, unit=\"D\")\n\n0     2020-01-01\n1     2020-01-01\n2     2020-01-01\n3     2020-01-01\n4     2020-01-01\n         ...    \n212   2020-01-09\n213   2020-01-09\n214   2020-01-09\n215   2020-01-09\n216   2020-01-10\nName: idx, Length: 217, dtype: datetime64[ns]\n\n\n\n# Works on Pandas Series\ndates.to_series().floor_date(unit=\"D\")\n\n2020-01-01 00:00:00   2020-01-01\n2020-01-01 01:00:00   2020-01-01\n2020-01-01 02:00:00   2020-01-01\n2020-01-01 03:00:00   2020-01-01\n2020-01-01 04:00:00   2020-01-01\n                         ...    \n2020-01-09 20:00:00   2020-01-09\n2020-01-09 21:00:00   2020-01-09\n2020-01-09 22:00:00   2020-01-09\n2020-01-09 23:00:00   2020-01-09\n2020-01-10 00:00:00   2020-01-10\nFreq: H, Length: 217, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html",
    "href": "reference/augment_timeseries_signature.html",
    "title": "augment_timeseries_signature",
    "section": "",
    "text": "augment_timeseries_signature(data, date_column)\nAdd 29 time series features to a DataFrame.\nThe function augment_timeseries_signature takes a DataFrame and a date column as input and returns the original DataFrame with the 29 different date and time based features added as new columns:"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#parameters",
    "href": "reference/augment_timeseries_signature.html#parameters",
    "title": "augment_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe data parameter is a pandas DataFrame that contains the time series data.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that represents the name of the date column in the data DataFrame.\nrequired"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#returns",
    "href": "reference/augment_timeseries_signature.html#returns",
    "title": "augment_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA pandas DataFrame that is the concatenation of the original data DataFrame and the ts_signature_df DataFrame."
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#examples",
    "href": "reference/augment_timeseries_signature.html#examples",
    "title": "augment_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\npd.set_option('display.max_columns', None)\n\n# Adds 29 new time series features as columns to the original DataFrame\n( \n    tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n        .augment_timeseries_signature(date_column = 'order_date')\n        .head()\n)\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\norder_date_leapyear\norder_date_half\norder_date_quarter\norder_date_quarteryear\norder_date_quarterstart\norder_date_quarterend\norder_date_month\norder_date_month_lbl\norder_date_monthstart\norder_date_monthend\norder_date_yweek\norder_date_mweek\norder_date_wday\norder_date_wday_lbl\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/augment_leads.html",
    "href": "reference/augment_leads.html",
    "title": "augment_leads",
    "section": "",
    "text": "augment_leads(data, date_column, value_column, leads=1)\nAdds leads to a Pandas DataFrame or DataFrameGroupBy object.\nThe augment_leads function takes a Pandas DataFrame or GroupBy object, a date column, a value column or list of value columns, and a lead or list of leads, and adds leaded versions of the value columns to the DataFrame."
  },
  {
    "objectID": "reference/augment_leads.html#parameters",
    "href": "reference/augment_leads.html#parameters",
    "title": "augment_leads",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to add leaded columns to.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to sort the data before adding the leaded values.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the column(s) in the DataFrame that you want to add leaded values for. It can be either a single column name (string) or a list of column names.\nrequired\n\n\nleads\nint or tuple or list\nThe leads parameter is an integer, tuple, or list that specifies the number of leaded values to add to the DataFrame. If it is an integer, the function will add that number of leaded values for each column specified in the value_column parameter. If it is a tuple, it will generate leads from the first to the second value (inclusive). If it is a list, it will generate leads based on the values in the list.\n1"
  },
  {
    "objectID": "reference/augment_leads.html#returns",
    "href": "reference/augment_leads.html#returns",
    "title": "augment_leads",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame with leaded columns added to it."
  },
  {
    "objectID": "reference/augment_leads.html#examples",
    "href": "reference/augment_leads.html#examples",
    "title": "augment_leads",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\ndf = tk.load_dataset('m4_daily', parse_dates=['date'])\n\n\n# Add a leaded value of 2 for each grouped time series\nleaded_df = (\n    df \n        .groupby('id')\n        .augment_leads(\n            date_column='date',\n            value_column='value',\n            leads=2\n        )\n)\nleaded_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lead_2\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2048.7\n\n\n1\nD10\n2014-07-04\n2073.4\n2048.9\n\n\n2\nD10\n2014-07-05\n2048.7\n2006.4\n\n\n3\nD10\n2014-07-06\n2048.9\n2017.6\n\n\n4\nD10\n2014-07-07\n2006.4\n2019.1\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9445.9\n\n\n9739\nD500\n2012-09-20\n9365.7\n9497.9\n\n\n9740\nD500\n2012-09-21\n9445.9\n9545.3\n\n\n9741\nD500\n2012-09-22\n9497.9\nNaN\n\n\n9742\nD500\n2012-09-23\n9545.3\nNaN\n\n\n\n\n9743 rows × 4 columns\n\n\n\n\n# Add 7 leaded values for a single time series\nleaded_df_single = (\n    df \n        .query('id == \"D10\"')\n        .augment_leads(\n            date_column='date',\n            value_column='value',\n            leads=(1, 7)\n        )\n)\nleaded_df_single \n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lead_1\nvalue_lead_2\nvalue_lead_3\nvalue_lead_4\nvalue_lead_5\nvalue_lead_6\nvalue_lead_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2073.4\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n\n\n1\nD10\n2014-07-04\n2073.4\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n\n\n2\nD10\n2014-07-05\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n\n\n3\nD10\n2014-07-06\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n1978.8\n\n\n4\nD10\n2014-07-07\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n1978.8\n1988.3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2649.3\n2631.8\n2622.5\n2620.1\nNaN\nNaN\nNaN\n\n\n670\nD10\n2016-05-03\n2649.3\n2631.8\n2622.5\n2620.1\nNaN\nNaN\nNaN\nNaN\n\n\n671\nD10\n2016-05-04\n2631.8\n2622.5\n2620.1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n672\nD10\n2016-05-05\n2622.5\n2620.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n673\nD10\n2016-05-06\n2620.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n674 rows × 10 columns\n\n\n\n\n# Add 2 leaded values, 2 and 4, for a single time series\nleaded_df_single_two = (\n    df \n        .query('id == \"D10\"')\n        .augment_leads(\n            date_column='date',\n            value_column='value',\n            leads=[2, 4]\n        )\n)\nleaded_df_single_two\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lead_2\nvalue_lead_4\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2048.7\n2006.4\n\n\n1\nD10\n2014-07-04\n2073.4\n2048.9\n2017.6\n\n\n2\nD10\n2014-07-05\n2048.7\n2006.4\n2019.1\n\n\n3\nD10\n2014-07-06\n2048.9\n2017.6\n2007.4\n\n\n4\nD10\n2014-07-07\n2006.4\n2019.1\n2010.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2631.8\n2620.1\n\n\n670\nD10\n2016-05-03\n2649.3\n2622.5\nNaN\n\n\n671\nD10\n2016-05-04\n2631.8\n2620.1\nNaN\n\n\n672\nD10\n2016-05-05\n2622.5\nNaN\nNaN\n\n\n673\nD10\n2016-05-06\n2620.1\nNaN\nNaN\n\n\n\n\n674 rows × 5 columns"
  },
  {
    "objectID": "reference/augment_holiday_signature.html",
    "href": "reference/augment_holiday_signature.html",
    "title": "augment_holiday_signature",
    "section": "",
    "text": "augment_holiday_signature(data, date_column, country_name='UnitedStates')\nEngineers 4 different holiday features from a single datetime for 80+ countries.\nNote: Requires the holidays package to be installed. See https://pypi.org/project/holidays/ for more information."
  },
  {
    "objectID": "reference/augment_holiday_signature.html#parameters",
    "href": "reference/augment_holiday_signature.html#parameters",
    "title": "augment_holiday_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nUnion[pd.DataFrame, pd.core.groupby.generic.DataFrameGroupBy]\nThe input DataFrame.\nrequired\n\n\ndate_column\nstr\nThe name of the datetime-like column in the DataFrame.\nrequired\n\n\ncountry_name\nstr\nThe name of the country for which to generate holiday features. Defaults to United States holidays, but the following countries are currently available and accessible by the full name or ISO code: Any of the following are acceptable keys for country_name: Available Countries: Full Country, Abrv. #1, #2, #3 Angola: Angola, AO, AGO, Argentina: Argentina, AR, ARG, Aruba: Aruba, AW, ABW, Australia: Australia, AU, AUS, Austria: Austria, AT, AUT, Bangladesh: Bangladesh, BD, BGD, Belarus: Belarus, BY, BLR, Belgium: Belgium, BE, BEL, Botswana: Botswana, BW, BWA, Brazil: Brazil, BR, BRA, Bulgaria: Bulgaria, BG, BLG, Burundi: Burundi, BI, BDI, Canada: Canada, CA, CAN, Chile: Chile, CL, CHL, Colombia: Colombia, CO, COL, Croatia: Croatia, HR, HRV, Curacao: Curacao, CW, CUW, Czechia: Czechia, CZ, CZE, Denmark: Denmark, DK, DNK, Djibouti: Djibouti, DJ, DJI, Dominican Republic: DominicanRepublic, DO, DOM, Egypt: Egypt, EG, EGY, England: England, Estonia: Estonia, EE, EST, European Central Bank: EuropeanCentralBank, Finland: Finland, FI, FIN, France: France, FR, FRA, Georgia: Georgia, GE, GEO, Germany: Germany, DE, DEU, Greece: Greece, GR, GRC, Honduras: Honduras, HN, HND, Hong Kong: HongKong, HK, HKG, Hungary: Hungary, HU, HUN, Iceland: Iceland, IS, ISL, India: India, IN, IND, Ireland: Ireland, IE, IRL, Isle Of Man: IsleOfMan, Israel: Israel, IL, ISR, Italy: Italy, IT, ITA, Jamaica: Jamaica, JM, JAM, Japan: Japan, JP, JPN, Kenya: Kenya, KE, KEN, Korea: Korea, KR, KOR, Latvia: Latvia, LV, LVA, Lithuania: Lithuania, LT, LTU, Luxembourg: Luxembourg, LU, LUX, Malaysia: Malaysia, MY, MYS, Malawi: Malawi, MW, MWI, Mexico: Mexico, MX, MEX, Morocco: Morocco, MA, MOR, Mozambique: Mozambique, MZ, MOZ, Netherlands: Netherlands, NL, NLD, NewZealand: NewZealand, NZ, NZL, Nicaragua: Nicaragua, NI, NIC, Nigeria: Nigeria, NG, NGA, Northern Ireland: NorthernIreland, Norway: Norway, NO, NOR, Paraguay: Paraguay, PY, PRY, Peru: Peru, PE, PER, Poland: Poland, PL, POL, Portugal: Portugal, PT, PRT, Portugal Ext: PortugalExt, PTE, Romania: Romania, RO, ROU, Russia: Russia, RU, RUS, Saudi Arabia: SaudiArabia, SA, SAU, Scotland: Scotland, Serbia: Serbia, RS, SRB, Singapore: Singapore, SG, SGP, Slovokia: Slovokia, SK, SVK, Slovenia: Slovenia, SI, SVN, South Africa: SouthAfrica, ZA, ZAF, Spain: Spain, ES, ESP, Sweden: Sweden, SE, SWE, Switzerland: Switzerland, CH, CHE, Turkey: Turkey, TR, TUR, Ukraine: Ukraine, UA, UKR, United Arab Emirates: UnitedArabEmirates, AE, ARE, United Kingdom: UnitedKingdom, GB, GBR, UK, United States: UnitedStates, US, USA, Venezuela: Venezuela, YV, VEN, Vietnam: Vietnam, VN, VNM, Wales: Wales\n'UnitedStates'"
  },
  {
    "objectID": "reference/augment_holiday_signature.html#returns",
    "href": "reference/augment_holiday_signature.html#returns",
    "title": "augment_holiday_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame:\nA pandas DataFrame with three holiday-specific features: - is_holiday: (0, 1) indicator for holiday - before_holiday: (0, 1) indicator for day before holiday - after_holiday: (0, 1) indicator for day after holiday - holiday_name: name of the holiday"
  },
  {
    "objectID": "reference/augment_holiday_signature.html#example",
    "href": "reference/augment_holiday_signature.html#example",
    "title": "augment_holiday_signature",
    "section": "Example",
    "text": "Example\n\nimport pandas as pd\nimport pytimetk as tk\n\n# Make a DataFrame with a date column\nstart_date = '2023-01-01'\nend_date = '2023-01-10'\ndf = pd.DataFrame(pd.date_range(start=start_date, end=end_date), columns=['date'])\n\n# Add holiday features for US\ntk.augment_holiday_signature(df, 'date', 'UnitedStates')\n\n\n\n\n\n\n\n\ndate\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n1\n0\nNew Year's Day\n\n\n1\n2023-01-02\n1\n0\n1\nNew Year's Day (Observed)\n\n\n2\n2023-01-03\n0\n0\n1\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN\n\n\n\n\n\n\n\n\n# Add holiday features for France\ntk.augment_holiday_signature(df, 'date', 'France')\n\n\n\n\n\n\n\n\ndate\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n0\n0\nNew Year's Day\n\n\n1\n2023-01-02\n0\n0\n1\nNaN\n\n\n2\n2023-01-03\n0\n0\n0\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PyTimeTK ",
    "section": "",
    "text": "The Time Series Toolkit for Python\n\nPyTimetk’s Mission: To make time series analysis easier, faster, and more enjoyable in Python.\n\n\n1 Installation\nInstall the Latest Stable Version:\npip install pytimetk\nAlternatively, install the Development GitHub Version:\npip install git+https://github.com/business-science/pytimetk.git\n\n\n2 Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import pytimetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport pytimetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format = 'False' to return the dataframe in a long format (Note long format is the default).\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        freq         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = False\n    )\n\n# First 5 rows shown\nsummary_category_1_df.head()\n\n\n\n\n\n\n\n\ncategory_1\norder_date\ntotal_price\n\n\n\n\n0\nMountain\n2011-01-01\n221490\n\n\n1\nMountain\n2011-02-01\n660555\n\n\n2\nMountain\n2011-03-01\n358855\n\n\n3\nMountain\n2011-04-01\n1075975\n\n\n4\nMountain\n2011-05-01\n450440\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nNow available: plot_timeseries().\n\n\n\n\n\nPlot time series is a quick and easy way to visualize time series and make professional time series plots.\n\n\n\nWith the data summarized by time, we can visualize with plot_timeseries(). pytimetk functions are groupby() aware meaning they understand if your data is grouped to do things by group. This is useful in time series where we often deal with 100s of time series groups.\n\nsummary_category_1_df \\\n    .groupby('category_1') \\\n    .plot_timeseries(\n        date_column  = 'order_date',\n        value_column = 'total_price',\n        smooth_frac  = 0.8\n    )\n\n\n                                                \n\n\n\n\n\n3 Contributing\nInterested in helping us make this the best Python package for time series analysis? We’d love your help.\nFollow these instructions to Contribute.\n\n\n4 More Coming Soon…\nWe are in the early stages of development. But it’s obvious the potential for pytimetk now in Python. 🐍\n\nPlease ⭐ us on GitHub (it takes 2-seconds and means a lot).\nTo make requests, please see our Project Roadmap GH Issue #2. You can make requests there.\nWant to contribute? See our contributing guide here."
  },
  {
    "objectID": "guides/04_wrangling.html",
    "href": "guides/04_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "This section will cover data wrangling for timeseries using pytimetk. We’ll show examples for the following functions:"
  },
  {
    "objectID": "guides/04_wrangling.html#basic-example",
    "href": "guides/04_wrangling.html#basic-example",
    "title": "Data Wrangling",
    "section": "1.1 Basic Example",
    "text": "1.1 Basic Example\nThe m4_daily dataset has a daily frequency. Say we are interested in forecasting at the weekly level. We can use summarize_by_time() to aggregate to a weekly level\n\n# summarize by time: daily to weekly\nsummarized_df = m4_daily_df \\\n    .summarize_by_time(\n        date_column  = 'date',\n        value_column = 'value',\n        freq         = 'W',\n        agg_func     = 'sum'\n    )\n\nprint(summarized_df.head())\nprint('\\nLength of the full dataset:', len(summarized_df))\n\n        date     value\n0 1978-06-25  27328.12\n1 1978-07-02  63621.88\n2 1978-07-09  63334.38\n3 1978-07-16  63737.51\n4 1978-07-23  64718.76\n\nLength of the full dataset: 1977\n\n\nThe data has now been aggregated at the weekly level. Notice we now have 1977 rows, compared to full dataset which had 9743 rows."
  },
  {
    "objectID": "guides/04_wrangling.html#additional-aggregate-functions",
    "href": "guides/04_wrangling.html#additional-aggregate-functions",
    "title": "Data Wrangling",
    "section": "1.2 Additional Aggregate Functions",
    "text": "1.2 Additional Aggregate Functions\nsummarize_by_time() can take additional aggregate functions in the agg_func argument.\n\n# summarize by time with additional aggregate functions\nsummarized_multiple_agg_df = m4_daily_df \\\n    .summarize_by_time(\n        date_column  = 'date',\n        value_column = 'value',\n        freq         = 'W',\n        agg_func     = ['sum', 'min', 'max']\n    )\n\nsummarized_multiple_agg_df.head()\n\n\n\n\n\n\n\n\ndate\nvalue_sum\nvalue_min\nvalue_max\n\n\n\n\n0\n1978-06-25\n27328.12\n9103.12\n9115.62\n\n\n1\n1978-07-02\n63621.88\n9046.88\n9115.62\n\n\n2\n1978-07-09\n63334.38\n9028.12\n9096.88\n\n\n3\n1978-07-16\n63737.51\n9075.00\n9146.88\n\n\n4\n1978-07-23\n64718.76\n9171.88\n9315.62"
  },
  {
    "objectID": "guides/04_wrangling.html#summarize-by-time-with-grouped-time-series",
    "href": "guides/04_wrangling.html#summarize-by-time-with-grouped-time-series",
    "title": "Data Wrangling",
    "section": "1.3 Summarize by Time with Grouped Time Series",
    "text": "1.3 Summarize by Time with Grouped Time Series\nsummarize_by_time() also works with groups.\n\n# summarize by time with groups and additional aggregate functions\ngrouped_summarized_df = (\n    m4_daily_df\n        .groupby('id')\n        .summarize_by_time(\n            date_column  = 'date',\n            value_column = 'value',\n            freq         = 'W',\n            agg_func     = [\n                'sum',\n                'min',\n                ('q25', lambda x: np.quantile(x, 0.25)),\n                'median',\n                ('q75', lambda x: np.quantile(x, 0.75)),\n                'max'\n            ],\n        )\n)\n\ngrouped_summarized_df.head()\n\n\n\n\n\n\n\n\nid\ndate\nvalue_sum\nvalue_min\nvalue_q25\nvalue_median\nvalue_q75\nvalue_max\n\n\n\n\n0\nD10\n2014-07-06\n8247.2\n2048.7\n2048.85\n2061.15\n2074.10\n2076.2\n\n\n1\nD10\n2014-07-13\n14040.8\n1978.8\n2003.95\n2007.40\n2013.80\n2019.1\n\n\n2\nD10\n2014-07-20\n13867.6\n1943.0\n1955.30\n1988.30\n2005.60\n2014.5\n\n\n3\nD10\n2014-07-27\n13266.3\n1876.0\n1887.15\n1891.00\n1895.85\n1933.3\n\n\n4\nD10\n2014-08-03\n13471.2\n1886.2\n1914.60\n1920.00\n1939.55\n1956.7"
  },
  {
    "objectID": "guides/04_wrangling.html#basic-example-1",
    "href": "guides/04_wrangling.html#basic-example-1",
    "title": "Data Wrangling",
    "section": "2.1 Basic Example",
    "text": "2.1 Basic Example\nWe’ll continue with our use of the m4_daily_df dataset. Recall we’ve alread aggregated at the weekly level (summarized_df). Lets checkout the last week in the summarized_df:\n\n# last week in dataset\nsummarized_df \\\n    .sort_values(by = 'date', ascending = True) \\\n    .iloc[: -1] \\\n    .tail(1)\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n1975\n2016-05-01\n17959.8\n\n\n\n\n\n\n\n\n\n\n\n\n\niloc()\n\n\n\n\n\niloc[: -1] is used to filter out the last row and keep only dates that are the start of the week.\n\n\n\nWe can see that the last week is the week of 2016-05-01. Now say we wanted to forecast the next 8 weeks. We can extend the dataset beyound the week of 2016-05-01:\n\n# extend dataset by 12 weeks\nsummarized_extended_df = summarized_df \\\n    .future_frame(\n        date_column = 'date',\n        length_out  = 8\n    )\n\nsummarized_extended_df\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n1978-06-25\n27328.12\n\n\n1\n1978-07-02\n63621.88\n\n\n2\n1978-07-09\n63334.38\n\n\n3\n1978-07-16\n63737.51\n\n\n4\n1978-07-23\n64718.76\n\n\n...\n...\n...\n\n\n1980\n2016-06-05\nNaN\n\n\n1981\n2016-06-12\nNaN\n\n\n1982\n2016-06-19\nNaN\n\n\n1983\n2016-06-26\nNaN\n\n\n1984\n2016-07-03\nNaN\n\n\n\n\n1985 rows × 2 columns\n\n\n\nTo get only the future data, we can filter the dataset for where value is missing (np.nan).\n\n# get only future data\nsummarized_extended_df \\\n    .query('value.isna()')\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n1977\n2016-05-15\nNaN\n\n\n1978\n2016-05-22\nNaN\n\n\n1979\n2016-05-29\nNaN\n\n\n1980\n2016-06-05\nNaN\n\n\n1981\n2016-06-12\nNaN\n\n\n1982\n2016-06-19\nNaN\n\n\n1983\n2016-06-26\nNaN\n\n\n1984\n2016-07-03\nNaN"
  },
  {
    "objectID": "guides/04_wrangling.html#future-frame-with-grouped-time-series",
    "href": "guides/04_wrangling.html#future-frame-with-grouped-time-series",
    "title": "Data Wrangling",
    "section": "2.2 Future Frame with Grouped Time Series",
    "text": "2.2 Future Frame with Grouped Time Series\nfuture_frame() also works for grouped time series. We can see an example using our grouped summarized dataset (grouped_summarized_df) from earlier:\n\n# future frame with grouped time series\ngrouped_summarized_df[['id', 'date', 'value_sum']] \\\n    .groupby('id') \\\n    .future_frame(\n        date_column = 'date',\n        length_out  = 8\n    ) \\\n    .query('value_sum.isna()') # filtering to return only the future data\n\n\n\n\n\n\n\n\nid\ndate\nvalue_sum\n\n\n\n\n97\nD10\n2016-05-15\nNaN\n\n\n98\nD10\n2016-05-22\nNaN\n\n\n99\nD10\n2016-05-29\nNaN\n\n\n100\nD10\n2016-06-05\nNaN\n\n\n101\nD10\n2016-06-12\nNaN\n\n\n102\nD10\n2016-06-19\nNaN\n\n\n103\nD10\n2016-06-26\nNaN\n\n\n104\nD10\n2016-07-03\nNaN\n\n\n600\nD160\n2011-07-10\nNaN\n\n\n601\nD160\n2011-07-17\nNaN\n\n\n602\nD160\n2011-07-24\nNaN\n\n\n603\nD160\n2011-07-31\nNaN\n\n\n604\nD160\n2011-08-07\nNaN\n\n\n605\nD160\n2011-08-14\nNaN\n\n\n606\nD160\n2011-08-21\nNaN\n\n\n607\nD160\n2011-08-28\nNaN\n\n\n98\nD410\n1980-05-11\nNaN\n\n\n99\nD410\n1980-05-18\nNaN\n\n\n100\nD410\n1980-05-25\nNaN\n\n\n101\nD410\n1980-06-01\nNaN\n\n\n102\nD410\n1980-06-08\nNaN\n\n\n103\nD410\n1980-06-15\nNaN\n\n\n104\nD410\n1980-06-22\nNaN\n\n\n105\nD410\n1980-06-29\nNaN\n\n\n600\nD500\n2012-09-30\nNaN\n\n\n601\nD500\n2012-10-07\nNaN\n\n\n602\nD500\n2012-10-14\nNaN\n\n\n603\nD500\n2012-10-21\nNaN\n\n\n604\nD500\n2012-10-28\nNaN\n\n\n605\nD500\n2012-11-04\nNaN\n\n\n606\nD500\n2012-11-11\nNaN\n\n\n607\nD500\n2012-11-18\nNaN"
  },
  {
    "objectID": "guides/04_wrangling.html#basic-example-2",
    "href": "guides/04_wrangling.html#basic-example-2",
    "title": "Data Wrangling",
    "section": "3.1 Basic Example",
    "text": "3.1 Basic Example\nLet’s start with a basic example to see how pad_by_time() works. We’ll create some sample data with missing timestamps:\n\n# libraries\nimport pytimetk as tk\nimport pandas as pd\nimport numpy as np\n\n# sample quarterly data with missing timestamp for Q3\ndates = pd.to_datetime([\"2021-01-01\", \"2021-04-01\", \"2021-10-01\"])\nvalue = range(len(dates))\n\ndf = pd.DataFrame({\n    'date': dates,\n    'value': range(len(dates))\n})\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2021-01-01\n0\n\n\n1\n2021-04-01\n1\n\n\n2\n2021-10-01\n2\n\n\n\n\n\n\n\nNow we can use pad_by_time() to fill in the missing timestamp:\n\n# pad by time\ndf \\\n    .pad_by_time(\n        date_column = 'date',\n        freq        = 'QS' # specifying quarter start frequency\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2021-01-01\n0.0\n\n\n1\n2021-04-01\n1.0\n\n\n2\n2021-07-01\nNaN\n\n\n3\n2021-10-01\n2.0\n\n\n\n\n\n\n\nWe can also specify shorter time frequency:\n\n# pad by time with shorter frequency\ndf \\\n    .pad_by_time(\n        date_column = 'date',\n        freq        = 'MS' # specifying month start frequency\n    ) \\\n    .assign(value = lambda x: x['value'].fillna(0)) # replace NaN with 0\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2021-01-01\n0.0\n\n\n1\n2021-02-01\n0.0\n\n\n2\n2021-03-01\n0.0\n\n\n3\n2021-04-01\n1.0\n\n\n4\n2021-05-01\n0.0\n\n\n5\n2021-06-01\n0.0\n\n\n6\n2021-07-01\n0.0\n\n\n7\n2021-08-01\n0.0\n\n\n8\n2021-09-01\n0.0\n\n\n9\n2021-10-01\n2.0"
  },
  {
    "objectID": "guides/04_wrangling.html#pad-by-time-with-grouped-time-series",
    "href": "guides/04_wrangling.html#pad-by-time-with-grouped-time-series",
    "title": "Data Wrangling",
    "section": "3.2 Pad by Time with Grouped Time Series",
    "text": "3.2 Pad by Time with Grouped Time Series\npad_by_time() can also be used with grouped time series. Let’s use the stocks_daily dataset to showcase an example:\n\n# load dataset\nstocks_df = tk.load_dataset('stocks_daily', parse_dates = ['date'])\n\n# pad by time\nstocks_df \\\n    .groupby('symbol') \\\n    .pad_by_time(\n        date_column = 'date',\n        freq        = 'D'\n    ) \\\n    .assign(id = lambda x: x['symbol'].ffill())\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\nid\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\nAAPL\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\nAAPL\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\nAAPL\n\n\n3\n2013-01-05\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n4\n2013-01-06\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n23485\n2023-09-17\nNVDA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNVDA\n\n\n23486\n2023-09-18\nNVDA\n427.480011\n442.420013\n420.000000\n439.660004\n50027100.0\n439.660004\nNVDA\n\n\n23487\n2023-09-19\nNVDA\n438.329987\n439.660004\n430.019989\n435.200012\n37306400.0\n435.200012\nNVDA\n\n\n23488\n2023-09-20\nNVDA\n436.000000\n439.029999\n422.230011\n422.390015\n36710800.0\n422.390015\nNVDA\n\n\n23489\n2023-09-21\nNVDA\n415.829987\n421.000000\n409.799988\n410.170013\n44893000.0\n410.170013\nNVDA\n\n\n\n\n23490 rows × 9 columns\n\n\n\nTo replace NaN with 0 in a dataframe with multiple columns:\n\nfrom functools import partial\n\n# columns to replace NaN with 0\ncols_to_fill = ['open', 'high', 'low', 'close', 'volume', 'adjusted']\n\n# define a function to fillna\ndef fill_na_col(df, col):\n    return df[col].fillna(0)\n\n# pad by time and replace NaN with 0\nstocks_df \\\n    .groupby('symbol') \\\n    .pad_by_time(\n        date_column = 'date',\n        freq        = 'D'\n    ) \\\n    .assign(id = lambda x: x['symbol'].ffill()) \\\n    .assign(**{col: partial(fill_na_col, col=col) for col in cols_to_fill})\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\nid\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\nAAPL\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\nAAPL\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\nAAPL\n\n\n3\n2013-01-05\nAAPL\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\nAAPL\n\n\n4\n2013-01-06\nAAPL\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\nAAPL\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n23485\n2023-09-17\nNVDA\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\nNVDA\n\n\n23486\n2023-09-18\nNVDA\n427.480011\n442.420013\n420.000000\n439.660004\n50027100.0\n439.660004\nNVDA\n\n\n23487\n2023-09-19\nNVDA\n438.329987\n439.660004\n430.019989\n435.200012\n37306400.0\n435.200012\nNVDA\n\n\n23488\n2023-09-20\nNVDA\n436.000000\n439.029999\n422.230011\n422.390015\n36710800.0\n422.390015\nNVDA\n\n\n23489\n2023-09-21\nNVDA\n415.829987\n421.000000\n409.799988\n410.170013\n44893000.0\n410.170013\nNVDA\n\n\n\n\n23490 rows × 9 columns"
  },
  {
    "objectID": "guides/02_timetk_concepts.html",
    "href": "guides/02_timetk_concepts.html",
    "title": "PyTimeTK Basics",
    "section": "",
    "text": "PyTimeTK has one mission: To make time series analysis simpler, easier, and faster in Python. This goal requires some opinionated ways of treating time series in Python. We will conceptually lay out how pytimetk can help.\nLet’s first start with how to think about time series data conceptually. Time series data has 3 core properties."
  },
  {
    "objectID": "guides/02_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "href": "guides/02_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "title": "PyTimeTK Basics",
    "section": "2.1 Type 1: Pandas DataFrame Operations",
    "text": "2.1 Type 1: Pandas DataFrame Operations\nBefore we start using pytimetk, let’s make sure our data is set up properly.\n\nTimetk Data Format Compliance\n\n\n\n\n\n\n3 Core Properties Must Be Upheald\n\n\n\n\n\nA pytimetk-Compliant Pandas DataFrame must have:\n\nTime Series Index: A Time Stamp column containing datetime64 values\nValue Column(s): The value column(s) containing float or int values\nGroup Column(s): Optionally for grouped time series analysis, one or more columns containg str or categorical values (shown as an object)\n\nIf these are NOT upheld, this will impact your ability to use pytimetk DataFrame operations.\n\n\n\n\n\n\n\n\n\nInspect the DataFrame\n\n\n\n\n\nUse Pandas info() method to check compliance.\n\n\n\nUsing pandas info() method, we can see that we have a compliant data frame with a date column containing datetime64 and a value column containing float64. For grouped analysis we have the id column containing object dtype.\n\n# Tip: Inspect for compliance with info()\nm4_daily_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9743 entries, 0 to 9742\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   id      9743 non-null   object        \n 1   date    9743 non-null   datetime64[ns]\n 2   value   9743 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 228.5+ KB\n\n\n\n\nGrouped Time Series Analysis with Summarize By Time\nFirst, inspect how the summarize_by_time function works by calling help().\n\n# Review the summarize_by_time documentation (output not shown)\nhelp(tk.summarize_by_time)\n\n\n\n\n\n\n\nHelp Doc Info: summarize_by_time()\n\n\n\n\n\n\nThe first parameter is data, indicating this is a DataFrame operation.\nThe Examples show different use cases for how to apply the function on a DataFrame\n\n\n\n\nLet’s test the summarize_by_time() DataFrame operation out using the grouped approach with method chaining. DataFrame operations can be used as Pandas methods with method-chaining, which allows us to more succinctly apply time series operations.\n\n# Grouped Summarize By Time with Method Chaining\ndf_summarized = (\n    m4_daily_df\n        .groupby('id')\n        .summarize_by_time(\n            date_column  = 'date',\n            value_column = 'value',\n            freq         = 'QS', # QS = Quarter Start\n            agg_func     = [\n                'mean', \n                'median', \n                'min',\n                ('q25', lambda x: np.quantile(x, 0.25)),\n                ('q75', lambda x: np.quantile(x, 0.75)),\n                'max',\n                ('range',lambda x: x.max() - x.min()),\n            ],\n        )\n)\n\ndf_summarized\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_q25\nvalue_q75\nvalue_max\nvalue_range\n\n\n\n\n0\nD10\n2014-07-01\n1960.078889\n1979.90\n1781.6\n1915.225\n2002.575\n2076.2\n294.6\n\n\n1\nD10\n2014-10-01\n2184.586957\n2154.05\n2022.8\n2125.075\n2274.150\n2344.9\n322.1\n\n\n2\nD10\n2015-01-01\n2309.830000\n2312.30\n2209.6\n2284.575\n2342.150\n2392.4\n182.8\n\n\n3\nD10\n2015-04-01\n2344.481319\n2333.00\n2185.1\n2301.750\n2391.000\n2499.8\n314.7\n\n\n4\nD10\n2015-07-01\n2156.754348\n2186.70\n1856.6\n1997.250\n2289.425\n2368.1\n511.5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n105\nD500\n2011-07-01\n9727.321739\n9745.55\n8964.5\n9534.125\n10003.900\n10463.9\n1499.4\n\n\n106\nD500\n2011-10-01\n8175.565217\n7897.00\n6755.0\n7669.875\n8592.575\n9860.0\n3105.0\n\n\n107\nD500\n2012-01-01\n8291.317582\n8412.60\n7471.5\n7814.800\n8677.850\n8980.7\n1509.2\n\n\n108\nD500\n2012-04-01\n8654.020879\n8471.10\n8245.6\n8389.850\n9017.250\n9349.2\n1103.6\n\n\n109\nD500\n2012-07-01\n8770.502353\n8690.50\n8348.1\n8604.400\n8846.000\n9545.3\n1197.2\n\n\n\n\n110 rows × 9 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: summarize_by_time()\n\n\n\n\n\n\nThe data must comply with the 3 core properties (date column, value column(s), and group column(s))\nThe aggregation functions were applied by combination of group (id) and resample (Quarter Start)\nThe result was a pandas DataFrame with group column, resampled date column, and summary values (mean, median, min, 25th-quantile, etc)\n\n\n\n\n\n\nAnother DataFrame Example: Creating 29 Engineered Features\nLet’s examine another DataFrame function, tk.augment_timeseries_signature(). Feel free to inspect the documentation with help(tk.augment_timeseries_signature).\n\n# Creating 29 engineered features from the date column\n# Not run: help(tk.augment_timeseries_signature)\ndf_augmented = (\n    m4_daily_df\n        .augment_timeseries_signature(date_column = 'date')\n)\n\ndf_augmented.head()\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: augment_timeseries_signature()\n\n\n\n\n\n\nThe data must comply with the 1 of the 3 core properties (date column)\nThe result was a pandas DataFrame with 29 time series features that can be used for Machine Learning and Forecasting\n\n\n\n\n\n\nMaking Future Dates with Future Frame\nA common time series task before forecasting with machine learning models is to make a future DataFrame some length_out into the future. You can do this with tk.future_frame(). Here’s how.\n\n# Preparing a time series data set for Machine Learning Forecasting\nfull_augmented_df = (\n    m4_daily_df \n        .groupby('id')\n        .future_frame('date', length_out = 365)\n        .augment_timeseries_signature('date')\n)\nfull_augmented_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4556\nD500\n2013-09-19\nNaN\n1379548800\n2013\n2013\n0\n0\n0\n2\n...\n19\n81\n262\n0\n0\n0\n0\n0\n0\nam\n\n\n4557\nD500\n2013-09-20\nNaN\n1379635200\n2013\n2013\n0\n0\n0\n2\n...\n20\n82\n263\n0\n0\n0\n0\n0\n0\nam\n\n\n4558\nD500\n2013-09-21\nNaN\n1379721600\n2013\n2013\n0\n0\n0\n2\n...\n21\n83\n264\n0\n0\n0\n0\n0\n0\nam\n\n\n4559\nD500\n2013-09-22\nNaN\n1379808000\n2013\n2013\n0\n0\n0\n2\n...\n22\n84\n265\n1\n0\n0\n0\n0\n0\nam\n\n\n4560\nD500\n2013-09-23\nNaN\n1379894400\n2013\n2013\n0\n0\n0\n2\n...\n23\n85\n266\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n11203 rows × 32 columns\n\n\n\nWe can then get the future data by keying in on the data with value column that is missing (np.nan).\n\n# Get the future data (just the observations that haven't happened yet)\nfuture_df = (\n    full_augmented_df\n        .query('value.isna()')\n)\nfuture_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n674\nD10\n2016-05-07\nNaN\n1462579200\n2016\n2016\n0\n0\n1\n1\n...\n7\n37\n128\n0\n0\n0\n0\n0\n0\nam\n\n\n675\nD10\n2016-05-08\nNaN\n1462665600\n2016\n2016\n0\n0\n1\n1\n...\n8\n38\n129\n1\n0\n0\n0\n0\n0\nam\n\n\n676\nD10\n2016-05-09\nNaN\n1462752000\n2016\n2016\n0\n0\n1\n1\n...\n9\n39\n130\n0\n0\n0\n0\n0\n0\nam\n\n\n677\nD10\n2016-05-10\nNaN\n1462838400\n2016\n2016\n0\n0\n1\n1\n...\n10\n40\n131\n0\n0\n0\n0\n0\n0\nam\n\n\n678\nD10\n2016-05-11\nNaN\n1462924800\n2016\n2016\n0\n0\n1\n1\n...\n11\n41\n132\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4556\nD500\n2013-09-19\nNaN\n1379548800\n2013\n2013\n0\n0\n0\n2\n...\n19\n81\n262\n0\n0\n0\n0\n0\n0\nam\n\n\n4557\nD500\n2013-09-20\nNaN\n1379635200\n2013\n2013\n0\n0\n0\n2\n...\n20\n82\n263\n0\n0\n0\n0\n0\n0\nam\n\n\n4558\nD500\n2013-09-21\nNaN\n1379721600\n2013\n2013\n0\n0\n0\n2\n...\n21\n83\n264\n0\n0\n0\n0\n0\n0\nam\n\n\n4559\nD500\n2013-09-22\nNaN\n1379808000\n2013\n2013\n0\n0\n0\n2\n...\n22\n84\n265\n1\n0\n0\n0\n0\n0\nam\n\n\n4560\nD500\n2013-09-23\nNaN\n1379894400\n2013\n2013\n0\n0\n0\n2\n...\n23\n85\n266\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n1460 rows × 32 columns"
  },
  {
    "objectID": "guides/02_timetk_concepts.html#type-2-pandas-series-operations",
    "href": "guides/02_timetk_concepts.html#type-2-pandas-series-operations",
    "title": "PyTimeTK Basics",
    "section": "2.2 Type 2: Pandas Series Operations",
    "text": "2.2 Type 2: Pandas Series Operations\nThe main difference between a DataFrame operation and a Series operation is that we are operating on an array of values from typically one of the following dtypes:\n\nTimestamps (datetime64)\nNumeric (float64 or int64)\n\nThe first argument of Series operations that operate on Timestamps will always be idx.\nLet’s take a look at one shall we? We’ll start with a common action: Making future time series from an existing time series with a regular frequency.\n\nThe Make Future Time Series Function\nSay we have a monthly sequence of timestamps. What if we want to create a forecast where we predict 12 months into the future? Well, we will need to create 12 future timestamps. Here’s how.\nFirst create a pd.date_range() with dates starting at the beginning of each month.\n\n# Make a monthly date range\ndates_dt = pd.date_range(\"2023-01\", \"2024-01\", freq=\"MS\")\ndates_dt\n\nDatetimeIndex(['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01',\n               '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01',\n               '2023-09-01', '2023-10-01', '2023-11-01', '2023-12-01',\n               '2024-01-01'],\n              dtype='datetime64[ns]', freq='MS')\n\n\nNext, use tk.make_future_timeseries() to create the next 12 timestamps in the sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas Series: Future Dates\nfuture_series = pd.Series(dates_dt).make_future_timeseries(12)\nfuture_series\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\n# DateTimeIndex: Future Dates\nfuture_dt = tk.make_future_timeseries(\n    idx      = dates_dt,\n    length_out = 12\n)\nfuture_dt\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\nWe can combine the actual and future timestamps into one combined timeseries.\n\n# Combining the 2 series and resetting the index\ncombined_timeseries = (\n    pd.concat(\n        [pd.Series(dates_dt), pd.Series(future_dt)],\n        axis=0\n    )\n        .reset_index(drop = True)\n)\n\ncombined_timeseries\n\n0    2023-01-01\n1    2023-02-01\n2    2023-03-01\n3    2023-04-01\n4    2023-05-01\n5    2023-06-01\n6    2023-07-01\n7    2023-08-01\n8    2023-09-01\n9    2023-10-01\n10   2023-11-01\n11   2023-12-01\n12   2024-01-01\n13   2024-02-01\n14   2024-03-01\n15   2024-04-01\n16   2024-05-01\n17   2024-06-01\n18   2024-07-01\n19   2024-08-01\n20   2024-09-01\n21   2024-10-01\n22   2024-11-01\n23   2024-12-01\n24   2025-01-01\ndtype: datetime64[ns]\n\n\nNext, we’ll take a look at how to go from an irregular time series to a regular time series.\n\n\nFlooring Dates\nAn example is tk.floor_date, which is used to round down dates. See help(tk.floor_date).\nFlooring dates is often used as part of a strategy to go from an irregular time series to regular by combining with an aggregation. Often summarize_by_time() is used (I’ll share why shortly). But conceptually, date flooring is the secret.\n\nWith FlooringWithout Flooring\n\n\n\n# Monthly flooring rounds dates down to 1st of the month\nm4_daily_df['date'].floor_date(unit = \"M\")\n\n0      2014-07-01\n1      2014-07-01\n2      2014-07-01\n3      2014-07-01\n4      2014-07-01\n          ...    \n9738   2012-09-01\n9739   2012-09-01\n9740   2012-09-01\n9741   2012-09-01\n9742   2012-09-01\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\n# Before Flooring\nm4_daily_df['date']\n\n0      2014-07-03\n1      2014-07-04\n2      2014-07-05\n3      2014-07-06\n4      2014-07-07\n          ...    \n9738   2012-09-19\n9739   2012-09-20\n9740   2012-09-21\n9741   2012-09-22\n9742   2012-09-23\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\nThis “date flooring” operation can be useful for creating date groupings.\n\n# Adding a date group with floor_date()\ndates_grouped_by_month = (\n    m4_daily_df\n        .assign(date_group = lambda x: x['date'].floor_date(\"M\"))\n)\n\ndates_grouped_by_month\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_group\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2014-07-01\n\n\n1\nD10\n2014-07-04\n2073.4\n2014-07-01\n\n\n2\nD10\n2014-07-05\n2048.7\n2014-07-01\n\n\n3\nD10\n2014-07-06\n2048.9\n2014-07-01\n\n\n4\nD10\n2014-07-07\n2006.4\n2014-07-01\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n2012-09-01\n\n\n9739\nD500\n2012-09-20\n9365.7\n2012-09-01\n\n\n9740\nD500\n2012-09-21\n9445.9\n2012-09-01\n\n\n9741\nD500\n2012-09-22\n9497.9\n2012-09-01\n\n\n9742\nD500\n2012-09-23\n9545.3\n2012-09-01\n\n\n\n\n9743 rows × 4 columns\n\n\n\nWe can then do grouped operations.\n\n# Example of a grouped operation with floored dates\nsummary_df = (\n    dates_grouped_by_month\n        .drop('date', axis=1) \\\n        .groupby(['id', 'date_group'])\n        .mean() \\\n        .reset_index()\n)\n\nsummary_df\n\n\n\n\n\n\n\n\nid\ndate_group\nvalue\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n\n\n1\nD10\n2014-08-01\n1985.548387\n\n\n2\nD10\n2014-09-01\n1926.593333\n\n\n3\nD10\n2014-10-01\n2100.077419\n\n\n4\nD10\n2014-11-01\n2155.326667\n\n\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n\n\n319\nD500\n2012-06-01\n9124.903333\n\n\n320\nD500\n2012-07-01\n8674.551613\n\n\n321\nD500\n2012-08-01\n8666.054839\n\n\n322\nD500\n2012-09-01\n9040.604348\n\n\n\n\n323 rows × 3 columns\n\n\n\nOf course for this operation, we can do it faster with summarize_by_time() (and it’s much more flexible).\n\n# Summarize by time is less code and more flexible\n(\n    m4_daily_df \n        .groupby('id')\n        .summarize_by_time(\n            'date', 'value', \n            freq = \"MS\",\n            agg_func = ['mean', 'median', 'min', 'max']\n        )\n)\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_max\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n1978.80\n1876.0\n2076.2\n\n\n1\nD10\n2014-08-01\n1985.548387\n1995.60\n1914.7\n2027.5\n\n\n2\nD10\n2014-09-01\n1926.593333\n1920.95\n1781.6\n2023.5\n\n\n3\nD10\n2014-10-01\n2100.077419\n2107.60\n2022.8\n2154.9\n\n\n4\nD10\n2014-11-01\n2155.326667\n2149.30\n2083.5\n2245.4\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n8430.80\n8245.6\n8578.1\n\n\n319\nD500\n2012-06-01\n9124.903333\n9163.85\n8686.1\n9349.2\n\n\n320\nD500\n2012-07-01\n8674.551613\n8673.60\n8407.5\n9091.1\n\n\n321\nD500\n2012-08-01\n8666.054839\n8667.40\n8348.1\n8939.6\n\n\n322\nD500\n2012-09-01\n9040.604348\n9091.40\n8500.0\n9545.3\n\n\n\n\n323 rows × 6 columns\n\n\n\nAnd that’s the core idea behind pytimetk, writing less code and getting more.\nNext, let’s do one more function. The brother of augment_timeseries_signature()…\n\n\nThe Get Time Series Signature Function\nThis function takes a pandas Series or DateTimeIndex and returns a DataFrame containing the 29 engineered features.\nStart with either a DateTimeIndex…\n\ntimestamps_dt = pd.date_range(\"2023\", \"2024\", freq = \"D\")\ntimestamps_dt\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10',\n               ...\n               '2023-12-23', '2023-12-24', '2023-12-25', '2023-12-26',\n               '2023-12-27', '2023-12-28', '2023-12-29', '2023-12-30',\n               '2023-12-31', '2024-01-01'],\n              dtype='datetime64[ns]', length=366, freq='D')\n\n\n… Or a Pandas Series.\n\ntimestamps_series = pd.Series(timestamps_dt)\ntimestamps_series\n\n0     2023-01-01\n1     2023-01-02\n2     2023-01-03\n3     2023-01-04\n4     2023-01-05\n         ...    \n361   2023-12-28\n362   2023-12-29\n363   2023-12-30\n364   2023-12-31\n365   2024-01-01\nLength: 366, dtype: datetime64[ns]\n\n\nAnd you can use the pandas Series function, tk.get_timeseries_signature() to create 29 features from the date sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas series: get_timeseries_signature\ntimestamps_series.get_timeseries_signature()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns\n\n\n\n\n\n\n# DateTimeIndex: get_timeseries_signature\ntk.get_timeseries_signature(timestamps_dt)\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns"
  },
  {
    "objectID": "getting-started/02_quick_start.html",
    "href": "getting-started/02_quick_start.html",
    "title": "Quick Start",
    "section": "",
    "text": "This is a simple exercise to showcase the power of our 2 most popular function:\n\nsummarize_by_time()\nplot_timeseries()\n\n\n\nFirst, import pytimetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport pytimetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\n\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format = 'False' to return the dataframe in a long format (Note long format is the default).\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        freq         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = False\n    )\n\n# First 5 rows shown\nsummary_category_1_df.head()\n\n\n\n\n\n\n\n\ncategory_1\norder_date\ntotal_price\n\n\n\n\n0\nMountain\n2011-01-01\n221490\n\n\n1\nMountain\n2011-02-01\n660555\n\n\n2\nMountain\n2011-03-01\n358855\n\n\n3\nMountain\n2011-04-01\n1075975\n\n\n4\nMountain\n2011-05-01\n450440\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow available: plot_timeseries().\n\n\n\n\n\nPlot time series is a quick and easy way to visualize time series and make professional time series plots.\n\n\n\nWith the data summarized by time, we can visualize with plot_timeseries(). pytimetk functions are groupby() aware meaning they understand if your data is grouped to do things by group. This is useful in time series where we often deal with 100s of time series groups.\n\nsummary_category_1_df \\\n    .groupby('category_1') \\\n    .plot_timeseries(\n        date_column  = 'order_date',\n        value_column = 'total_price',\n        smooth_frac  = 0.8\n    )"
  },
  {
    "objectID": "getting-started/02_quick_start.html#import-libraries-data",
    "href": "getting-started/02_quick_start.html#import-libraries-data",
    "title": "Quick Start",
    "section": "",
    "text": "First, import pytimetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport pytimetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns"
  },
  {
    "objectID": "getting-started/02_quick_start.html#using-summarize_by_time-for-a-sales-analysis",
    "href": "getting-started/02_quick_start.html#using-summarize_by_time-for-a-sales-analysis",
    "title": "Quick Start",
    "section": "",
    "text": "Your company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format = 'False' to return the dataframe in a long format (Note long format is the default).\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        freq         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = False\n    )\n\n# First 5 rows shown\nsummary_category_1_df.head()\n\n\n\n\n\n\n\n\ncategory_1\norder_date\ntotal_price\n\n\n\n\n0\nMountain\n2011-01-01\n221490\n\n\n1\nMountain\n2011-02-01\n660555\n\n\n2\nMountain\n2011-03-01\n358855\n\n\n3\nMountain\n2011-04-01\n1075975\n\n\n4\nMountain\n2011-05-01\n450440"
  },
  {
    "objectID": "getting-started/02_quick_start.html#visualizing-sales-patterns",
    "href": "getting-started/02_quick_start.html#visualizing-sales-patterns",
    "title": "Quick Start",
    "section": "",
    "text": "Now available: plot_timeseries().\n\n\n\n\n\nPlot time series is a quick and easy way to visualize time series and make professional time series plots.\n\n\n\nWith the data summarized by time, we can visualize with plot_timeseries(). pytimetk functions are groupby() aware meaning they understand if your data is grouped to do things by group. This is useful in time series where we often deal with 100s of time series groups.\n\nsummary_category_1_df \\\n    .groupby('category_1') \\\n    .plot_timeseries(\n        date_column  = 'order_date',\n        value_column = 'total_price',\n        smooth_frac  = 0.8\n    )"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing (Developer Setup)",
    "section": "",
    "text": "Interested in contributing?\n\n\n\n\n\nMake sure to Fork the GitHub Repo. Clone your fork. Then use poetry to install the pytimetk package.\n\n\n\n\n1 GitHub\nTo contribute, you’ll need to have a GitHub account. Then:\n\n1. Fork our pytimetk repository\nHead to our GitHub Repo and select “fork”. This makes a copied version of pytimetk for your personal use.\n\n\n2. Clone your forked version\nCloning will put your own personal version of pytimetk on your local machine. Make sure to replace [your_user_name] with your user name.\ngit clone https://github.com/[your_user_name]/pytimetk\n\n\n\n2 Poetry Environment Setup\nTo install pytimetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Install Dependencies\nUse Poetry to install the package and its dependencies:\npoetry install\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install\n\n\n\n3 Submit a Pull Request\n\n1. Make changes on a Branch\nMake changes in your local version on a branch where my-feature-branch is a branch you’d like to create that contains modifications.\ngit checkout -b my-feature-branch\n\n\n2. Push to your forked version of pytimetk\ngit push origin my-feature-branch\n\n\n3. Create a Pull Request\n\nGo to your forked repository on GitHub and switch to your branch.\nClick on “New pull request” and compare the changes you made with the original repository.\nFill out the pull request template with the necessary information, explaining your changes, the reason for them, and any other relevant information.\n\n\n\n4. Submit the Pull Request\n\nReview your changes and submit the pull request.\n\n\n\n\n4 Next Steps 🍻\nWe will review your PR. If all goes well, we’ll merge! And then you’ve just helped the community. 🍻"
  },
  {
    "objectID": "changelog.html",
    "href": "changelog.html",
    "title": "Changelog for PyTimeTK",
    "section": "",
    "text": "1 pytimetk 0.1.0.9000 (in development)\nNew Functions:\n\naugment_expanding()\nget_frequency()\n\nNew Data Sets:\n\nexpedia: Expedia hotel searches\n\nNew Applied Tutorials:\n\nSales Customer Relationship Management (CRM) Training\nFinance and Investment Analysis\nDemand Forecasting\n\n\n\n2 pytimetk 0.1.0 (2023-10-02)\n\nAbout the Initial release.\nThis release includes the following features:\n\nA workhorse plotting function called plot_timeseries() 💪\nThree (3) data wrangling functions that will simplify 90% of time series tasks 🙏\nFive (5) “augmentor” functions: These add hundreds of features to time series to help in predictive tasks 🧠\nTwo (2) time series feature summarizes: identify key aspects of your time series 🔍\nNine (9) pandas series and DatetimeIndex helpers (work more easily with these timestamp data structures) ⏲\nFour (4) date utility functions that fill in missing function gaps in pandas 🐼\nTwo (2) Visualization utilities to help you customize your visualizations and make them look MORE professional 📈\nTwo (2) Pandas helpers that help clean up and understand pandas data frames with time series 🎇\nTwelve (12) time series datasets that you can practice PyTimeTK time series analysis on 🔢\n\n\n\nThe PyTimeTK website comes with:\n\nTwo (2) Getting started tutorials\nFive (5) Guides covering common tasks\nComing Soon: Applied Tutorials in Sales, Finance, Demand Forecasting, Anomaly Detection, and more."
  },
  {
    "objectID": "getting-started/01_installation.html",
    "href": "getting-started/01_installation.html",
    "title": "Install",
    "section": "",
    "text": "1 Quick Install\nLet’s get you up and running with pytimetk fast with the latest stable release.\npip install pytimetk\nYou can install from GitHub with this code.\npip install git+https://github.com/business-science/pytimetk.git\n\n\n2 Next steps\nCheck out the Quick Start Guide Next.\n\n\n3 More Coming Soon…\nWe are in the early stages of development. But it’s obvious the potential for pytimetk now in Python. 🐍\n\nPlease ⭐ us on GitHub (it takes 2-seconds and means a lot).\nTo make requests, please see our Project Roadmap GH Issue #2. You can make requests there.\nWant to contribute? See our contributing guide here."
  },
  {
    "objectID": "guides/01_visualization.html",
    "href": "guides/01_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "How this guide benefits you\n\n\n\n\n\nThis guide covers how to use the plot_timeseries() for data visualization. Once you understand how it works, you can apply explore time series data easier than ever.\nThis tutorial focuses on, plot_timeseries(), a workhorse time-series plotting function that:"
  },
  {
    "objectID": "guides/01_visualization.html#plotting-groups",
    "href": "guides/01_visualization.html#plotting-groups",
    "title": "Data Visualization",
    "section": "2.1 Plotting Groups",
    "text": "2.1 Plotting Groups\nNext, let’s move on to a dataset with time series groups, m4_monthly, which is a sample of 4 time series from the M4 competition that are sampled at a monthly frequency.\n\n# Import a Time Series Data Set\nm4_monthly = tk.load_dataset(\"m4_monthly\", parse_dates = ['date'])\nm4_monthly\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nM1\n1976-06-01\n8000\n\n\n1\nM1\n1976-07-01\n8350\n\n\n2\nM1\n1976-08-01\n8570\n\n\n3\nM1\n1976-09-01\n7700\n\n\n4\nM1\n1976-10-01\n7080\n\n\n...\n...\n...\n...\n\n\n1569\nM1000\n2015-02-01\n880\n\n\n1570\nM1000\n2015-03-01\n800\n\n\n1571\nM1000\n2015-04-01\n1140\n\n\n1572\nM1000\n2015-05-01\n970\n\n\n1573\nM1000\n2015-06-01\n1430\n\n\n\n\n1574 rows × 3 columns\n\n\n\nVisualizing grouped data is as simple as grouping the data set with groupby() before run it into the plot_timeseries() function. Here are the key points:\n\nGroups can be added using the pandas groupby().\nThese groups are then converted into facets.\nUsing facet_ncol = 2 returns a 2-column faceted plot.\nSetting facet_scales = \"free\" allows the x and y-axes of each plot to scale independently of the other plots.\n\n\nm4_monthly.groupby('id').plot_timeseries(\n    'date', 'value', \n    facet_ncol = 2, \n    facet_scales = \"free\"\n)\n\n\n                                                \n\n\nThe groups can also be vizualized in the same plot using color_column paramenter. Let’s come back to taylor_30_min dataframe.\n\n# load data\ntaylor_30_min = tk.load_dataset(\"taylor_30_min\", parse_dates = ['date'])\n\n# extract the month using pandas\ntaylor_30_min['month'] = pd.to_datetime(taylor_30_min['date']).dt.month\n\n# plot groups\ntaylor_30_min.plot_timeseries(\n    'date', 'value', \n    color_column = 'month'\n)"
  },
  {
    "objectID": "guides/03_pandas_frequency.html",
    "href": "guides/03_pandas_frequency.html",
    "title": "Pandas Frequencies",
    "section": "",
    "text": "How this guide benefits you\n\n\n\n\n\nThis guide covers how to use the pandas frequency strings within pytimetk. Once you understand key frequencies, you can apply them to manipulate time series data like a pro.\n\n\n\n\n1 Pandas Frequencies\nPandas offers a variety of frequency strings, also known as offset aliases, to define the frequency of a time series. Here are some common frequency strings used in pandas:\n\n‘B’: Business Day\n‘D’: Calendar day\n‘W’: Weekly\n‘M’: Month end\n‘BM’: Business month end\n‘MS’: Month start\n‘BMS’: Business month start\n‘Q’: Quarter end\n‘BQ’: Business quarter end\n‘QS’: Quarter start\n‘BQS’: Business quarter start\n‘A’ or ‘Y’: Year end\n‘BA’ or ‘BY’: Business year end\n‘AS’ or ‘YS’: Year start\n‘BAS’ or ‘BYS’: Business year start\n‘H’: Hourly\n‘T’ or ‘min’: Minutely\n‘S’: Secondly\n‘L’ or ‘ms’: Milliseconds\n‘U’: Microseconds\n‘N’: Nanoseconds\n\n\nCustom Frequencies:\n\nYou can also create custom frequencies by combining base frequencies, like:\n\n‘2D’: Every 2 days\n‘3W’: Every 3 weeks\n‘4H’: Every 4 hours\n‘1H30T’: Every 1 hour and 30 minutes\n\n\n\n\nCompound Frequencies:\n\nYou can combine multiple frequencies by adding them together.\n\n‘1D1H’: 1 day and 1 hour\n‘1H30T’: 1 hour and 30 minutes\n\n\n\n\nExample:\n\nimport pandas as pd\n\n# Creating a date range with daily frequency\ndate_range_daily = pd.date_range(start='2023-01-01', end='2023-01-10', freq='D')\n\ndate_range_daily\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n# Creating a date range with 2 days frequency\ndate_range_two_days = pd.date_range(start='2023-01-01', end='2023-01-10', freq='2D')\n\ndate_range_two_days\n\nDatetimeIndex(['2023-01-01', '2023-01-03', '2023-01-05', '2023-01-07',\n               '2023-01-09'],\n              dtype='datetime64[ns]', freq='2D')\n\n\nThese frequency strings help in resampling, creating date ranges, and handling time-series data efficiently in pandas.\n\n\n\n2 Timetk Incorporates Pandas Frequencies\nNow that you’ve seen pandas frequencies, you’ll see them pop up in many of the pytimetk functions.\n\nExample: Padding Dates\nThis example shows how to use Pandas frequencies inside of pytimetk functions.\nWe’ll use pad_by_time to show how to use freq to fill in missing dates.\n\n# DataFrame with missing dates\nimport pandas as pd\n\ndata = {\n    # '2023-09-05' is missing\n    'datetime': ['2023-09-01', '2023-09-02', '2023-09-03', '2023-09-04', '2023-09-06'],  \n    'value': [10, 30, 40, 50, 60]\n}\n\ndf = pd.DataFrame(data)\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01\n10\n\n\n1\n2023-09-02\n30\n\n\n2\n2023-09-03\n40\n\n\n3\n2023-09-04\n50\n\n\n4\n2023-09-06\n60\n\n\n\n\n\n\n\nWe can resample to fill in the missing day using pad_by_time with freq = 'D'.\n\nimport pytimetk as tk\n\ndf.pad_by_time('datetime', freq = 'D')\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01\n10.0\n\n\n1\n2023-09-02\n30.0\n\n\n2\n2023-09-03\n40.0\n\n\n3\n2023-09-04\n50.0\n\n\n4\n2023-09-05\nNaN\n\n\n5\n2023-09-06\n60.0\n\n\n\n\n\n\n\nWhat about resampling every 12 hours? Just set `freq = ‘12H’.\n\nimport pytimetk as tk\n\ndf.pad_by_time('datetime', freq = '12H')\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01 00:00:00\n10.0\n\n\n1\n2023-09-01 12:00:00\nNaN\n\n\n2\n2023-09-02 00:00:00\n30.0\n\n\n3\n2023-09-02 12:00:00\nNaN\n\n\n4\n2023-09-03 00:00:00\n40.0\n\n\n5\n2023-09-03 12:00:00\nNaN\n\n\n6\n2023-09-04 00:00:00\n50.0\n\n\n7\n2023-09-04 12:00:00\nNaN\n\n\n8\n2023-09-05 00:00:00\nNaN\n\n\n9\n2023-09-05 12:00:00\nNaN\n\n\n10\n2023-09-06 00:00:00\n60.0\n\n\n\n\n\n\n\nYou’ll see these pandas frequencies come up as the parameter freq in many pytimetk functions.\n\n\n\n3 Next Steps\nCheck out the Data Wrangling Guide next.\n\n\n4 More Coming Soon…\nWe are in the early stages of development. But it’s obvious the potential for pytimetk now in Python. 🐍\n\nPlease ⭐ us on GitHub (it takes 2-seconds and means a lot).\nTo make requests, please see our Project Roadmap GH Issue #2. You can make requests there.\nWant to contribute? See our contributing guide here."
  },
  {
    "objectID": "guides/05_augmenting.html",
    "href": "guides/05_augmenting.html",
    "title": "Adding Features (Augmenting)",
    "section": "",
    "text": "This section will cover the augment set of functions, use to add many additional time series features to a dataset. We’ll cover how to use the following set of functions"
  },
  {
    "objectID": "guides/05_augmenting.html#basic-examples",
    "href": "guides/05_augmenting.html#basic-examples",
    "title": "Adding Features (Augmenting)",
    "section": "1.1 Basic Examples",
    "text": "1.1 Basic Examples\nAdd 1 or more lags / leads to a dataset:\n\n# import libraries\nimport pytimetk as tk\nimport pandas as pd\nimport numpy as np\nimport random\n\n# create sample data\ndates = pd.date_range(start = '2023-09-18', end = '2023-09-24')\nvalues = [random.randint(10, 50) for _ in range(7)]\n\ndf = pd.DataFrame({\n    'date': dates,\n    'value': values\n})\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2023-09-18\n40\n\n\n1\n2023-09-19\n37\n\n\n2\n2023-09-20\n25\n\n\n3\n2023-09-21\n15\n\n\n4\n2023-09-22\n49\n\n\n5\n2023-09-23\n41\n\n\n6\n2023-09-24\n15\n\n\n\n\n\n\n\nCreate lag / lead of 3 days:\n\nLagLead\n\n\n\n# augment lag\ndf \\\n    .augment_lags(\n        date_column  = 'date',\n        value_column = 'value',\n        lags         = 3\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_lag_3\n\n\n\n\n0\n2023-09-18\n40\nNaN\n\n\n1\n2023-09-19\n37\nNaN\n\n\n2\n2023-09-20\n25\nNaN\n\n\n3\n2023-09-21\n15\n40.0\n\n\n4\n2023-09-22\n49\n37.0\n\n\n5\n2023-09-23\n41\n25.0\n\n\n6\n2023-09-24\n15\n15.0\n\n\n\n\n\n\n\n\n\n\n# augment leads\ndf \\\n    .augment_leads(\n        date_column  = 'date',\n        value_column = 'value',\n        leads        = 3\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_lead_3\n\n\n\n\n0\n2023-09-18\n40\n15.0\n\n\n1\n2023-09-19\n37\n49.0\n\n\n2\n2023-09-20\n25\n41.0\n\n\n3\n2023-09-21\n15\n15.0\n\n\n4\n2023-09-22\n49\nNaN\n\n\n5\n2023-09-23\n41\nNaN\n\n\n6\n2023-09-24\n15\nNaN\n\n\n\n\n\n\n\n\n\n\nWe can create multiple lag / lead values for a single time series:\n\nLagLead\n\n\n\n# multiple lagged values for a single time series\ndf \\\n    .augment_lags(\n        date_column  = 'date',\n        value_column = 'value',\n        lags         = (1, 3)\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_lag_1\nvalue_lag_2\nvalue_lag_3\n\n\n\n\n0\n2023-09-18\n40\nNaN\nNaN\nNaN\n\n\n1\n2023-09-19\n37\n40.0\nNaN\nNaN\n\n\n2\n2023-09-20\n25\n37.0\n40.0\nNaN\n\n\n3\n2023-09-21\n15\n25.0\n37.0\n40.0\n\n\n4\n2023-09-22\n49\n15.0\n25.0\n37.0\n\n\n5\n2023-09-23\n41\n49.0\n15.0\n25.0\n\n\n6\n2023-09-24\n15\n41.0\n49.0\n15.0\n\n\n\n\n\n\n\n\n\n\n# multiple leads values for a single time series\ndf \\\n    .augment_leads(\n        date_column  = 'date',\n        value_column = 'value',\n        leads        = (1, 3)\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_lead_1\nvalue_lead_2\nvalue_lead_3\n\n\n\n\n0\n2023-09-18\n40\n37.0\n25.0\n15.0\n\n\n1\n2023-09-19\n37\n25.0\n15.0\n49.0\n\n\n2\n2023-09-20\n25\n15.0\n49.0\n41.0\n\n\n3\n2023-09-21\n15\n49.0\n41.0\n15.0\n\n\n4\n2023-09-22\n49\n41.0\n15.0\nNaN\n\n\n5\n2023-09-23\n41\n15.0\nNaN\nNaN\n\n\n6\n2023-09-24\n15\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "guides/05_augmenting.html#augment-lags-leads-for-grouped-time-series",
    "href": "guides/05_augmenting.html#augment-lags-leads-for-grouped-time-series",
    "title": "Adding Features (Augmenting)",
    "section": "1.2 Augment Lags / Leads For Grouped Time Series",
    "text": "1.2 Augment Lags / Leads For Grouped Time Series\naugment_lags() and augment_leads() also works for grouped time series data. Lets use the m4_daily_df dataset to showcase examples:\n\n# load m4_daily_df\nm4_daily_df = tk.load_dataset('m4_daily', parse_dates = ['date'])\n\n\nLagLead\n\n\n\n# agument lags for grouped time series\nm4_daily_df \\\n    .groupby(\"id\") \\\n    .augment_lags(\n        date_column  = 'date',\n        value_column = 'value',\n        lags         = (1, 7)\n    )\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_1\nvalue_lag_2\nvalue_lag_3\nvalue_lag_4\nvalue_lag_5\nvalue_lag_6\nvalue_lag_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n9286.9\n9265.4\n9091.4\n\n\n9739\nD500\n2012-09-20\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n9286.9\n9265.4\n\n\n9740\nD500\n2012-09-21\n9445.9\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n9286.9\n\n\n9741\nD500\n2012-09-22\n9497.9\n9445.9\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n\n\n9742\nD500\n2012-09-23\n9545.3\n9497.9\n9445.9\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n\n\n\n\n9743 rows × 10 columns\n\n\n\n\n\n\n# augment leads for grouped time series\nm4_daily_df \\\n    .groupby(\"id\") \\\n    .augment_leads(\n        date_column  = 'date',\n        value_column = 'value',\n        leads        = (1, 7)\n    )\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lead_1\nvalue_lead_2\nvalue_lead_3\nvalue_lead_4\nvalue_lead_5\nvalue_lead_6\nvalue_lead_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2073.4\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n\n\n1\nD10\n2014-07-04\n2073.4\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n\n\n2\nD10\n2014-07-05\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n\n\n3\nD10\n2014-07-06\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n1978.8\n\n\n4\nD10\n2014-07-07\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n1978.8\n1988.3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9365.7\n9445.9\n9497.9\n9545.3\nNaN\nNaN\nNaN\n\n\n9739\nD500\n2012-09-20\n9365.7\n9445.9\n9497.9\n9545.3\nNaN\nNaN\nNaN\nNaN\n\n\n9740\nD500\n2012-09-21\n9445.9\n9497.9\n9545.3\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9741\nD500\n2012-09-22\n9497.9\n9545.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9742\nD500\n2012-09-23\n9545.3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n9743 rows × 10 columns"
  },
  {
    "objectID": "guides/05_augmenting.html#basic-examples-1",
    "href": "guides/05_augmenting.html#basic-examples-1",
    "title": "Adding Features (Augmenting)",
    "section": "2.1 Basic Examples",
    "text": "2.1 Basic Examples\nWe’ll continue with the use of our sample df created earlier:\n\n# window = 3 days, window function = mean\ndf \\\n    .augment_rolling(\n        date_column  = 'date',\n        value_column = 'value',\n        window       = 3,\n        window_func  = 'mean'\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_rolling_mean_win_3\n\n\n\n\n0\n2023-09-18\n40\nNaN\n\n\n1\n2023-09-19\n37\nNaN\n\n\n2\n2023-09-20\n25\n34.000000\n\n\n3\n2023-09-21\n15\n25.666667\n\n\n4\n2023-09-22\n49\n29.666667\n\n\n5\n2023-09-23\n41\n35.000000\n\n\n6\n2023-09-24\n15\n35.000000\n\n\n\n\n\n\n\nIt is important to understand how the center parameter in augment_rolling() works.\n\n\n\n\n\n\ncenter\n\n\n\n\n\nWhen set to True (default) the value of the rolling window will be centered, meaning that the value at the center of the window will be used as the result. When set to False (default) the rolling window will not be centered, meaning that the value at the end of the window will be used as the result.\n\n\n\nLets see an example:\n\nAugment Rolling: Center = TrueAugment Rolling: Center = False\n\n\n\n# agument rolling: center = true\ndf \\\n    .augment_rolling(\n        date_column  = 'date',\n        value_column = 'value',\n        window       = 3,\n        window_func  = 'mean',\n        center       = True\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_rolling_mean_win_3\n\n\n\n\n0\n2023-09-18\n40\nNaN\n\n\n1\n2023-09-19\n37\n34.000000\n\n\n2\n2023-09-20\n25\n25.666667\n\n\n3\n2023-09-21\n15\n29.666667\n\n\n4\n2023-09-22\n49\n35.000000\n\n\n5\n2023-09-23\n41\n35.000000\n\n\n6\n2023-09-24\n15\nNaN\n\n\n\n\n\n\n\nNote that we are using a 3 day rolling window and applying a mean to value. In simplier terms, value_rolling_mean_win_3 is a 3 day rolling average of value with center set to True. Thus the function starts computing the mean from 2023-09-19\n\n\n\n# agument rolling: center = false\ndf \\\n    .augment_rolling(\n        date_column  = 'date',\n        value_column = 'value',\n        window       = 3,\n        window_func  = 'mean',\n        center       = True\n    )\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_rolling_mean_win_3\n\n\n\n\n0\n2023-09-18\n40\nNaN\n\n\n1\n2023-09-19\n37\n34.000000\n\n\n2\n2023-09-20\n25\n25.666667\n\n\n3\n2023-09-21\n15\n29.666667\n\n\n4\n2023-09-22\n49\n35.000000\n\n\n5\n2023-09-23\n41\n35.000000\n\n\n6\n2023-09-24\n15\nNaN\n\n\n\n\n\n\n\nNote that we are using a 3 day rolling window and applying a mean to value. In simplier terms, value_rolling_mean_win_3 is a 3 day rolling average of value with center set to False. Thus the function starts computing the mean from 2023-09-20. The same value for 2023-19-18 and 2023-09-19 are returned as value_rolling_mean_win_3 since it did not detected the third to apply the 3 day rolling average."
  },
  {
    "objectID": "guides/05_augmenting.html#augment-rolling-with-multiple-windows-and-window-functions",
    "href": "guides/05_augmenting.html#augment-rolling-with-multiple-windows-and-window-functions",
    "title": "Adding Features (Augmenting)",
    "section": "2.2 Augment Rolling with Multiple Windows and Window Functions",
    "text": "2.2 Augment Rolling with Multiple Windows and Window Functions\nMultiple window functions can be passed to the window and window_func parameters:\n\n# augment rolling: window of 2 & 7 days, window_func of mean and standard deviation\nm4_daily_df \\\n    .query('id == \"D10\"') \\\n    .augment_rolling(\n                date_column = 'date',\n                value_column = 'value',\n                window = [2,7],\n                window_func = ['mean', ('std', lambda x: x.std())]\n            )\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_rolling_mean_win_2\nvalue_rolling_std_win_2\nvalue_rolling_mean_win_7\nvalue_rolling_std_win_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2074.80\n1.40\n2074.800000\n1.400000\n\n\n2\nD10\n2014-07-05\n2048.7\n2061.05\n12.35\n2066.100000\n12.356645\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.80\n0.10\n2061.800000\n13.037830\n\n\n4\nD10\n2014-07-07\n2006.4\n2027.65\n21.25\n2050.720000\n25.041038\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2615.85\n14.85\n2579.471429\n28.868159\n\n\n670\nD10\n2016-05-03\n2649.3\n2640.00\n9.30\n2594.800000\n33.081631\n\n\n671\nD10\n2016-05-04\n2631.8\n2640.55\n8.75\n2601.371429\n35.145563\n\n\n672\nD10\n2016-05-05\n2622.5\n2627.15\n4.65\n2607.457143\n34.584508\n\n\n673\nD10\n2016-05-06\n2620.1\n2621.30\n1.20\n2618.328571\n22.923270\n\n\n\n\n674 rows × 7 columns"
  },
  {
    "objectID": "guides/05_augmenting.html#augment-rolling-with-grouped-time-series",
    "href": "guides/05_augmenting.html#augment-rolling-with-grouped-time-series",
    "title": "Adding Features (Augmenting)",
    "section": "2.3 Augment Rolling with Grouped Time Series",
    "text": "2.3 Augment Rolling with Grouped Time Series\nagument_rolling can be used on grouped time series data:\n\n## augment rolling on grouped time series: window of 2 & 7 days, window_func of mean and standard deviation\nm4_daily_df \\\n    .groupby('id') \\\n    .augment_rolling(\n                date_column = 'date',\n                value_column = 'value',\n                window = [2,7],\n                window_func = ['mean', ('std', lambda x: x.std())]\n            )\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_rolling_mean_win_2\nvalue_rolling_std_win_2\nvalue_rolling_mean_win_7\nvalue_rolling_std_win_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2074.80\n1.40\n2074.800000\n1.400000\n\n\n2\nD10\n2014-07-05\n2048.7\n2061.05\n12.35\n2066.100000\n12.356645\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.80\n0.10\n2061.800000\n13.037830\n\n\n4\nD10\n2014-07-07\n2006.4\n2027.65\n21.25\n2050.720000\n25.041038\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9425.35\n6.55\n9382.071429\n74.335988\n\n\n9739\nD500\n2012-09-20\n9365.7\n9392.25\n26.55\n9396.400000\n58.431303\n\n\n9740\nD500\n2012-09-21\n9445.9\n9405.80\n40.10\n9419.114286\n39.184451\n\n\n9741\nD500\n2012-09-22\n9497.9\n9471.90\n26.00\n9438.928571\n38.945336\n\n\n9742\nD500\n2012-09-23\n9545.3\n9521.60\n23.70\n9449.028571\n53.379416\n\n\n\n\n9743 rows × 7 columns"
  },
  {
    "objectID": "guides/05_augmenting.html#basic-example",
    "href": "guides/05_augmenting.html#basic-example",
    "title": "Adding Features (Augmenting)",
    "section": "3.1 Basic Example",
    "text": "3.1 Basic Example\nWe’ll showcase an example using the m4_daily_df dataset by generating 29 additional features from the date column:\n\n# augment time series signature\nm4_daily_df \\\n    .query('id == \"D10\"') \\\n    .augment_timeseries_signature(\n        date_column = 'date'\n    ) \\\n    .head()\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n5 rows × 32 columns"
  },
  {
    "objectID": "guides/05_augmenting.html#basic-example-1",
    "href": "guides/05_augmenting.html#basic-example-1",
    "title": "Adding Features (Augmenting)",
    "section": "4.1 Basic Example",
    "text": "4.1 Basic Example\nWe’ll showcase an example using some sample data:\n\n# create sample data\ndates = pd.date_range(start = '2022-12-25', end = '2023-01-05')\n\ndf = pd.DataFrame({'date': dates})\n\n# augment time series signature: USA\ndf \\\n    .augment_holiday_signature(\n        date_column  = 'date',\n        country_name = 'UnitedStates'\n    )\n\n\n\n\n\n\n\n\ndate\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2022-12-25\n1\n1\n0\nChristmas Day\n\n\n1\n2022-12-26\n1\n0\n1\nChristmas Day (Observed)\n\n\n2\n2022-12-27\n0\n0\n1\nNaN\n\n\n3\n2022-12-28\n0\n0\n0\nNaN\n\n\n4\n2022-12-29\n0\n0\n0\nNaN\n\n\n5\n2022-12-30\n0\n0\n0\nNaN\n\n\n6\n2022-12-31\n0\n1\n0\nNaN\n\n\n7\n2023-01-01\n1\n1\n0\nNew Year's Day\n\n\n8\n2023-01-02\n1\n0\n1\nNew Year's Day (Observed)\n\n\n9\n2023-01-03\n0\n0\n1\nNaN\n\n\n10\n2023-01-04\n0\n0\n0\nNaN\n\n\n11\n2023-01-05\n0\n0\n0\nNaN"
  },
  {
    "objectID": "reference/augment_expanding.html",
    "href": "reference/augment_expanding.html",
    "title": "augment_expanding",
    "section": "",
    "text": "augment_expanding(data, date_column, value_column, use_independent_variables=False, window_func='mean', min_periods=None, **kwargs)\nApply one or more expanding functions and window sizes to one or more columns of a DataFrame.\nParameters\n----------\ndata : Union[pd.DataFrame, pd.core.groupby.generic.DataFrameGroupBy]\n    The `data` parameter is the input DataFrame or GroupBy object that contains the data to be processed. It can be either a Pandas DataFrame or a GroupBy object.\ndate_column : str\n    The `date_column` parameter is the name of the datetime column in the DataFrame by which the data should be sorted within each group.\nvalue_column : Union[str, list]\n    The `value_column` parameter is the name of the column(s) in the DataFrame to which the expanding window function(s) should be applied. It can be a single column name or a list of column names.\nuse_independent_variables : bool\n    The `use_independent_variables` parameter is an optional parameter that specifies whether the expanding function(s) require independent variables, such as expanding correlation or expanding regression. (See Examples below.)\nwindow_func : Union[str, list, Tuple[str, Callable]], optional\n    The `window_func` parameter in the `augment_expanding` function is used to specify the function(s) to be applied to the expanding windows. \n    \n    1. It can be a string or a list of strings, where each string represents the name of the function to be applied. \n    \n    2. Alternatively, it can be a list of tuples, where each tuple contains the name of the function to be applied and the function itself. The function is applied as a Pandas Series. (See Examples below.)\n    \n    3. If the function requires independent variables, the `use_independent_variables` parameter must be specified. The independent variables will be passed to the function as a DataFrame containing the window of rows. (See Examples below.)\n    \nReturns\n-------\npd.DataFrame\n    The `augment_expanding` function returns a DataFrame with new columns for each applied function, window size, and value column.\n\nExamples\n--------\n\n\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pytimetk as tk\nimport pandas as pd\nimport numpy as np\n\ndf = tk.load_dataset(\"m4_daily\", parse_dates = ['date'])\n```\n:::\n\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# String Function Name and Series Lambda Function (no independent variables)\nrolled_df = (\n    df\n        .groupby('id')\n        .augment_expanding(\n            date_column = 'date', \n            value_column = 'value',  \n            window_func = ['mean', ('std', lambda x: x.std())]\n        )\n)\nrolled_df\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n\n```{=html}\n&lt;div&gt;\n&lt;style scoped&gt;\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n&lt;/style&gt;\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;id&lt;/th&gt;\n      &lt;th&gt;date&lt;/th&gt;\n      &lt;th&gt;value&lt;/th&gt;\n      &lt;th&gt;value_expanding_mean&lt;/th&gt;\n      &lt;th&gt;value_expanding_std&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;0&lt;/th&gt;\n      &lt;td&gt;D10&lt;/td&gt;\n      &lt;td&gt;2014-07-03&lt;/td&gt;\n      &lt;td&gt;2076.2&lt;/td&gt;\n      &lt;td&gt;2076.200000&lt;/td&gt;\n      &lt;td&gt;0.000000&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;1&lt;/th&gt;\n      &lt;td&gt;D10&lt;/td&gt;\n      &lt;td&gt;2014-07-04&lt;/td&gt;\n      &lt;td&gt;2073.4&lt;/td&gt;\n      &lt;td&gt;2074.800000&lt;/td&gt;\n      &lt;td&gt;1.400000&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;2&lt;/th&gt;\n      &lt;td&gt;D10&lt;/td&gt;\n      &lt;td&gt;2014-07-05&lt;/td&gt;\n      &lt;td&gt;2048.7&lt;/td&gt;\n      &lt;td&gt;2066.100000&lt;/td&gt;\n      &lt;td&gt;12.356645&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;3&lt;/th&gt;\n      &lt;td&gt;D10&lt;/td&gt;\n      &lt;td&gt;2014-07-06&lt;/td&gt;\n      &lt;td&gt;2048.9&lt;/td&gt;\n      &lt;td&gt;2061.800000&lt;/td&gt;\n      &lt;td&gt;13.037830&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;4&lt;/th&gt;\n      &lt;td&gt;D10&lt;/td&gt;\n      &lt;td&gt;2014-07-07&lt;/td&gt;\n      &lt;td&gt;2006.4&lt;/td&gt;\n      &lt;td&gt;2050.720000&lt;/td&gt;\n      &lt;td&gt;25.041038&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;...&lt;/th&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;9738&lt;/th&gt;\n      &lt;td&gt;D500&lt;/td&gt;\n      &lt;td&gt;2012-09-19&lt;/td&gt;\n      &lt;td&gt;9418.8&lt;/td&gt;\n      &lt;td&gt;8286.606679&lt;/td&gt;\n      &lt;td&gt;2456.667418&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;9739&lt;/th&gt;\n      &lt;td&gt;D500&lt;/td&gt;\n      &lt;td&gt;2012-09-20&lt;/td&gt;\n      &lt;td&gt;9365.7&lt;/td&gt;\n      &lt;td&gt;8286.864035&lt;/td&gt;\n      &lt;td&gt;2456.430967&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;9740&lt;/th&gt;\n      &lt;td&gt;D500&lt;/td&gt;\n      &lt;td&gt;2012-09-21&lt;/td&gt;\n      &lt;td&gt;9445.9&lt;/td&gt;\n      &lt;td&gt;8287.140391&lt;/td&gt;\n      &lt;td&gt;2456.203287&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;9741&lt;/th&gt;\n      &lt;td&gt;D500&lt;/td&gt;\n      &lt;td&gt;2012-09-22&lt;/td&gt;\n      &lt;td&gt;9497.9&lt;/td&gt;\n      &lt;td&gt;8287.429011&lt;/td&gt;\n      &lt;td&gt;2455.981643&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;9742&lt;/th&gt;\n      &lt;td&gt;D500&lt;/td&gt;\n      &lt;td&gt;2012-09-23&lt;/td&gt;\n      &lt;td&gt;9545.3&lt;/td&gt;\n      &lt;td&gt;8287.728789&lt;/td&gt;\n      &lt;td&gt;2455.765726&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n&lt;p&gt;9743 rows × 5 columns&lt;/p&gt;\n&lt;/div&gt;\n```\n\n:::\n:::\n\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Expanding Correlation: Uses independent variables (value2)\n\ndf = pd.DataFrame({\n    'id': [1, 1, 1, 2, 2, 2],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06']),\n    'value1': [10, 20, 29, 42, 53, 59],\n    'value2': [2, 16, 20, 40, 41, 50],\n})\n\nresult_df = (\n    df.groupby('id')\n    .augment_expanding(\n        date_column='date',\n        value_column='value1',\n        use_independent_variables=True,\n        window_func=[('corr', lambda df: df['value1'].corr(df['value2']))],\n    )\n)\nresult_df\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n\n```{=html}\n&lt;div&gt;\n&lt;style scoped&gt;\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n&lt;/style&gt;\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;id&lt;/th&gt;\n      &lt;th&gt;date&lt;/th&gt;\n      &lt;th&gt;value1&lt;/th&gt;\n      &lt;th&gt;value2&lt;/th&gt;\n      &lt;th&gt;value1_expanding_corr&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;0&lt;/th&gt;\n      &lt;td&gt;1&lt;/td&gt;\n      &lt;td&gt;2023-01-01&lt;/td&gt;\n      &lt;td&gt;10&lt;/td&gt;\n      &lt;td&gt;2&lt;/td&gt;\n      &lt;td&gt;NaN&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;1&lt;/th&gt;\n      &lt;td&gt;1&lt;/td&gt;\n      &lt;td&gt;2023-01-02&lt;/td&gt;\n      &lt;td&gt;20&lt;/td&gt;\n      &lt;td&gt;16&lt;/td&gt;\n      &lt;td&gt;1.000000&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;2&lt;/th&gt;\n      &lt;td&gt;1&lt;/td&gt;\n      &lt;td&gt;2023-01-03&lt;/td&gt;\n      &lt;td&gt;29&lt;/td&gt;\n      &lt;td&gt;20&lt;/td&gt;\n      &lt;td&gt;0.961054&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;3&lt;/th&gt;\n      &lt;td&gt;2&lt;/td&gt;\n      &lt;td&gt;2023-01-04&lt;/td&gt;\n      &lt;td&gt;42&lt;/td&gt;\n      &lt;td&gt;40&lt;/td&gt;\n      &lt;td&gt;NaN&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;4&lt;/th&gt;\n      &lt;td&gt;2&lt;/td&gt;\n      &lt;td&gt;2023-01-05&lt;/td&gt;\n      &lt;td&gt;53&lt;/td&gt;\n      &lt;td&gt;41&lt;/td&gt;\n      &lt;td&gt;1.000000&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;5&lt;/th&gt;\n      &lt;td&gt;2&lt;/td&gt;\n      &lt;td&gt;2023-01-06&lt;/td&gt;\n      &lt;td&gt;59&lt;/td&gt;\n      &lt;td&gt;50&lt;/td&gt;\n      &lt;td&gt;0.824831&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/div&gt;\n```\n\n:::\n:::\n\n\n# Expanding Regression: Using independent variables (value2 and value3)\n\n# Requires: scikit-learn\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.DataFrame({\n    'id': [1, 1, 1, 2, 2, 2],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06']),\n    'value1': [10, 20, 29, 42, 53, 59],\n    'value2': [5, 16, 24, 35, 45, 58],\n    'value3': [2, 3, 6, 9, 10, 13]\n})\n\n# Define Regression Function\ndef regression(df):\n\n    model = LinearRegression()\n    X = df[['value2', 'value3']]  # Extract X values (independent variables)\n    y = df['value1']  # Extract y values (dependent variable)\n    model.fit(X, y)\n    ret = pd.Series([model.intercept_, model.coef_[0]], index=['Intercept', 'Slope'])\n    return ret # Return intercept and slope as a Series\n    \n\n# Example to call the function\nresult_df = (\n    df.groupby('id')\n    .augment_expanding(\n        date_column='date',\n        value_column='value1',\n        use_independent_variables=True,\n        window_func=[('regression', regression)]\n    )\n    .dropna()\n)\nresult_df\n\n# Display Results in Wide Format since returning multiple values\nregression_wide_df = pd.concat(result_df['value1_expanding_regression'].to_list(), axis=1).T\n\nregression_wide_df = pd.concat([result_df.reset_index(drop = True), regression_wide_df], axis=1)\n\nregression_wide_df\n```"
  },
  {
    "objectID": "reference/augment_lags.html",
    "href": "reference/augment_lags.html",
    "title": "augment_lags",
    "section": "",
    "text": "augment_lags(data, date_column, value_column, lags=1)\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\nThe augment_lags function takes a Pandas DataFrame or GroupBy object, a date column, a value column or list of value columns, and a lag or list of lags, and adds lagged versions of the value columns to the DataFrame."
  },
  {
    "objectID": "reference/augment_lags.html#parameters",
    "href": "reference/augment_lags.html#parameters",
    "title": "augment_lags",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to add lagged columns to.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to sort the data before adding the lagged values.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the column(s) in the DataFrame that you want to add lagged values for. It can be either a single column name (string) or a list of column names.\nrequired\n\n\nlags\nint or tuple or list\nThe lags parameter is an integer, tuple, or list that specifies the number of lagged values to add to the DataFrame. - If it is an integer, the function will add that number of lagged values for each column specified in the value_column parameter. - If it is a tuple, it will generate lags from the first to the second value (inclusive). - If it is a list, it will generate lags based on the values in the list.\n1"
  },
  {
    "objectID": "reference/augment_lags.html#returns",
    "href": "reference/augment_lags.html#returns",
    "title": "augment_lags",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame with lagged columns added to it."
  },
  {
    "objectID": "reference/augment_lags.html#examples",
    "href": "reference/augment_lags.html#examples",
    "title": "augment_lags",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\ndf = tk.load_dataset('m4_daily', parse_dates=['date'])\ndf\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n\n\n1\nD10\n2014-07-04\n2073.4\n\n\n2\nD10\n2014-07-05\n2048.7\n\n\n3\nD10\n2014-07-06\n2048.9\n\n\n4\nD10\n2014-07-07\n2006.4\n\n\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n\n\n9739\nD500\n2012-09-20\n9365.7\n\n\n9740\nD500\n2012-09-21\n9445.9\n\n\n9741\nD500\n2012-09-22\n9497.9\n\n\n9742\nD500\n2012-09-23\n9545.3\n\n\n\n\n9743 rows × 3 columns\n\n\n\n\n# Add a lagged value of 2 for each grouped time series\nlagged_df = (\n    df \n        .groupby('id')\n        .augment_lags(\n            date_column='date',\n            value_column='value',\n            lags=2\n        )\n)\nlagged_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_2\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2076.2\n\n\n3\nD10\n2014-07-06\n2048.9\n2073.4\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.7\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9437.7\n\n\n9739\nD500\n2012-09-20\n9365.7\n9431.9\n\n\n9740\nD500\n2012-09-21\n9445.9\n9418.8\n\n\n9741\nD500\n2012-09-22\n9497.9\n9365.7\n\n\n9742\nD500\n2012-09-23\n9545.3\n9445.9\n\n\n\n\n9743 rows × 4 columns\n\n\n\n\n# Add 7 lagged values for a single time series\nlagged_df_single = (\n    df \n        .query('id == \"D10\"')\n        .augment_lags(\n            date_column='date',\n            value_column='value',\n            lags=(1, 7)\n        )\n)\nlagged_df_single\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_1\nvalue_lag_2\nvalue_lag_3\nvalue_lag_4\nvalue_lag_5\nvalue_lag_6\nvalue_lag_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n2542.0\n2534.2\n\n\n670\nD10\n2016-05-03\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n2542.0\n\n\n671\nD10\n2016-05-04\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n\n\n672\nD10\n2016-05-05\n2622.5\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n\n\n673\nD10\n2016-05-06\n2620.1\n2622.5\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n\n\n\n\n674 rows × 10 columns\n\n\n\n\n# Add 2 lagged values, 2 and 4, for a single time series\nlagged_df_single_two = (\n    df \n        .query('id == \"D10\"')\n        .augment_lags(\n            date_column='date',\n            value_column='value',\n            lags=[2, 4]\n        )\n)\nlagged_df_single_two\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_2\nvalue_lag_4\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\nNaN\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2076.2\nNaN\n\n\n3\nD10\n2014-07-06\n2048.9\n2073.4\nNaN\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.7\n2076.2\n\n\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2572.9\n2579.9\n\n\n670\nD10\n2016-05-03\n2649.3\n2601.0\n2544.0\n\n\n671\nD10\n2016-05-04\n2631.8\n2630.7\n2572.9\n\n\n672\nD10\n2016-05-05\n2622.5\n2649.3\n2601.0\n\n\n673\nD10\n2016-05-06\n2620.1\n2631.8\n2630.7\n\n\n\n\n674 rows × 5 columns"
  },
  {
    "objectID": "reference/augment_rolling.html",
    "href": "reference/augment_rolling.html",
    "title": "augment_rolling",
    "section": "",
    "text": "augment_rolling(data, date_column, value_column, use_independent_variables=False, window=2, window_func='mean', min_periods=None, center=False, **kwargs)\nApply one or more rolling functions and window sizes to one or more columns of a DataFrame."
  },
  {
    "objectID": "reference/augment_rolling.html#parameters",
    "href": "reference/augment_rolling.html#parameters",
    "title": "augment_rolling",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nUnion[pd.DataFrame, pd.core.groupby.generic.DataFrameGroupBy]\nThe data parameter is the input DataFrame or GroupBy object that contains the data to be processed. It can be either a Pandas DataFrame or a GroupBy object.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is the name of the datetime column in the DataFrame by which the data should be sorted within each group.\nrequired\n\n\nvalue_column\nUnion[str, list]\nThe value_column parameter is the name of the column(s) in the DataFrame to which the rolling window function(s) should be applied. It can be a single column name or a list of column names.\nrequired\n\n\nuse_independent_variables\nbool\nThe use_independent_variables parameter is an optional parameter that specifies whether the rolling function(s) require independent variables, such as rolling correlation or rolling regression. (See Examples below.)\nFalse\n\n\nwindow\nUnion[int, tuple, list]\nThe window parameter in the augment_rolling function is used to specify the size of the rolling windows. It can be either an integer or a list of integers. - If it is an integer, the same window size will be applied to all columns specified in the value_column. - If it is a tuple, it will generate windows from the first to the second value (inclusive). - If it is a list of integers, each integer in the list will be used as the window size for the corresponding column in the value_column list.\n2\n\n\nwindow_func\nUnion[str, list, Tuple[str, Callable]]\nThe window_func parameter in the augment_rolling function is used to specify the function(s) to be applied to the rolling windows. 1. It can be a string or a list of strings, where each string represents the name of the function to be applied. 2. Alternatively, it can be a list of tuples, where each tuple contains the name of the function to be applied and the function itself. The function is applied as a Pandas Series. (See Examples below.) 3. If the function requires independent variables, the use_independent_variables parameter must be specified. The independent variables will be passed to the function as a DataFrame containing the window of rows. (See Examples below.)\n'mean'\n\n\ncenter\nbool\nThe center parameter in the augment_rolling function determines whether the rolling window is centered or not. If center is set to True, the rolling window will be centered, meaning that the value at the center of the window will be used as the result. If |False`"
  },
  {
    "objectID": "reference/augment_rolling.html#returns",
    "href": "reference/augment_rolling.html#returns",
    "title": "augment_rolling",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe augment_rolling function returns a DataFrame with new columns for each applied function, window size, and value column."
  },
  {
    "objectID": "reference/augment_rolling.html#examples",
    "href": "reference/augment_rolling.html#examples",
    "title": "augment_rolling",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\nimport numpy as np\n\ndf = tk.load_dataset(\"m4_daily\", parse_dates = ['date'])\n\n\n# String Function Name and Series Lambda Function (no independent variables)\n# window = [2,7] yields only 2 and 7\nrolled_df = (\n    df\n        .groupby('id')\n        .augment_rolling(\n            date_column = 'date', \n            value_column = 'value', \n            window = [2,7], \n            window_func = ['mean', ('std', lambda x: x.std())]\n        )\n)\nrolled_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_rolling_mean_win_2\nvalue_rolling_std_win_2\nvalue_rolling_mean_win_7\nvalue_rolling_std_win_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2074.80\n1.40\n2074.800000\n1.400000\n\n\n2\nD10\n2014-07-05\n2048.7\n2061.05\n12.35\n2066.100000\n12.356645\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.80\n0.10\n2061.800000\n13.037830\n\n\n4\nD10\n2014-07-07\n2006.4\n2027.65\n21.25\n2050.720000\n25.041038\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9425.35\n6.55\n9382.071429\n74.335988\n\n\n9739\nD500\n2012-09-20\n9365.7\n9392.25\n26.55\n9396.400000\n58.431303\n\n\n9740\nD500\n2012-09-21\n9445.9\n9405.80\n40.10\n9419.114286\n39.184451\n\n\n9741\nD500\n2012-09-22\n9497.9\n9471.90\n26.00\n9438.928571\n38.945336\n\n\n9742\nD500\n2012-09-23\n9545.3\n9521.60\n23.70\n9449.028571\n53.379416\n\n\n\n\n9743 rows × 7 columns\n\n\n\n\n# String Function Name and Series Lambda Function (no independent variables)\n# window = (1,3) yields 1, 2, and 3\nrolled_df = (\n    df\n        .groupby('id')\n        .augment_rolling(\n            date_column = 'date', \n            value_column = 'value', \n            window = (1,3), \n            window_func = ['mean', ('std', lambda x: x.std())]\n        )\n)\nrolled_df \n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_rolling_mean_win_1\nvalue_rolling_std_win_1\nvalue_rolling_mean_win_2\nvalue_rolling_std_win_2\nvalue_rolling_mean_win_3\nvalue_rolling_std_win_3\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2076.2\n0.0\n2076.20\n0.00\n2076.200000\n0.000000\n\n\n1\nD10\n2014-07-04\n2073.4\n2073.4\n0.0\n2074.80\n1.40\n2074.800000\n1.400000\n\n\n2\nD10\n2014-07-05\n2048.7\n2048.7\n0.0\n2061.05\n12.35\n2066.100000\n12.356645\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.9\n0.0\n2048.80\n0.10\n2057.000000\n11.596839\n\n\n4\nD10\n2014-07-07\n2006.4\n2006.4\n0.0\n2027.65\n21.25\n2034.666667\n19.987718\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9418.8\n0.0\n9425.35\n6.55\n9429.466667\n7.905413\n\n\n9739\nD500\n2012-09-20\n9365.7\n9365.7\n0.0\n9392.25\n26.55\n9405.466667\n28.623339\n\n\n9740\nD500\n2012-09-21\n9445.9\n9445.9\n0.0\n9405.80\n40.10\n9410.133333\n33.310092\n\n\n9741\nD500\n2012-09-22\n9497.9\n9497.9\n0.0\n9471.90\n26.00\n9436.500000\n54.378182\n\n\n9742\nD500\n2012-09-23\n9545.3\n9545.3\n0.0\n9521.60\n23.70\n9496.366667\n40.594362\n\n\n\n\n9743 rows × 9 columns\n\n\n\n\n# Rolling Correlation: Uses independent variables (value2)\n\ndf = pd.DataFrame({\n    'id': [1, 1, 1, 2, 2, 2],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06']),\n    'value1': [10, 20, 29, 42, 53, 59],\n    'value2': [2, 16, 20, 40, 41, 50],\n})\n\nresult_df = (\n    df.groupby('id')\n    .augment_rolling(\n        date_column='date',\n        value_column='value1',\n        use_independent_variables=True,\n        window=3,\n        window_func=[('corr', lambda df: df['value1'].corr(df['value2']))],\n        center = False\n    )\n)\nresult_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue1\nvalue2\nvalue1_rolling_corr_win_3\n\n\n\n\n0\n1\n2023-01-01\n10\n2\nNaN\n\n\n1\n1\n2023-01-02\n20\n16\nNaN\n\n\n2\n1\n2023-01-03\n29\n20\n0.961054\n\n\n3\n2\n2023-01-04\n42\n40\nNaN\n\n\n4\n2\n2023-01-05\n53\n41\nNaN\n\n\n5\n2\n2023-01-06\n59\n50\n0.824831\n\n\n\n\n\n\n\n\n# Rolling Regression: Using independent variables (value2 and value3)\n\n# Requires: scikit-learn\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.DataFrame({\n    'id': [1, 1, 1, 2, 2, 2],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06']),\n    'value1': [10, 20, 29, 42, 53, 59],\n    'value2': [5, 16, 24, 35, 45, 58],\n    'value3': [2, 3, 6, 9, 10, 13]\n})\n\n# Define Regression Function\ndef regression(df):\n\n    model = LinearRegression()\n    X = df[['value2', 'value3']]  # Extract X values (independent variables)\n    y = df['value1']  # Extract y values (dependent variable)\n    model.fit(X, y)\n    ret = pd.Series([model.intercept_, model.coef_[0]], index=['Intercept', 'Slope'])\n    \n    return ret # Return intercept and slope as a Series\n    \n\n# Example to call the function\nresult_df = (\n    df.groupby('id')\n    .augment_rolling(\n        date_column='date',\n        value_column='value1',\n        use_independent_variables=True,\n        window=3,\n        window_func=[('regression', regression)]\n    )\n    .dropna()\n)\n\n# Display Results in Wide Format since returning multiple values\nregression_wide_df = pd.concat(result_df['value1_rolling_regression_win_3'].to_list(), axis=1).T\n\nregression_wide_df = pd.concat([result_df.reset_index(drop = True), regression_wide_df], axis=1)\n\nregression_wide_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue1\nvalue2\nvalue3\nvalue1_rolling_regression_win_3\nIntercept\nSlope\n\n\n\n\n0\n1\n2023-01-03\n29\n24\n6\nIntercept 4.28 Slope 0.84 dtype: flo...\n4.280000\n0.840000\n\n\n1\n2\n2023-01-06\n59\n58\n13\nIntercept 30.352941 Slope 1.588235 ...\n30.352941\n1.588235"
  },
  {
    "objectID": "reference/flatten_multiindex_column_names.html",
    "href": "reference/flatten_multiindex_column_names.html",
    "title": "flatten_multiindex_column_names",
    "section": "",
    "text": "flatten_multiindex_column_names(data, sep='_')\nTakes a DataFrame as input and flattens the column names if they are in a multi-index format."
  },
  {
    "objectID": "reference/flatten_multiindex_column_names.html#parameters",
    "href": "reference/flatten_multiindex_column_names.html#parameters",
    "title": "flatten_multiindex_column_names",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe parameter “data” is expected to be a pandas DataFrame object.\nrequired"
  },
  {
    "objectID": "reference/flatten_multiindex_column_names.html#returns",
    "href": "reference/flatten_multiindex_column_names.html#returns",
    "title": "flatten_multiindex_column_names",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe input data with flattened multiindex column names."
  },
  {
    "objectID": "reference/flatten_multiindex_column_names.html#examples",
    "href": "reference/flatten_multiindex_column_names.html#examples",
    "title": "flatten_multiindex_column_names",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\ndate_rng = pd.date_range(start='2023-01-01', end='2023-01-03', freq='D')\n\ndata = {\n    'date': date_rng,\n    ('values', 'value1'): [1, 4, 7],\n    ('values', 'value2'): [2, 5, 8],\n    ('metrics', 'metric1'): [3, 6, 9],\n    ('metrics', 'metric2'): [3, 6, 9],\n}\ndf = pd.DataFrame(data)\n\ndf.flatten_multiindex_column_names()\n\n\n\n\n\n\n\n\ndate\nvalues_value1\nvalues_value2\nmetrics_metric1\nmetrics_metric2\n\n\n\n\n0\n2023-01-01\n1\n2\n3\n3\n\n\n1\n2023-01-02\n4\n5\n6\n6\n\n\n2\n2023-01-03\n7\n8\n9\n9"
  },
  {
    "objectID": "reference/future_frame.html",
    "href": "reference/future_frame.html",
    "title": "future_frame",
    "section": "",
    "text": "future_frame(data, date_column, length_out, force_regular=False, bind_data=True)\nExtend a DataFrame or GroupBy object with future dates.\nThe future_frame function extends a given DataFrame or GroupBy object with future dates based on a specified length, optionally binding the original data."
  },
  {
    "objectID": "reference/future_frame.html#parameters",
    "href": "reference/future_frame.html#parameters",
    "title": "future_frame",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to extend with future dates.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to generate future dates.\nrequired\n\n\nlength_out\nint\nThe length_out parameter specifies the number of future dates to be added to the DataFrame.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether the frequency of the future dates should be forced to be regular. If force_regular is set to True, the frequency of the future dates will be forced to be regular. If force_regular is set to False, the frequency of the future dates will be inferred from the input data (e.g. business calendars might be used). The default value is False.\nFalse\n\n\nbind_data\nbool\nThe bind_data parameter is a boolean flag that determines whether the extended data should be concatenated with the original data or returned separately. If bind_data is set to True, the extended data will be concatenated with the original data using pd.concat. If bind_data is set to False, the extended data will be returned separately. The default value is True.\nTrue"
  },
  {
    "objectID": "reference/future_frame.html#returns",
    "href": "reference/future_frame.html#returns",
    "title": "future_frame",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nAn extended DataFrame with future dates."
  },
  {
    "objectID": "reference/future_frame.html#see-also",
    "href": "reference/future_frame.html#see-also",
    "title": "future_frame",
    "section": "See Also",
    "text": "See Also\nmake_future_timeseries: Generate future dates for a time series."
  },
  {
    "objectID": "reference/future_frame.html#examples",
    "href": "reference/future_frame.html#examples",
    "title": "future_frame",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\ndf = tk.load_dataset('m4_hourly', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490\n\n\n...\n...\n...\n...\n\n\n3055\nH410\n2017-02-10 07:00:00+00:00\n108\n\n\n3056\nH410\n2017-02-10 08:00:00+00:00\n70\n\n\n3057\nH410\n2017-02-10 09:00:00+00:00\n72\n\n\n3058\nH410\n2017-02-10 10:00:00+00:00\n79\n\n\n3059\nH410\n2017-02-10 11:00:00+00:00\n77\n\n\n\n\n3060 rows × 3 columns\n\n\n\n\n# Extend the data for a single time series group by 12 hours\nextended_df = (\n    df\n        .query('id == \"H10\"')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12\n        )\n        .assign(id = lambda x: x['id'].ffill())\n)\nextended_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513.0\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512.0\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506.0\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500.0\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490.0\n\n\n...\n...\n...\n...\n\n\n707\nH10\n2015-07-30 23:00:00\nNaN\n\n\n708\nH10\n2015-07-31 00:00:00\nNaN\n\n\n709\nH10\n2015-07-31 01:00:00\nNaN\n\n\n710\nH10\n2015-07-31 02:00:00\nNaN\n\n\n711\nH10\n2015-07-31 03:00:00\nNaN\n\n\n\n\n712 rows × 3 columns\n\n\n\n\n# Extend the data for each group by 12 hours\nextended_df = (\n    df\n        .groupby('id')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513.0\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512.0\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506.0\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500.0\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490.0\n\n\n...\n...\n...\n...\n\n\n707\nH50\n2015-07-30 23:00:00\nNaN\n\n\n708\nH50\n2015-07-31 00:00:00\nNaN\n\n\n709\nH50\n2015-07-31 01:00:00\nNaN\n\n\n710\nH50\n2015-07-31 02:00:00\nNaN\n\n\n711\nH50\n2015-07-31 03:00:00\nNaN\n\n\n\n\n3108 rows × 3 columns\n\n\n\n\n# Same as above, but just return the extended data with bind_data=False\nextended_df = (\n    df\n        .groupby('id')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            bind_data   = False # Returns just future data\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nid\n\n\n\n\n0\n2015-07-30 16:00:00\nH10\n\n\n1\n2015-07-30 17:00:00\nH10\n\n\n2\n2015-07-30 18:00:00\nH10\n\n\n3\n2015-07-30 19:00:00\nH10\n\n\n4\n2015-07-30 20:00:00\nH10\n\n\n5\n2015-07-30 21:00:00\nH10\n\n\n6\n2015-07-30 22:00:00\nH10\n\n\n7\n2015-07-30 23:00:00\nH10\n\n\n8\n2015-07-31 00:00:00\nH10\n\n\n9\n2015-07-31 01:00:00\nH10\n\n\n10\n2015-07-31 02:00:00\nH10\n\n\n11\n2015-07-31 03:00:00\nH10\n\n\n0\n2013-09-30 16:00:00\nH150\n\n\n1\n2013-09-30 17:00:00\nH150\n\n\n2\n2013-09-30 18:00:00\nH150\n\n\n3\n2013-09-30 19:00:00\nH150\n\n\n4\n2013-09-30 20:00:00\nH150\n\n\n5\n2013-09-30 21:00:00\nH150\n\n\n6\n2013-09-30 22:00:00\nH150\n\n\n7\n2013-09-30 23:00:00\nH150\n\n\n8\n2013-10-01 00:00:00\nH150\n\n\n9\n2013-10-01 01:00:00\nH150\n\n\n10\n2013-10-01 02:00:00\nH150\n\n\n11\n2013-10-01 03:00:00\nH150\n\n\n0\n2017-02-10 12:00:00\nH410\n\n\n1\n2017-02-10 13:00:00\nH410\n\n\n2\n2017-02-10 14:00:00\nH410\n\n\n3\n2017-02-10 15:00:00\nH410\n\n\n4\n2017-02-10 16:00:00\nH410\n\n\n5\n2017-02-10 17:00:00\nH410\n\n\n6\n2017-02-10 18:00:00\nH410\n\n\n7\n2017-02-10 19:00:00\nH410\n\n\n8\n2017-02-10 20:00:00\nH410\n\n\n9\n2017-02-10 21:00:00\nH410\n\n\n10\n2017-02-10 22:00:00\nH410\n\n\n11\n2017-02-10 23:00:00\nH410\n\n\n0\n2015-07-30 16:00:00\nH50\n\n\n1\n2015-07-30 17:00:00\nH50\n\n\n2\n2015-07-30 18:00:00\nH50\n\n\n3\n2015-07-30 19:00:00\nH50\n\n\n4\n2015-07-30 20:00:00\nH50\n\n\n5\n2015-07-30 21:00:00\nH50\n\n\n6\n2015-07-30 22:00:00\nH50\n\n\n7\n2015-07-30 23:00:00\nH50\n\n\n8\n2015-07-31 00:00:00\nH50\n\n\n9\n2015-07-31 01:00:00\nH50\n\n\n10\n2015-07-31 02:00:00\nH50\n\n\n11\n2015-07-31 03:00:00\nH50\n\n\n\n\n\n\n\n\n # Working with irregular dates: Business Days (Stocks Data)\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Allow irregular future dates (i.e. business days)\nextended_df = (\n    df\n        .groupby('symbol')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            force_regular = False, # Allow irregular future dates (i.e. business days)),\n            bind_data   = False\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nsymbol\n\n\n\n\n0\n2023-09-22\nAAPL\n\n\n1\n2023-09-25\nAAPL\n\n\n2\n2023-09-26\nAAPL\n\n\n3\n2023-09-27\nAAPL\n\n\n4\n2023-09-28\nAAPL\n\n\n...\n...\n...\n\n\n7\n2023-10-03\nNVDA\n\n\n8\n2023-10-04\nNVDA\n\n\n9\n2023-10-05\nNVDA\n\n\n10\n2023-10-06\nNVDA\n\n\n11\n2023-10-09\nNVDA\n\n\n\n\n72 rows × 2 columns\n\n\n\n\n# Force regular: Include Weekends\nextended_df = (\n    df\n        .groupby('symbol')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            force_regular = True, # Force regular future dates (i.e. include weekends)),\n            bind_data   = False\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nsymbol\n\n\n\n\n0\n2023-09-22\nAAPL\n\n\n1\n2023-09-23\nAAPL\n\n\n2\n2023-09-24\nAAPL\n\n\n3\n2023-09-25\nAAPL\n\n\n4\n2023-09-26\nAAPL\n\n\n...\n...\n...\n\n\n7\n2023-09-29\nNVDA\n\n\n8\n2023-09-30\nNVDA\n\n\n9\n2023-10-01\nNVDA\n\n\n10\n2023-10-02\nNVDA\n\n\n11\n2023-10-03\nNVDA\n\n\n\n\n72 rows × 2 columns"
  },
  {
    "objectID": "reference/get_date_summary.html",
    "href": "reference/get_date_summary.html",
    "title": "get_date_summary",
    "section": "",
    "text": "get_date_summary(idx)\nReturns a summary of the date-related information, including the number of dates, the time zone, the start date, and the end date.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe parameter idx can be either a pandas Series or a pandas DateTimeIndex. It represents the dates or timestamps for which we want to generate a summary.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA pandas DataFrame with the following columns: - date_n: The number of dates in the index. - date_tz: The time zone of the dates in the index. - date_start: The first date in the index. - date_end: The last date in the index."
  },
  {
    "objectID": "reference/get_date_summary.html#parameters",
    "href": "reference/get_date_summary.html#parameters",
    "title": "get_date_summary",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe parameter idx can be either a pandas Series or a pandas DateTimeIndex. It represents the dates or timestamps for which we want to generate a summary.\nrequired"
  },
  {
    "objectID": "reference/get_date_summary.html#returns",
    "href": "reference/get_date_summary.html#returns",
    "title": "get_date_summary",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\npd.DataFrame\nA pandas DataFrame with the following columns: - date_n: The number of dates in the index. - date_tz: The time zone of the dates in the index. - date_start: The first date in the index. - date_end: The last date in the index."
  },
  {
    "objectID": "reference/get_frequency.html",
    "href": "reference/get_frequency.html",
    "title": "get_frequency",
    "section": "",
    "text": "get_frequency(idx, force_regular=False)\nGet the frequency of a pandas Series or DatetimeIndex.\nThe function get_frequency first attempts to get a pandas inferred frequency. If the inferred frequency is None, it will attempt calculate the frequency manually. If the frequency cannot be determined, the function will raise a ValueError.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/get_frequency.html#parameters",
    "href": "reference/get_frequency.html#parameters",
    "title": "get_frequency",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse"
  },
  {
    "objectID": "reference/get_frequency.html#returns",
    "href": "reference/get_frequency.html#returns",
    "title": "get_frequency",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/get_holiday_signature.html",
    "href": "reference/get_holiday_signature.html",
    "title": "get_holiday_signature",
    "section": "",
    "text": "get_holiday_signature(idx, country_name='UnitedStates')\nEngineers 4 different holiday features from a single datetime for 80+ countries.\nNote: Requires the holidays package to be installed. See https://pypi.org/project/holidays/ for more information."
  },
  {
    "objectID": "reference/get_holiday_signature.html#parameters",
    "href": "reference/get_holiday_signature.html#parameters",
    "title": "get_holiday_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.DatetimeIndex or pd.Series\nA pandas DatetimeIndex or Series containing the dates for which you want to get the holiday signature.\nrequired\n\n\ncountry_name\nstr\nThe name of the country for which to generate holiday features. Defaults to United States holidays, but the following countries are currently available and accessible by the full name or ISO code: Any of the following are acceptable keys for country_name: Available Countries: Full Country, Abrv. #1, #2, #3 Angola: Angola, AO, AGO, Argentina: Argentina, AR, ARG, Aruba: Aruba, AW, ABW, Australia: Australia, AU, AUS, Austria: Austria, AT, AUT, Bangladesh: Bangladesh, BD, BGD, Belarus: Belarus, BY, BLR, Belgium: Belgium, BE, BEL, Botswana: Botswana, BW, BWA, Brazil: Brazil, BR, BRA, Bulgaria: Bulgaria, BG, BLG, Burundi: Burundi, BI, BDI, Canada: Canada, CA, CAN, Chile: Chile, CL, CHL, Colombia: Colombia, CO, COL, Croatia: Croatia, HR, HRV, Curacao: Curacao, CW, CUW, Czechia: Czechia, CZ, CZE, Denmark: Denmark, DK, DNK, Djibouti: Djibouti, DJ, DJI, Dominican Republic: DominicanRepublic, DO, DOM, Egypt: Egypt, EG, EGY, England: England, Estonia: Estonia, EE, EST, European Central Bank: EuropeanCentralBank, Finland: Finland, FI, FIN, France: France, FR, FRA, Georgia: Georgia, GE, GEO, Germany: Germany, DE, DEU, Greece: Greece, GR, GRC, Honduras: Honduras, HN, HND, Hong Kong: HongKong, HK, HKG, Hungary: Hungary, HU, HUN, Iceland: Iceland, IS, ISL, India: India, IN, IND, Ireland: Ireland, IE, IRL, Isle Of Man: IsleOfMan, Israel: Israel, IL, ISR, Italy: Italy, IT, ITA, Jamaica: Jamaica, JM, JAM, Japan: Japan, JP, JPN, Kenya: Kenya, KE, KEN, Korea: Korea, KR, KOR, Latvia: Latvia, LV, LVA, Lithuania: Lithuania, LT, LTU, Luxembourg: Luxembourg, LU, LUX, Malaysia: Malaysia, MY, MYS, Malawi: Malawi, MW, MWI, Mexico: Mexico, MX, MEX, Morocco: Morocco, MA, MOR, Mozambique: Mozambique, MZ, MOZ, Netherlands: Netherlands, NL, NLD, NewZealand: NewZealand, NZ, NZL, Nicaragua: Nicaragua, NI, NIC, Nigeria: Nigeria, NG, NGA, Northern Ireland: NorthernIreland, Norway: Norway, NO, NOR, Paraguay: Paraguay, PY, PRY, Peru: Peru, PE, PER, Poland: Poland, PL, POL, Portugal: Portugal, PT, PRT, Portugal Ext: PortugalExt, PTE, Romania: Romania, RO, ROU, Russia: Russia, RU, RUS, Saudi Arabia: SaudiArabia, SA, SAU, Scotland: Scotland, Serbia: Serbia, RS, SRB, Singapore: Singapore, SG, SGP, Slovokia: Slovokia, SK, SVK, Slovenia: Slovenia, SI, SVN, South Africa: SouthAfrica, ZA, ZAF, Spain: Spain, ES, ESP, Sweden: Sweden, SE, SWE, Switzerland: Switzerland, CH, CHE, Turkey: Turkey, TR, TUR, Ukraine: Ukraine, UA, UKR, United Arab Emirates: UnitedArabEmirates, AE, ARE, United Kingdom: UnitedKingdom, GB, GBR, UK, United States: UnitedStates, US, USA, Venezuela: Venezuela, YV, VEN, Vietnam: Vietnam, VN, VNM, Wales: Wales\n'UnitedStates'"
  },
  {
    "objectID": "reference/get_holiday_signature.html#returns",
    "href": "reference/get_holiday_signature.html#returns",
    "title": "get_holiday_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame:\nA pandas DataFrame with three holiday-specific features: - is_holiday: (0, 1) indicator for holiday - before_holiday: (0, 1) indicator for day before holiday - after_holiday: (0, 1) indicator for day after holiday - holiday_name: name of the holiday"
  },
  {
    "objectID": "reference/get_holiday_signature.html#example",
    "href": "reference/get_holiday_signature.html#example",
    "title": "get_holiday_signature",
    "section": "Example",
    "text": "Example\n\nimport pandas as pd\nimport pytimetk as tk\n\n# Make a DataFrame with a date column\nstart_date = '2023-01-01'\nend_date = '2023-01-10'\ndates = pd.date_range(start=start_date, end=end_date)\n\n# Get holiday features for US\ntk.get_holiday_signature(dates, 'UnitedStates')\n\n\n\n\n\n\n\n\nidx\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n1\n0\nNew Year's Day\n\n\n1\n2023-01-02\n1\n0\n1\nNew Year's Day (Observed)\n\n\n2\n2023-01-03\n0\n0\n1\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN\n\n\n\n\n\n\n\n\n# Get holiday features for France\ntk.get_holiday_signature(dates, 'France')\n\n\n\n\n\n\n\n\nidx\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n0\n0\nNew Year's Day\n\n\n1\n2023-01-02\n0\n0\n1\nNaN\n\n\n2\n2023-01-03\n0\n0\n0\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN\n\n\n\n\n\n\n\n\n# Pandas Series\npd.Series(dates, name='dates').get_holiday_signature('UnitedStates')\n\n\n\n\n\n\n\n\ndates\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n1\n0\nNew Year's Day\n\n\n1\n2023-01-02\n1\n0\n1\nNew Year's Day (Observed)\n\n\n2\n2023-01-03\n0\n0\n1\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN"
  },
  {
    "objectID": "reference/get_timeseries_signature.html",
    "href": "reference/get_timeseries_signature.html",
    "title": "get_timeseries_signature",
    "section": "",
    "text": "get_timeseries_signature(idx)\nConvert a timestamp to a set of 29 time series features.\nThe function tk_get_timeseries_signature engineers 29 different date and time based features from a single datetime index idx:"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#parameters",
    "href": "reference/get_timeseries_signature.html#parameters",
    "title": "get_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nidx is a pandas Series object containing datetime values. Alternatively a pd.DatetimeIndex can be passed.\nrequired"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#returns",
    "href": "reference/get_timeseries_signature.html#returns",
    "title": "get_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function tk_get_timeseries_signature returns a pandas DataFrame that contains 29 different date and time based features derived from a single datetime column."
  },
  {
    "objectID": "reference/get_timeseries_signature.html#examples",
    "href": "reference/get_timeseries_signature.html#examples",
    "title": "get_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\npd.set_option('display.max_columns', None)\n\ndates = pd.date_range(start = '2019-01', end = '2019-03', freq = 'D')\n\n# Makes 29 new time series features from the dates\ntk.get_timeseries_signature(dates).head()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\nquarterend\nmonth\nmonth_lbl\nmonthstart\nmonthend\nyweek\nmweek\nwday\nwday_lbl\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1546300800\n2019\n2019\n1\n0\n0\n1\n1\n2019Q1\n1\n0\n1\nJanuary\n1\n0\n1\n1\n2\nTuesday\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1546387200\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n3\nWednesday\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1546473600\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n4\nThursday\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1546560000\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1546646400\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n6\nSaturday\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Visualize time series data with one line of code.\n\n\n\nplot_timeseries\nCreates time series plots using different plotting engines such as Plotnine, Matplotlib, and Plotly.\n\n\n\n\n\n\nBend time series data to your will.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\npad_by_time\nMake irregular time series regular by padding with missing dates.\n\n\nfuture_frame\nExtend a DataFrame or GroupBy object with future dates.\n\n\n\n\n\n\nAdd one or more feature columns to time series data.\n\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame.\n\n\naugment_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries.\n\n\naugment_lags\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_leads\nAdds leads to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_rolling\nApply one or more rolling functions and window sizes to one or more columns of a DataFrame.\n\n\naugment_expanding\nApply one or more expanding functions and window sizes to one or more columns of a DataFrame.\n\n\n\n\n\n\nPython implementation of the R package tsfeatures.\n\n\n\nts_features\nExtracts aggregated time series features from a DataFrame or DataFrameGroupBy object using the tsfeatures package.\n\n\nts_summary\nComputes summary statistics for a time series data, either for the entire dataset or grouped by a specific column.\n\n\n\n\n\n\nTime series functions that generate / manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nmake_weekday_sequence\nGenerate a sequence of weekday dates within a specified date range, optionally excluding weekends and holidays.\n\n\nmake_weekend_sequence\nGenerate a sequence of weekend dates within a specified date range, optionally excluding holidays.\n\n\nget_date_summary\nReturns a summary of the date-related information, including the number of dates, the time zone, the start\n\n\nget_frequency_summary\nMore robust version of pandas inferred frequency.\n\n\nget_diff_summary\nCalculates summary statistics of the time differences between consecutive values in a datetime index.\n\n\nget_frequency\nGet the frequency of a pandas Series or DatetimeIndex.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nget_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries.\n\n\n\n\n\n\nHelper functions to make your life easier.\n\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nis_holiday\nCheck if a given list of dates are holidays for a specified country.\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month.\n\n\ntimeseries_unit_frequency_table\nThe function timeseries_unit_frequency_table returns a pandas DataFrame with units of time and\n\n\n\n\n\n\nHelper functions to make your life easier.\n\n\n\ntheme_timetk\nReturns a plotnine theme with timetk styles applied, allowing for customization of the appearance of plots in Python.\n\n\npalette_timetk\nThe function palette_timetk returns a dictionary of color codes for various colors in the timetk theme.\n\n\n\n\n\n\n\n\n\nglimpse\nTakes a pandas DataFrame and prints a summary of\n\n\nflatten_multiindex_column_names\nTakes a DataFrame as input and flattens the column\n\n\n\n\n\n\nPractice pytimetk with 13 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 12 datasets that can be loaded with pytimetk.load_dataset.\n\n\nload_dataset\nLoad one of 12 Time Series Datasets."
  },
  {
    "objectID": "reference/index.html#data-visualization",
    "href": "reference/index.html#data-visualization",
    "title": "Function reference",
    "section": "",
    "text": "Visualize time series data with one line of code.\n\n\n\nplot_timeseries\nCreates time series plots using different plotting engines such as Plotnine, Matplotlib, and Plotly."
  },
  {
    "objectID": "reference/index.html#wrangling-pandas-time-series-dataframes",
    "href": "reference/index.html#wrangling-pandas-time-series-dataframes",
    "title": "Function reference",
    "section": "",
    "text": "Bend time series data to your will.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\npad_by_time\nMake irregular time series regular by padding with missing dates.\n\n\nfuture_frame\nExtend a DataFrame or GroupBy object with future dates."
  },
  {
    "objectID": "reference/index.html#adding-features-to-time-series-dataframes-augmenting",
    "href": "reference/index.html#adding-features-to-time-series-dataframes-augmenting",
    "title": "Function reference",
    "section": "",
    "text": "Add one or more feature columns to time series data.\n\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame.\n\n\naugment_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries.\n\n\naugment_lags\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_leads\nAdds leads to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_rolling\nApply one or more rolling functions and window sizes to one or more columns of a DataFrame.\n\n\naugment_expanding\nApply one or more expanding functions and window sizes to one or more columns of a DataFrame."
  },
  {
    "objectID": "reference/index.html#ts-features",
    "href": "reference/index.html#ts-features",
    "title": "Function reference",
    "section": "",
    "text": "Python implementation of the R package tsfeatures.\n\n\n\nts_features\nExtracts aggregated time series features from a DataFrame or DataFrameGroupBy object using the tsfeatures package.\n\n\nts_summary\nComputes summary statistics for a time series data, either for the entire dataset or grouped by a specific column."
  },
  {
    "objectID": "reference/index.html#time-series-for-pandas-series",
    "href": "reference/index.html#time-series-for-pandas-series",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that generate / manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nmake_weekday_sequence\nGenerate a sequence of weekday dates within a specified date range, optionally excluding weekends and holidays.\n\n\nmake_weekend_sequence\nGenerate a sequence of weekend dates within a specified date range, optionally excluding holidays.\n\n\nget_date_summary\nReturns a summary of the date-related information, including the number of dates, the time zone, the start\n\n\nget_frequency_summary\nMore robust version of pandas inferred frequency.\n\n\nget_diff_summary\nCalculates summary statistics of the time differences between consecutive values in a datetime index.\n\n\nget_frequency\nGet the frequency of a pandas Series or DatetimeIndex.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nget_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries."
  },
  {
    "objectID": "reference/index.html#date-utilities",
    "href": "reference/index.html#date-utilities",
    "title": "Function reference",
    "section": "",
    "text": "Helper functions to make your life easier.\n\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nis_holiday\nCheck if a given list of dates are holidays for a specified country.\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month.\n\n\ntimeseries_unit_frequency_table\nThe function timeseries_unit_frequency_table returns a pandas DataFrame with units of time and"
  },
  {
    "objectID": "reference/index.html#visualization-utilities",
    "href": "reference/index.html#visualization-utilities",
    "title": "Function reference",
    "section": "",
    "text": "Helper functions to make your life easier.\n\n\n\ntheme_timetk\nReturns a plotnine theme with timetk styles applied, allowing for customization of the appearance of plots in Python.\n\n\npalette_timetk\nThe function palette_timetk returns a dictionary of color codes for various colors in the timetk theme."
  },
  {
    "objectID": "reference/index.html#extra-pandas-helpers-that-help-beyond-just-time-series",
    "href": "reference/index.html#extra-pandas-helpers-that-help-beyond-just-time-series",
    "title": "Function reference",
    "section": "",
    "text": "glimpse\nTakes a pandas DataFrame and prints a summary of\n\n\nflatten_multiindex_column_names\nTakes a DataFrame as input and flattens the column"
  },
  {
    "objectID": "reference/index.html#datasets",
    "href": "reference/index.html#datasets",
    "title": "Function reference",
    "section": "",
    "text": "Practice pytimetk with 13 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 12 datasets that can be loaded with pytimetk.load_dataset.\n\n\nload_dataset\nLoad one of 12 Time Series Datasets."
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "load_dataset(name='m4_daily', verbose=False, **kwargs)\nLoad one of 12 Time Series Datasets.\nThe load_dataset function is used to load various time series datasets by name, with options to print the available datasets and pass additional arguments to pandas.read_csv. The available datasets are:\nThe datasets can be loaded with pytimetk.load_dataset(name), where name is the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned above."
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name parameter is used to specify the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned in the function’s docstring.\n'm4_daily'\n\n\nverbose\nbool\nThe verbose parameter is a boolean flag that determines whether or not to print the names of the available datasets. If verbose is set to True, the function will print the names of the available datasets. If verbose is set to False, the function will not print anything.\nFalse\n\n\n**kwargs\n\nThe **kwargs parameter is used to pass additional arguments to pandas.read_csv.\n{}"
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe load_dataset function returns the requested dataset as a pandas DataFrame."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\n\n# Stocks Daily Dataset: META, APPL, AMZN, NFLX, NVDA, GOOG\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Bike Sales CRM Sample Dataset\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Taylor 30-Minute Power Demand Dataset\ndf = tk.load_dataset('taylor_30_min', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2000-06-05 00:00:00+00:00\n22262\n\n\n1\n2000-06-05 00:30:00+00:00\n21756\n\n\n2\n2000-06-05 01:00:00+00:00\n22247\n\n\n3\n2000-06-05 01:30:00+00:00\n22759\n\n\n4\n2000-06-05 02:00:00+00:00\n22549\n\n\n...\n...\n...\n\n\n4027\n2000-08-27 21:30:00+00:00\n27946\n\n\n4028\n2000-08-27 22:00:00+00:00\n27133\n\n\n4029\n2000-08-27 22:30:00+00:00\n25996\n\n\n4030\n2000-08-27 23:00:00+00:00\n24610\n\n\n4031\n2000-08-27 23:30:00+00:00\n23132\n\n\n\n\n4032 rows × 2 columns"
  },
  {
    "objectID": "reference/make_weekday_sequence.html",
    "href": "reference/make_weekday_sequence.html",
    "title": "make_weekday_sequence",
    "section": "",
    "text": "make_weekday_sequence(start_date, end_date, sunday_to_thursday=False, remove_holidays=False, country=None)\nGenerate a sequence of weekday dates within a specified date range, optionally excluding weekends and holidays."
  },
  {
    "objectID": "reference/make_weekday_sequence.html#parameters",
    "href": "reference/make_weekday_sequence.html#parameters",
    "title": "make_weekday_sequence",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nstr or datetime or pd.DatetimeIndex\nThe start date of the date range.\nrequired\n\n\nend_date\nstr or datetime or pd.DatetimeIndex\nThe end date of the date range.\nrequired\n\n\nsunday_to_thursday\nbool\nIf True, generates a sequence with Sunday to Thursday weekdays (excluding Friday and Saturday). If False (default), generates a sequence with Monday to Friday weekdays.\nFalse\n\n\nremove_holidays\n(bool, optional)\nIf True, excludes holidays (based on the specified country) from the generated sequence. If False (default), includes holidays in the sequence.\nFalse\n\n\ncountry\nstr\nThe name of the country for which to generate holiday-specific sequences. Defaults to None, which uses the United States as the default country.\nNone"
  },
  {
    "objectID": "reference/make_weekday_sequence.html#returns",
    "href": "reference/make_weekday_sequence.html#returns",
    "title": "make_weekday_sequence",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA Series containing the generated weekday dates."
  },
  {
    "objectID": "reference/make_weekday_sequence.html#examples",
    "href": "reference/make_weekday_sequence.html#examples",
    "title": "make_weekday_sequence",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\n# United States has Monday to Friday as weekdays (excluding Saturday and Sunday and holidays)\ntk.make_weekday_sequence(\"2023-01-01\", \"2023-01-15\", sunday_to_thursday=False, remove_holidays=True, country='UnitedStates')\n\n0   2023-01-03\n1   2023-01-04\n2   2023-01-05\n3   2023-01-06\n4   2023-01-09\n5   2023-01-10\n6   2023-01-11\n7   2023-01-12\n8   2023-01-13\nName: Weekday Dates, dtype: datetime64[ns]\n\n\n\n# Israel has Sunday to Thursday as weekdays (excluding Friday and Saturday and Israel holidays)\ntk.make_weekday_sequence(\"2023-01-01\", \"2023-01-15\", sunday_to_thursday=True, remove_holidays=True, country='Israel')\n\n0    2023-01-01\n1    2023-01-02\n2    2023-01-03\n3    2023-01-04\n4    2023-01-05\n5    2023-01-08\n6    2023-01-09\n7    2023-01-10\n8    2023-01-11\n9    2023-01-12\n10   2023-01-15\nName: Weekday Dates, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/pad_by_time.html",
    "href": "reference/pad_by_time.html",
    "title": "pad_by_time",
    "section": "",
    "text": "pad_by_time(data, date_column, freq='D', start_date=None, end_date=None)\nMake irregular time series regular by padding with missing dates.\nThe pad_by_time function inserts missing dates into a Pandas DataFrame or DataFrameGroupBy object, through the process making an irregularly spaced time series regularly spaced."
  },
  {
    "objectID": "reference/pad_by_time.html#parameters",
    "href": "reference/pad_by_time.html#parameters",
    "title": "pad_by_time",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter can be either a Pandas DataFrame or a Pandas DataFrameGroupBy object. It represents the data that you want to pad with missing dates.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to determine the minimum and maximum dates in theDataFrame, and to generate the regular date range for padding.\nrequired\n\n\nfreq\nstr\nThe freq parameter specifies the frequency at which the missing timestamps should be generated. It accepts a string representing a pandas frequency alias. Some common frequency aliases include: - S: secondly frequency - min: minute frequency - H: hourly frequency - B: business day frequency - D: daily frequency - W: weekly frequency - M: month end frequency - MS: month start frequency - BMS: Business month start - Q: quarter end frequency - QS: quarter start frequency - Y: year end frequency - YS: year start frequency\n'D'\n\n\nstart_date\nstr\nSpecifies the start of the padded series. If NULL, it will use the lowest value of the input variable. In the case of groups, it will use the lowest value by group.\nNone\n\n\nend_date\nstr\nSpecifies the end of the padded series. If NULL, it will use the highest value of the input variable. In the case of groups, it will use the highest value by group.\nNone"
  },
  {
    "objectID": "reference/pad_by_time.html#returns",
    "href": "reference/pad_by_time.html#returns",
    "title": "pad_by_time",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe function pad_by_time returns a Pandas DataFrame that has been extended with future dates."
  },
  {
    "objectID": "reference/pad_by_time.html#examples",
    "href": "reference/pad_by_time.html#examples",
    "title": "pad_by_time",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport pytimetk as tk\n\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Pad Single Time Series: Fill missing dates\npadded_df = (\n    df\n        .query('symbol == \"AAPL\"')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'D'\n        )\n        .assign(id = lambda x: x['symbol'].ffill())\n)\npadded_df \n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\nid\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\nAAPL\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\nAAPL\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\nAAPL\n\n\n3\n2013-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n4\n2013-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3910\n2023-09-17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n3911\n2023-09-18\nAAPL\n176.479996\n179.380005\n176.169998\n177.970001\n67257600.0\n177.970001\nAAPL\n\n\n3912\n2023-09-19\nAAPL\n177.520004\n179.630005\n177.130005\n179.070007\n51826900.0\n179.070007\nAAPL\n\n\n3913\n2023-09-20\nAAPL\n179.259995\n179.699997\n175.399994\n175.490005\n58436200.0\n175.490005\nAAPL\n\n\n3914\n2023-09-21\nAAPL\n174.550003\n176.300003\n173.860001\n173.929993\n63047900.0\n173.929993\nAAPL\n\n\n\n\n3915 rows × 9 columns\n\n\n\n\n# Pad by Group: Pad each group with missing dates\npadded_df = (\n    df\n        .groupby('symbol')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'D'\n        )\n)\npadded_df\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\n\n\n3\n2013-01-05\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2013-01-06\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n23485\n2023-09-17\nNVDA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n23486\n2023-09-18\nNVDA\n427.480011\n442.420013\n420.000000\n439.660004\n50027100.0\n439.660004\n\n\n23487\n2023-09-19\nNVDA\n438.329987\n439.660004\n430.019989\n435.200012\n37306400.0\n435.200012\n\n\n23488\n2023-09-20\nNVDA\n436.000000\n439.029999\n422.230011\n422.390015\n36710800.0\n422.390015\n\n\n23489\n2023-09-21\nNVDA\n415.829987\n421.000000\n409.799988\n410.170013\n44893000.0\n410.170013\n\n\n\n\n23490 rows × 8 columns"
  },
  {
    "objectID": "reference/plot_timeseries.html",
    "href": "reference/plot_timeseries.html",
    "title": "plot_timeseries",
    "section": "",
    "text": "plot_timeseries(data, date_column, value_column, color_column=None, color_palette=None, facet_ncol=1, facet_nrow=None, facet_scales='free_y', facet_dir='h', line_color='#2c3e50', line_size=0.65, line_type='solid', line_alpha=1.0, y_intercept=None, y_intercept_color='#2c3e50', x_intercept=None, x_intercept_color='#2c3e50', smooth=True, smooth_color='#3366FF', smooth_frac=0.2, smooth_size=1.0, smooth_alpha=1.0, legend_show=True, title='Time Series Plot', x_lab='', y_lab='', color_lab='Legend', x_axis_date_labels='%b %Y', base_size=11, width=None, height=None, engine='plotly')\nCreates time series plots using different plotting engines such as Plotnine, Matplotlib, and Plotly."
  },
  {
    "objectID": "reference/plot_timeseries.html#parameters",
    "href": "reference/plot_timeseries.html#parameters",
    "title": "plot_timeseries",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe input data for the plot. It can be either a Pandas DataFrame or a Pandas DataFrameGroupBy object.\nrequired\n\n\ndate_column\nstr\nThe name of the column in the DataFrame that contains the dates for the time series data.\nrequired\n\n\nvalue_column\nstr\nThe value_column parameter is used to specify the name of the column in the DataFrame that contains the values for the time series data. This column will be plotted on the y-axis of the time series plot.\nrequired\n\n\ncolor_column\nstr\nThe color_column parameter is an optional parameter that specifies the column in the DataFrame that will be used to assign colors to the different time series. If this parameter is not provided, all time series will have the same color.\nNone\n\n\ncolor_palette\nlist\nThe color_palette parameter is used to specify the colors to be used for the different time series. It accepts a list of color codes or names. If the color_column parameter is not provided, the tk.palette_timetk() color palette will be used.\nNone\n\n\nfacet_ncol\nint\nThe facet_ncol parameter determines the number of columns in the facet grid. It specifies how many subplots will be arranged horizontally in the plot.\n1\n\n\nfacet_nrow\nint\nThe facet_nrow parameter determines the number of rows in the facet grid. It specifies how many subplots will be arranged vertically in the grid.\nNone\n\n\nfacet_scales\nstr\nThe facet_scales parameter determines the scaling of the y-axis in the facetted plots. It can take the following values: - “free_y”: The y-axis scale will be free for each facet, but the x-axis scale will be fixed for all facets. This is the default value. - “free_x”: The y-axis scale will be free for each facet, but the x-axis scale will be fixed for all facets. - “free”: The y-axis scale will be free for each facet (subplot). This is the default value.\n'free_y'\n\n\nfacet_dir\nstr\nThe facet_dir parameter determines the direction in which the facets (subplots) are arranged. It can take two possible values: - “h”: The facets will be arranged horizontally (in rows). This is the default value. - “v”: The facets will be arranged vertically (in columns).\n'h'\n\n\nline_color\nstr\nThe line_color parameter is used to specify the color of the lines in the time series plot. It accepts a string value representing a color code or name. The default value is “#2c3e50”, which corresponds to a dark blue color.\n'#2c3e50'\n\n\nline_size\nfloat\nThe line_size parameter is used to specify the size of the lines in the time series plot. It determines the thickness of the lines.\n0.65\n\n\nline_type\nstr\nThe line_type parameter is used to specify the type of line to be used in the time series plot.\n'solid'\n\n\nline_alpha\nfloat\nThe line_alpha parameter controls the transparency of the lines in the time series plot. It accepts a value between 0 and 1, where 0 means completely transparent (invisible) and 1 means completely opaque (solid).\n1.0\n\n\ny_intercept\nfloat\nThe y_intercept parameter is used to add a horizontal line to the plot at a specific y-value. It can be set to a numeric value to specify the y-value of the intercept. If set to None (default), no y-intercept line will be added to the plot\nNone\n\n\ny_intercept_color\nstr\nThe y_intercept_color parameter is used to specify the color of the y-intercept line in the plot. It accepts a string value representing a color code or name. The default value is “#2c3e50”, which corresponds to a dark blue color. You can change this value.\n'#2c3e50'\n\n\nx_intercept\nstr\nThe x_intercept parameter is used to add a vertical line at a specific x-axis value on the plot. It is used to highlight a specific point or event in the time series data. - By default, it is set to None, which means no vertical line will be added. - You can use a date string to specify the x-axis value of the intercept. For example, “2020-01-01” would add a vertical line at the beginning of the year 2020.\nNone\n\n\nx_intercept_color\nstr\nThe x_intercept_color parameter is used to specify the color of the vertical line that represents the x-intercept in the plot. By default, it is set to “#2c3e50”, which is a dark blue color. You can change this value to any valid color code.\n'#2c3e50'\n\n\nsmooth\nbool\nThe smooth parameter is a boolean indicating whether or not to apply smoothing to the time eries data. If set to True, the time series will be smoothed using the lowess algorithm. The default value is True.\nTrue\n\n\nsmooth_color\nstr\nThe smooth_color parameter is used to specify the color of the smoothed line in the time series plot. It accepts a string value representing a color code or name. The default value is #3366FF, which corresponds to a shade of blue. You can change this value to any valid color code.\n'#3366FF'\n\n\nsmooth_frac\nfloat\nThe smooth_frac parameter is used to control the fraction of data points used for smoothing the time series. It determines the degree of smoothing applied to the data. A smaller value of smooth_frac will result in more smoothing, while a larger value will result in less smoothing. The default value is 0.2.\n0.2\n\n\nsmooth_size\nfloat\nThe smooth_size parameter is used to specify the size of the line used to plot the smoothed values in the time series plot. It is a numeric value that controls the thickness of the line. A larger value will result in a thicker line, while a smaller value will result in a thinner line\n1.0\n\n\nsmooth_alpha\nfloat\nThe smooth_alpha parameter controls the transparency of the smoothed line in the plot. It accepts a value between 0 and 1, where 0 means completely transparent and 1 means completely opaque.\n1.0\n\n\nlegend_show\nbool\nThe legend_show parameter is a boolean indicating whether or not to show the legend in the plot. If set to True, the legend will be displayed. The default value is True.\nTrue\n\n\ntitle\nstr\nThe title of the plot.\n'Time Series Plot'\n\n\nx_lab\nstr\nThe x_lab parameter is used to specify the label for the x-axis in the plot. It is a string that represents the label text.\n''\n\n\ny_lab\nstr\nThe y_lab parameter is used to specify the label for the y-axis in the plot. It is a string that represents the label for the y-axis.\n''\n\n\ncolor_lab\nstr\nThe color_lab parameter is used to specify the label for the legend or color scale in the plot. It is used to provide a description of the colors used in the plot, typically when a color column is specified.\n'Legend'\n\n\nx_axis_date_labels\nstr\nThe x_axis_date_labels parameter is used to specify the format of the date labels on the x-axis of the plot. It accepts a string representing the format of the date labels. For example, “%b %Y” would display the month abbreviation and year (e.g., Jan 2020).\n'%b %Y'\n\n\nbase_size\nfloat\nThe base_size parameter is used to set the base font size for the plot. It determines the size of the text elements such as axis labels, titles, and legends.\n11\n\n\nwidth\nint\nThe width parameter is used to specify the width of the plot. It determines the horizontal size of the plot in pixels.\nNone\n\n\nheight\nint\nThe height parameter is used to specify the height of the plot in pixels. It determines the vertical size of the plot when it is rendered.\nNone\n\n\nengine\nstr\nThe engine parameter specifies the plotting library to use for creating the time series plot. It can take one of the following values: - “plotly” (interactive): Use the plotly library to create the plot. This is the default value. - “plotnine” (static): Use the plotnine library to create the plot. This is the default value. - “matplotlib” (static): Use the matplotlib library to create the plot.\n'plotly'"
  },
  {
    "objectID": "reference/plot_timeseries.html#returns",
    "href": "reference/plot_timeseries.html#returns",
    "title": "plot_timeseries",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function plot_timeseries returns a plot object, depending on the specified engine parameter.\n- If engine is set to ‘plotnine’ or ‘matplotlib’, the function returns a plot object that can be further customized or displayed. - If engine is set to ‘plotly’, the function returns a plotly figure object."
  },
  {
    "objectID": "reference/plot_timeseries.html#examples",
    "href": "reference/plot_timeseries.html#examples",
    "title": "plot_timeseries",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\n\ndf = tk.load_dataset('m4_monthly', parse_dates = ['date'])\n\n# Plotly Object: Single Time Series\nfig = (\n    df\n        .query('id == \"M750\"')\n        .plot_timeseries(\n            'date', 'value', \n            facet_ncol = 1,\n            x_axis_date_labels = \"%Y\",\n            engine = 'plotly',\n        )\n)\nfig\n\n\n                                                \n\n\n\n# Plotly Object: Grouped Time Series\nfig = (\n    df\n        .groupby('id')\n        .plot_timeseries(\n            'date', 'value', \n            facet_ncol = 2,\n            facet_scales = \"free_y\",\n            smooth_frac = 0.2,\n            smooth_size = 2.0,\n            y_intercept = None,\n            x_axis_date_labels = \"%Y\",\n            engine = 'plotly',\n            width = 600,\n            height = 500,\n        )\n)\nfig\n\n\n                                                \n\n\n\n# Plotly Object: Color Column\nfig = (\n    df\n        .plot_timeseries(\n            'date', 'value', \n            color_column = 'id',\n            smooth = False,\n            y_intercept = 0,\n            x_axis_date_labels = \"%Y\",\n            engine = 'plotly',\n        )\n)\nfig\n\n\n                                                \n\n\n\n# Plotnine Object: Single Time Series\nfig = (\n    df\n        .query('id == \"M1\"')\n        .plot_timeseries(\n            'date', 'value', \n            x_axis_date_labels = \"%Y\",\n            engine = 'plotnine'\n        )\n)\nfig\n\n\n\n\n&lt;Figure Size: (700 x 500)&gt;\n\n\n\n# Plotnine Object: Grouped Time Series\nfig = (\n    df\n        .groupby('id')\n        .plot_timeseries(\n            'date', 'value',\n            facet_ncol = 2,\n            facet_scales = \"free\",\n            line_size = 0.35,\n            x_axis_date_labels = \"%Y\",\n            engine = 'plotnine'\n        )\n)\nfig\n\n\n\n\n&lt;Figure Size: (700 x 500)&gt;\n\n\n\n# Plotnine Object: Color Column\nfig = (\n    df\n        .plot_timeseries(\n            'date', 'value', \n            color_column = 'id',\n            smooth = False,\n            y_intercept = 0,\n            x_axis_date_labels = \"%Y\",\n            engine = 'plotnine',\n        )\n)\nfig\n\n\n\n\n&lt;Figure Size: (700 x 500)&gt;\n\n\n\n# Matplotlib object (same as plotnine, but converted to matplotlib object)\nfig = (\n    df\n        .groupby('id')\n        .plot_timeseries(\n            'date', 'value', \n            color_column = 'id',\n            facet_ncol = 2,\n            x_axis_date_labels = \"%Y\",\n            engine = 'matplotlib',\n        )\n)\nfig"
  },
  {
    "objectID": "reference/theme_timetk.html",
    "href": "reference/theme_timetk.html",
    "title": "theme_timetk",
    "section": "",
    "text": "theme_timetk(base_size=11, base_family=['Arial', 'Helvetica', 'sans-serif'], dpi=100, width=700, height=500)\nReturns a plotnine theme with timetk styles applied, allowing for customization of the appearance of plots in Python."
  },
  {
    "objectID": "reference/theme_timetk.html#parameters",
    "href": "reference/theme_timetk.html#parameters",
    "title": "theme_timetk",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbase_size\nint\nThe base_size parameter determines the base font size for the theme. It is set to 11 by default, but you can change it to any desired value.\n11\n\n\nbase_family\nlist\nThe base_family parameter is a list of font families that will be used as the base font for the theme. The default value is ['Arial', 'Helvetica', 'sans-serif'], which means that the theme will use Arial font if available, otherwise it will try Helvetica, and if that is not available either, it will use the generic sans-serif font.\n['Arial', 'Helvetica', 'sans-serif']\n\n\ndpi\nint\nThe dpi parameter stands for dots per inch and determines the resolution of the plot. It specifies the number of pixels per inch in the output image. Higher dpi values result in higher resolution images.\n100\n\n\nwidth\nint\nThe width parameter is used to specify the width of the plot in pixels at dpi. It determines the horizontal size of the plot. The default value is 700 pixels.\n700\n\n\nheight\nint\nThe height parameter is used to specify the height of the plot in inches. It is an optional parameter, so if you don’t provide a value for it, the default height will be 5 inches (500 pixels).\n500"
  },
  {
    "objectID": "reference/theme_timetk.html#returns",
    "href": "reference/theme_timetk.html#returns",
    "title": "theme_timetk",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA theme object that can be used to customize the appearance of plots in R. The theme object contains various elements such as line, rect, axis, panel, legend, strip, and plot, each with their own set of properties that can be customized."
  },
  {
    "objectID": "reference/theme_timetk.html#examples",
    "href": "reference/theme_timetk.html#examples",
    "title": "theme_timetk",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\nfrom plotnine import ggplot, aes, geom_line, labs, scale_x_date, facet_wrap\n\ndata = {\n    'date': pd.date_range(start='2023-01-01', end='2023-01-10'),\n    'value': [1, 3, 7, 9, 11, 14, 18, 21, 24, 29]\n}\ndf = pd.DataFrame(data)\n\n# Plotnine chart without styling\nfig = (\n    ggplot(df, aes(x='date', y='value')) +\n        geom_line(color='blue') +\n        labs(title='Time Series Plot', x='Date', y='Value') +\n        scale_x_date(date_labels='%a')\n)\nfig\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n# Plotnine chart with timetk styling\nfig + tk.theme_timetk()\n\n\n\n\n&lt;Figure Size: (700 x 500)&gt;\n\n\n\n# Faceted plot with timetk styling\ndata = {\n    'date': pd.date_range(start='2023-01-01', end='2023-01-10').tolist() * 2,\n    'value': [1, 3, 7, 9, 11, 14, 18, 21, 24, 29] * 2,\n    'category': ['A'] * 10 + ['B'] * 10,\n}\ndf = pd.DataFrame(data)\n\n(\n    ggplot(df, aes(x='date', y='value')) +\n        geom_line(color='blue') +\n        labs(title='Faceted Time Series Plot', x='Date', y='Value') +\n        facet_wrap('~category') +\n        scale_x_date(date_labels='%a') +\n        tk.theme_timetk()\n)\n\n\n\n\n&lt;Figure Size: (700 x 500)&gt;"
  },
  {
    "objectID": "reference/ts_features.html",
    "href": "reference/ts_features.html",
    "title": "ts_features",
    "section": "",
    "text": "ts_features(data, date_column, value_column, features=None, freq=None, scale=True, threads=1)\nExtracts aggregated time series features from a DataFrame or DataFrameGroupBy object using the tsfeatures package.\nNote: Requires the tsfeatures package to be installed."
  },
  {
    "objectID": "reference/ts_features.html#parameters",
    "href": "reference/ts_features.html#parameters",
    "title": "ts_features",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input data that can be either a Pandas DataFrame or a grouped DataFrame. It contains the time series data that you want to extract features from.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is the name of the column in the input data that contains the dates or timestamps of the time series data.\nrequired\n\n\nvalue_column\nstr\nThe value_column parameter is the name of the column in the DataFrame that contains the time series values.\nrequired\n\n\nfeatures\nlist\nThe features parameter is a list of functions that represent the time series features to be extracted. Each function should take a time series as input and return a scalar value as output. When None, uses the default list of features: - acf_features - arch_stat - crossing_points - entropy - flat_spots - heterogeneity - holt_parameters - lumpiness - nonlinearity - pacf_features - stl_features - stability - hw_parameters - unitroot_kpss - unitroot_pp - series_length - hurst\nNone\n\n\nfreq\nstr\nThe freq parameter specifies the frequency of the time series data. It is used to calculate features that are dependent on the frequency, such as seasonal features. - The frequency can be specified as a string, such as ‘D’ for daily, ‘W’ for weekly, ‘M’ for monthly. - The frequency can be a numeric value representing the number of observations per year, such as 365 for daily, 52 for weekly, 12 for monthly.\nNone\n\n\nscale\nbool\nThe scale parameter in the ts_features function determines whether or not to scale the extracted features. - If scale is set to True, the features will be scaled using z-score normalization. - If scale is set to False, the features will not be scaled.\nTrue\n\n\nthreads\nOptional[int]\nThe threads parameter is an optional parameter that specifies the number of threads to use for parallel processing. - If is None, tthe function will use all available threads on the system. - If is -1, the function will use all available threads on the system.\n1"
  },
  {
    "objectID": "reference/ts_features.html#returns",
    "href": "reference/ts_features.html#returns",
    "title": "ts_features",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe function ts_features returns a pandas DataFrame containing the extracted time series features. If grouped data is provided, the DataFrame will contain the grouping columns as well."
  },
  {
    "objectID": "reference/ts_features.html#examples",
    "href": "reference/ts_features.html#examples",
    "title": "ts_features",
    "section": "Examples",
    "text": "Examples\n\n# REQUIRES tsfeatures: pip install tsfeatures\nimport pandas as pd\nimport pytimetk as tk\n\n# tsfeatures comes with these features:\nfrom tsfeatures import (\n    acf_features, arch_stat, crossing_points,\n    entropy, flat_spots, heterogeneity,\n    holt_parameters, lumpiness, nonlinearity,\n    pacf_features, stl_features, stability,\n    hw_parameters, unitroot_kpss, unitroot_pp,\n    series_length, hurst\n)\n\ndf = tk.load_dataset('m4_daily', parse_dates = ['date'])\n\n# Feature Extraction\nfeature_df = (\n    df\n        .groupby('id')\n        .ts_features(    \n            date_column  = 'date', \n            value_column = 'value',\n            features     = [acf_features, hurst],\n            freq         = 7,\n            threads      = 1\n        )\n) \nfeature_df\n\n\n\n\n\n\n\n\nid\nhurst\nx_acf1\nx_acf10\ndiff1_acf1\ndiff1_acf10\ndiff2_acf1\ndiff2_acf10\nseas_acf1\n\n\n\n\n0\nD10\n0.966295\n0.984991\n8.366800\n0.002487\n0.020569\n-0.517569\n0.293474\n0.889696\n\n\n1\nD160\nNaN\n0.999208\n9.913240\n0.025369\n0.012643\n-0.473298\n0.246242\n0.994513\n\n\n2\nD410\n1.005350\n0.993756\n9.314835\n0.102720\n0.032648\n-0.437454\n0.256661\n0.956028\n\n\n3\nD500\n0.926306\n0.998401\n9.839732\n0.004199\n0.005579\n-0.488000\n0.241043\n0.989937"
  },
  {
    "objectID": "reference/week_of_month.html",
    "href": "reference/week_of_month.html",
    "title": "week_of_month",
    "section": "",
    "text": "week_of_month(idx)\nThe “week_of_month” function calculates the week number of a given date within its month."
  },
  {
    "objectID": "reference/week_of_month.html#parameters",
    "href": "reference/week_of_month.html#parameters",
    "title": "week_of_month",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe parameter “idx” is a pandas Series object that represents a specific date for which you want to determine the week of the month.\nrequired"
  },
  {
    "objectID": "reference/week_of_month.html#returns",
    "href": "reference/week_of_month.html#returns",
    "title": "week_of_month",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe week of the month for a given date."
  },
  {
    "objectID": "reference/week_of_month.html#examples",
    "href": "reference/week_of_month.html#examples",
    "title": "week_of_month",
    "section": "Examples",
    "text": "Examples\n\nimport pytimetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"1D\")\ndates\n\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10', '2020-01-11', '2020-01-12',\n               '2020-01-13', '2020-01-14', '2020-01-15', '2020-01-16',\n               '2020-01-17', '2020-01-18', '2020-01-19', '2020-01-20',\n               '2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24',\n               '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28',\n               '2020-01-29', '2020-01-30', '2020-01-31', '2020-02-01',\n               '2020-02-02', '2020-02-03', '2020-02-04', '2020-02-05',\n               '2020-02-06', '2020-02-07', '2020-02-08', '2020-02-09',\n               '2020-02-10', '2020-02-11', '2020-02-12', '2020-02-13',\n               '2020-02-14', '2020-02-15', '2020-02-16', '2020-02-17',\n               '2020-02-18', '2020-02-19', '2020-02-20', '2020-02-21',\n               '2020-02-22', '2020-02-23', '2020-02-24', '2020-02-25',\n               '2020-02-26', '2020-02-27', '2020-02-28'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n# Works on DateTimeIndex\ntk.week_of_month(dates)\n\n0     1\n1     1\n2     1\n3     1\n4     1\n5     1\n6     1\n7     2\n8     2\n9     2\n10    2\n11    2\n12    2\n13    2\n14    3\n15    3\n16    3\n17    3\n18    3\n19    3\n20    3\n21    4\n22    4\n23    4\n24    4\n25    4\n26    4\n27    4\n28    5\n29    5\n30    5\n31    1\n32    1\n33    1\n34    1\n35    1\n36    1\n37    1\n38    2\n39    2\n40    2\n41    2\n42    2\n43    2\n44    2\n45    3\n46    3\n47    3\n48    3\n49    3\n50    3\n51    3\n52    4\n53    4\n54    4\n55    4\n56    4\n57    4\n58    4\nName: week_of_month, dtype: int32\n\n\n\n# Works on Pandas Series\ndates.to_series().week_of_month()\n\n2020-01-01    1\n2020-01-02    1\n2020-01-03    1\n2020-01-04    1\n2020-01-05    1\n2020-01-06    1\n2020-01-07    1\n2020-01-08    2\n2020-01-09    2\n2020-01-10    2\n2020-01-11    2\n2020-01-12    2\n2020-01-13    2\n2020-01-14    2\n2020-01-15    3\n2020-01-16    3\n2020-01-17    3\n2020-01-18    3\n2020-01-19    3\n2020-01-20    3\n2020-01-21    3\n2020-01-22    4\n2020-01-23    4\n2020-01-24    4\n2020-01-25    4\n2020-01-26    4\n2020-01-27    4\n2020-01-28    4\n2020-01-29    5\n2020-01-30    5\n2020-01-31    5\n2020-02-01    1\n2020-02-02    1\n2020-02-03    1\n2020-02-04    1\n2020-02-05    1\n2020-02-06    1\n2020-02-07    1\n2020-02-08    2\n2020-02-09    2\n2020-02-10    2\n2020-02-11    2\n2020-02-12    2\n2020-02-13    2\n2020-02-14    2\n2020-02-15    3\n2020-02-16    3\n2020-02-17    3\n2020-02-18    3\n2020-02-19    3\n2020-02-20    3\n2020-02-21    3\n2020-02-22    4\n2020-02-23    4\n2020-02-24    4\n2020-02-25    4\n2020-02-26    4\n2020-02-27    4\n2020-02-28    4\nFreq: D, Name: week_of_month, dtype: int32"
  },
  {
    "objectID": "tutorials/02_finance.html",
    "href": "tutorials/02_finance.html",
    "title": "Finance Analysis",
    "section": "",
    "text": "Timetk is designed to work with any time series domain. Arguably the most important is Finance. This tutorial showcases how you can perform Financial Investment and Stock Analysis at scale with pytimetk. This applied tutorial covers financial analysis with:\nLoad the following packages before proceeding with this tutorial.\nCode\nimport pytimetk as tk\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "tutorials/02_finance.html#application-moving-averages-10-day-and-50-day",
    "href": "tutorials/02_finance.html#application-moving-averages-10-day-and-50-day",
    "title": "Finance Analysis",
    "section": "3.1 Application: Moving Averages, 10-Day and 50-Day",
    "text": "3.1 Application: Moving Averages, 10-Day and 50-Day\nThis code template can be used to make and visualize the 10-day and 50-Day moving average of a group of stock symbols. Click to expand the code.\n\nPlotlyPlotnine\n\n\n\n\nCode\n# Add 2 moving averages (10-day and 50-Day)\nsma_df = stocks_df[['symbol', 'date', 'adjusted']] \\\n    .groupby('symbol') \\\n    .augment_rolling(\n        date_column = 'date',\n        value_column = 'adjusted',\n        window = [10, 50],\n        window_func = ['mean'],\n        center = False\n    )\n\n# Visualize \n(sma_df \n\n    # zoom in on dates\n    .query('date &gt;= \"2023-01-01\"') \n\n    # Convert to long format\n    .melt(\n        id_vars = ['symbol', 'date'],\n        value_vars = [\"adjusted\", \"adjusted_rolling_mean_win_10\", \"adjusted_rolling_mean_win_50\"]\n    ) \n\n    # Group on symbol and visualize\n    .groupby(\"symbol\") \n    .plot_timeseries(\n        date_column = 'date',\n        value_column = 'value',\n        color_column = 'variable',\n        smooth = False, \n        facet_ncol = 2,\n        width = 900,\n        height = 700,\n        engine = \"plotly\"\n    )\n)\n\n\n\n                                                \n\n\n\n\n\n\nCode\n# Add 2 moving averages (10-day and 50-Day)\nsma_df = stocks_df[['symbol', 'date', 'adjusted']] \\\n    .groupby('symbol') \\\n    .augment_rolling(\n        date_column = 'date',\n        value_column = 'adjusted',\n        window = [10, 50],\n        window_func = ['mean'],\n        center = False\n    )\n\n# Visualize \n(sma_df \n\n    # zoom in on dates\n    .query('date &gt;= \"2023-01-01\"') \n\n    # Convert to long format\n    .melt(\n        id_vars = ['symbol', 'date'],\n        value_vars = [\"adjusted\", \"adjusted_rolling_mean_win_10\", \"adjusted_rolling_mean_win_50\"]\n    ) \n\n    # Group on symbol and visualize\n    .groupby(\"symbol\") \n    .plot_timeseries(\n        date_column = 'date',\n        value_column = 'value',\n        color_column = 'variable',\n        smooth = False, \n        facet_ncol = 2,\n        width = 900,\n        height = 700,\n        engine = \"plotnine\"\n    )\n)\n\n\n\n\n\n&lt;Figure Size: (900 x 700)&gt;"
  },
  {
    "objectID": "tutorials/02_finance.html#application-bollinger-bands",
    "href": "tutorials/02_finance.html#application-bollinger-bands",
    "title": "Finance Analysis",
    "section": "3.2 Application: Bollinger Bands",
    "text": "3.2 Application: Bollinger Bands\nBollinger Bands are a volatility indicator commonly used in financial trading. They consist of three lines:\n\nThe middle band, which is a simple moving average (usually over 20 periods).\nThe upper band, calculated as the middle band plus k times the standard deviation of the price (typically, k=2).\nThe lower band, calculated as the middle band minus k times the standard deviation of the price.\n\nHere’s how you can calculate and plot Bollinger Bands with pytimetk using this code template (click to expand):\n\nPlotlyPlotnine\n\n\n\n\nCode\n# Bollinger Bands\nbollinger_df = stocks_df[['symbol', 'date', 'adjusted']] \\\n    .groupby('symbol') \\\n    .augment_rolling(\n        date_column = 'date',\n        value_column = 'adjusted',\n        window = 20,\n        window_func = ['mean', 'std'],\n        center = False\n    ) \\\n    .assign(\n        upper_band = lambda x: x['adjusted_rolling_mean_win_20'] + 2*x['adjusted_rolling_std_win_20'],\n        lower_band = lambda x: x['adjusted_rolling_mean_win_20'] - 2*x['adjusted_rolling_std_win_20']\n    )\n\n\n# Visualize\n(bollinger_df\n\n    # zoom in on dates\n    .query('date &gt;= \"2023-01-01\"') \n\n    # Convert to long format\n    .melt(\n        id_vars = ['symbol', 'date'],\n        value_vars = [\"adjusted\", \"adjusted_rolling_mean_win_20\", \"upper_band\", \"lower_band\"]\n    ) \n\n    # Group on symbol and visualize\n    .groupby(\"symbol\") \n    .plot_timeseries(\n        date_column = 'date',\n        value_column = 'value',\n        color_column = 'variable',\n        # Adjust colors for Bollinger Bands\n        color_palette =[\"#2C3E50\", \"#E31A1C\", '#18BC9C', '#18BC9C'],\n        smooth = False, \n        facet_ncol = 2,\n        width = 900,\n        height = 700,\n        engine = \"plotly\" \n    )\n)\n\n\n\n                                                \n\n\n\n\n\n\nCode\n# Bollinger Bands\nbollinger_df = stocks_df[['symbol', 'date', 'adjusted']] \\\n    .groupby('symbol') \\\n    .augment_rolling(\n        date_column = 'date',\n        value_column = 'adjusted',\n        window = 20,\n        window_func = ['mean', 'std'],\n        center = False\n    ) \\\n    .assign(\n        upper_band = lambda x: x['adjusted_rolling_mean_win_20'] + 2*x['adjusted_rolling_std_win_20'],\n        lower_band = lambda x: x['adjusted_rolling_mean_win_20'] - 2*x['adjusted_rolling_std_win_20']\n    )\n\n\n# Visualize\n(bollinger_df\n\n    # zoom in on dates\n    .query('date &gt;= \"2023-01-01\"') \n\n    # Convert to long format\n    .melt(\n        id_vars = ['symbol', 'date'],\n        value_vars = [\"adjusted\", \"adjusted_rolling_mean_win_20\", \"upper_band\", \"lower_band\"]\n    ) \n\n    # Group on symbol and visualize\n    .groupby(\"symbol\") \n    .plot_timeseries(\n        date_column = 'date',\n        value_column = 'value',\n        color_column = 'variable',\n        # Adjust colors for Bollinger Bands\n        color_palette =[\"#2C3E50\", \"#E31A1C\", '#18BC9C', '#18BC9C'],\n        smooth = False, \n        facet_ncol = 2,\n        width = 900,\n        height = 700,\n        engine = \"plotnine\"\n    )\n)\n\n\n\n\n\n&lt;Figure Size: (900 x 700)&gt;"
  },
  {
    "objectID": "tutorials/02_finance.html#returns-analysis-by-time",
    "href": "tutorials/02_finance.html#returns-analysis-by-time",
    "title": "Finance Analysis",
    "section": "4.1 Returns Analysis By Time",
    "text": "4.1 Returns Analysis By Time\n\n\n\n\n\n\nReturns are NOT static (so analyze them by time)\n\n\n\n\n\n\nWe can use rolling window calculations with tk.augment_rolling() to compute many rolling features at scale such as rolling mean, std, range (spread).\nWe can expand our tk.augment_rolling() rolling calculations to Rolling Correlation and Rolling Regression (to make comparisons over time)\n\n\n\n\n\nApplication: Descriptive Statistic Analysis\nMany traders compute descriptive statistics like mean, median, mode, skewness, kurtosis, and standard deviation to understand the central tendency, spread, and shape of the return distribution.\n\n\nStep 1: Returns\nUse this code to get the pct_change() in wide format. Click expand to get the code.\n\n\nCode\nreturns_wide_df = stocks_df[['symbol', 'date', 'adjusted']] \\\n    .pivot(index = 'date', columns = 'symbol', values = 'adjusted') \\\n    .pct_change() \\\n    .reset_index() \\\n    [1:]\n\nreturns_wide_df\n\n\n\n\n\n\n\n\nsymbol\ndate\nAAPL\nAMZN\nGOOG\nMETA\nNFLX\nNVDA\n\n\n\n\n1\n2013-01-03\n-0.012622\n0.004547\n0.000581\n-0.008214\n0.049777\n0.000786\n\n\n2\n2013-01-04\n-0.027854\n0.002592\n0.019760\n0.035650\n-0.006315\n0.032993\n\n\n3\n2013-01-07\n-0.005883\n0.035925\n-0.004363\n0.022949\n0.033549\n-0.028897\n\n\n4\n2013-01-08\n0.002691\n-0.007748\n-0.001974\n-0.012237\n-0.020565\n-0.021926\n\n\n5\n2013-01-09\n-0.015629\n-0.000113\n0.006573\n0.052650\n-0.012865\n-0.022418\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2694\n2023-09-15\n-0.004154\n-0.029920\n-0.004964\n-0.036603\n-0.008864\n-0.036879\n\n\n2695\n2023-09-18\n0.016913\n-0.002920\n0.004772\n0.007459\n-0.006399\n0.001503\n\n\n2696\n2023-09-19\n0.006181\n-0.016788\n-0.000936\n0.008329\n0.004564\n-0.010144\n\n\n2697\n2023-09-20\n-0.019992\n-0.017002\n-0.030541\n-0.017701\n-0.024987\n-0.029435\n\n\n2698\n2023-09-21\n-0.008889\n-0.044053\n-0.023999\n-0.013148\n-0.005566\n-0.028931\n\n\n\n\n2698 rows × 7 columns\n\n\n\n\n\nStep 2: Descriptive Stats\nUse this code to get standard statistics with the describe() method. Click expand to get the code.\n\n\nCode\nreturns_wide_df.describe()\n\n\n\n\n\n\n\n\nsymbol\ndate\nAAPL\nAMZN\nGOOG\nMETA\nNFLX\nNVDA\n\n\n\n\ncount\n2698\n2698.000000\n2698.000000\n2698.000000\n2698.000000\n2698.000000\n2698.000000\n\n\nmean\n2018-05-12 22:05:14.899926016\n0.001030\n0.001068\n0.000885\n0.001170\n0.001689\n0.002229\n\n\nmin\n2013-01-03 00:00:00\n-0.128647\n-0.140494\n-0.111008\n-0.263901\n-0.351166\n-0.187559\n\n\n25%\n2015-09-08 06:00:00\n-0.007410\n-0.008635\n-0.006900\n-0.009610\n-0.012071\n-0.010938\n\n\n50%\n2018-05-12 12:00:00\n0.000892\n0.001050\n0.000700\n0.001051\n0.000544\n0.001918\n\n\n75%\n2021-01-14 18:00:00\n0.010324\n0.011363\n0.009053\n0.012580\n0.014678\n0.015202\n\n\nmax\n2023-09-21 00:00:00\n0.119808\n0.141311\n0.160524\n0.296115\n0.422235\n0.298067\n\n\nstd\nNaN\n0.018036\n0.020621\n0.017267\n0.024291\n0.029683\n0.028320\n\n\n\n\n\n\n\n\n\nStep 3: Correlation\nAnd run a correlation with corr(). Click expand to get the code.\n\n\nCode\ncorr_table_df = returns_wide_df.drop('date', axis=1).corr()\ncorr_table_df\n\n\n\n\n\n\n\n\nsymbol\nAAPL\nAMZN\nGOOG\nMETA\nNFLX\nNVDA\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\nAAPL\n1.000000\n0.497906\n0.566452\n0.479787\n0.321694\n0.526508\n\n\nAMZN\n0.497906\n1.000000\n0.628103\n0.544481\n0.475078\n0.490234\n\n\nGOOG\n0.566452\n0.628103\n1.000000\n0.595728\n0.428470\n0.531382\n\n\nMETA\n0.479787\n0.544481\n0.595728\n1.000000\n0.407417\n0.450586\n\n\nNFLX\n0.321694\n0.475078\n0.428470\n0.407417\n1.000000\n0.380153\n\n\nNVDA\n0.526508\n0.490234\n0.531382\n0.450586\n0.380153\n1.000000\n\n\n\n\n\n\n\n\nThe problem is that the stock market is constantly changing. And these descriptive statistics aren’t representative of the most recent fluctuations. This is where pytimetk comes into play with rolling descriptive statistics.\n\n\n\nApplication: 90-Day Rolling Descriptive Statistics Analysis\nLet’s compute and visualize the 90-day rolling statistics.\n\n\n\n\n\n\nGetting More Info: tk.augment_rolling()\n\n\n\n\n\n\nClick here to see our Augmenting Guide\nUse help(tk.augment_rolling) to review additional helpful documentation.\n\n\n\n\n\nStep 1: Long Format Pt.1\nUse this code to get the date melt() into long format. Click expand to get the code.\n\n\nCode\nreturns_long_df = returns_wide_df \\\n    .melt(id_vars='date', value_name='returns') \n\nreturns_long_df\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nreturns\n\n\n\n\n0\n2013-01-03\nAAPL\n-0.012622\n\n\n1\n2013-01-04\nAAPL\n-0.027854\n\n\n2\n2013-01-07\nAAPL\n-0.005883\n\n\n3\n2013-01-08\nAAPL\n0.002691\n\n\n4\n2013-01-09\nAAPL\n-0.015629\n\n\n...\n...\n...\n...\n\n\n16183\n2023-09-15\nNVDA\n-0.036879\n\n\n16184\n2023-09-18\nNVDA\n0.001503\n\n\n16185\n2023-09-19\nNVDA\n-0.010144\n\n\n16186\n2023-09-20\nNVDA\n-0.029435\n\n\n16187\n2023-09-21\nNVDA\n-0.028931\n\n\n\n\n16188 rows × 3 columns\n\n\n\n\n\nStep 2: Augment Rolling Statistic\nLet’s add multiple columns of rolling statistics. Click to expand the code.\n\n\nCode\nrolling_stats_df = returns_long_df \\\n    .groupby('symbol') \\\n    .augment_rolling(\n        date_column = 'date',\n        value_column = 'returns',\n        window = [90],\n        window_func = [\n            'mean', \n            'std', \n            'min',\n            ('q25', lambda x: np.quantile(x, 0.25)),\n            'median',\n            ('q75', lambda x: np.quantile(x, 0.75)),\n            'max'\n        ]\n    ) \\\n    .dropna()\n\nrolling_stats_df\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nreturns\nreturns_rolling_mean_win_90\nreturns_rolling_std_win_90\nreturns_rolling_min_win_90\nreturns_rolling_q25_win_90\nreturns_rolling_median_win_90\nreturns_rolling_q75_win_90\nreturns_rolling_max_win_90\n\n\n\n\n89\n2013-05-13\nAAPL\n0.003908\n-0.001702\n0.022233\n-0.123558\n-0.010533\n-0.001776\n0.012187\n0.041509\n\n\n90\n2013-05-14\nAAPL\n-0.023926\n-0.001827\n0.022327\n-0.123558\n-0.010533\n-0.001776\n0.012187\n0.041509\n\n\n91\n2013-05-15\nAAPL\n-0.033817\n-0.001894\n0.022414\n-0.123558\n-0.010533\n-0.001776\n0.012187\n0.041509\n\n\n92\n2013-05-16\nAAPL\n0.013361\n-0.001680\n0.022467\n-0.123558\n-0.010533\n-0.001360\n0.013120\n0.041509\n\n\n93\n2013-05-17\nAAPL\n-0.003037\n-0.001743\n0.022462\n-0.123558\n-0.010533\n-0.001776\n0.013120\n0.041509\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16183\n2023-09-15\nNVDA\n-0.036879\n0.005159\n0.036070\n-0.056767\n-0.012587\n-0.000457\n0.018480\n0.243696\n\n\n16184\n2023-09-18\nNVDA\n0.001503\n0.005396\n0.035974\n-0.056767\n-0.011117\n0.000177\n0.018480\n0.243696\n\n\n16185\n2023-09-19\nNVDA\n-0.010144\n0.005162\n0.036006\n-0.056767\n-0.011117\n-0.000457\n0.018480\n0.243696\n\n\n16186\n2023-09-20\nNVDA\n-0.029435\n0.004953\n0.036153\n-0.056767\n-0.012587\n-0.000457\n0.018480\n0.243696\n\n\n16187\n2023-09-21\nNVDA\n-0.028931\n0.004724\n0.036303\n-0.056767\n-0.013166\n-0.000457\n0.018480\n0.243696\n\n\n\n\n15654 rows × 10 columns\n\n\n\n\n\nStep 3: Long Format Pt.2\nFinally, we can .melt() each of the rolling statistics for a Long Format Analysis. Click to expand the code.\n\n\nCode\nrolling_stats_long_df = rolling_stats_df \\\n    .melt(\n        id_vars = [\"symbol\", \"date\"],\n        var_name = \"statistic_type\"\n    )\n\nrolling_stats_long_df\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nstatistic_type\nvalue\n\n\n\n\n0\nAAPL\n2013-05-13\nreturns\n0.003908\n\n\n1\nAAPL\n2013-05-14\nreturns\n-0.023926\n\n\n2\nAAPL\n2013-05-15\nreturns\n-0.033817\n\n\n3\nAAPL\n2013-05-16\nreturns\n0.013361\n\n\n4\nAAPL\n2013-05-17\nreturns\n-0.003037\n\n\n...\n...\n...\n...\n...\n\n\n125227\nNVDA\n2023-09-15\nreturns_rolling_max_win_90\n0.243696\n\n\n125228\nNVDA\n2023-09-18\nreturns_rolling_max_win_90\n0.243696\n\n\n125229\nNVDA\n2023-09-19\nreturns_rolling_max_win_90\n0.243696\n\n\n125230\nNVDA\n2023-09-20\nreturns_rolling_max_win_90\n0.243696\n\n\n125231\nNVDA\n2023-09-21\nreturns_rolling_max_win_90\n0.243696\n\n\n\n\n125232 rows × 4 columns\n\n\n\nWith the data formatted properly we can evaluate the 90-Day Rolling Statistics using .plot_timeseries().\n\nPlotlyPlotnine\n\n\n\n\nCode\nrolling_stats_long_df \\\n    .groupby(['symbol', 'statistic_type']) \\\n    .plot_timeseries(\n        date_column = 'date',\n        value_column = 'value',\n        facet_ncol = 6,\n        width = 1500,\n        height = 1000,\n        title = \"90-Day Rolling Statistics\"\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nrolling_stats_long_df \\\n    .groupby(['symbol', 'statistic_type']) \\\n    .plot_timeseries(\n        date_column = 'date',\n        value_column = 'value',\n        facet_ncol = 6,\n        facet_dir = 'v',\n        width = 1500,\n        height = 1000,\n        title = \"90-Day Rolling Statistics\",\n        engine = \"plotnine\"\n    )\n\n\n\n\n\n&lt;Figure Size: (1500 x 1000)&gt;"
  },
  {
    "objectID": "tutorials/02_finance.html#about-rolling-correlation",
    "href": "tutorials/02_finance.html#about-rolling-correlation",
    "title": "Finance Analysis",
    "section": "5.1 About: Rolling Correlation",
    "text": "5.1 About: Rolling Correlation\nRolling correlation calculates the correlation between two time series over a rolling window of a specified size, moving one period at a time. In stock analysis, this is often used to assess:\n\nDiversification: Helps in identifying how different stocks move in relation to each other, aiding in the creation of a diversified portfolio.\nMarket Dependency: Measures how a particular stock or sector is correlated with a broader market index.\nRisk Management: Helps in identifying changes in correlation structures over time which is crucial for risk assessment and management.\n\nFor example, if the rolling correlation between two stocks starts increasing, it might suggest that they are being influenced by similar factors or market conditions."
  },
  {
    "objectID": "tutorials/02_finance.html#application-rolling-correlation",
    "href": "tutorials/02_finance.html#application-rolling-correlation",
    "title": "Finance Analysis",
    "section": "5.2 Application: Rolling Correlation",
    "text": "5.2 Application: Rolling Correlation\nLet’s revisit the returns wide and long format. We can combine these two using the merge() method.\n\nStep 1: Create the return_combinations_long_df\nPerform data wrangling to get the pairwise combinations in long format:\n\nWe first .merge() to join the long returns with the wide returns by date.\nWe then .melt() to get the wide data into long format.\n\n\n\nCode\nreturn_combinations_long_df = returns_long_df \\\n    .merge(returns_wide_df, how='left', on = 'date') \\\n    .melt(\n        id_vars = ['date', 'symbol', 'returns'],\n        var_name = \"comp\",\n        value_name = \"returns_comp\"\n    )\nreturn_combinations_long_df\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nreturns\ncomp\nreturns_comp\n\n\n\n\n0\n2013-01-03\nAAPL\n-0.012622\nAAPL\n-0.012622\n\n\n1\n2013-01-04\nAAPL\n-0.027854\nAAPL\n-0.027854\n\n\n2\n2013-01-07\nAAPL\n-0.005883\nAAPL\n-0.005883\n\n\n3\n2013-01-08\nAAPL\n0.002691\nAAPL\n0.002691\n\n\n4\n2013-01-09\nAAPL\n-0.015629\nAAPL\n-0.015629\n\n\n...\n...\n...\n...\n...\n...\n\n\n97123\n2023-09-15\nNVDA\n-0.036879\nNVDA\n-0.036879\n\n\n97124\n2023-09-18\nNVDA\n0.001503\nNVDA\n0.001503\n\n\n97125\n2023-09-19\nNVDA\n-0.010144\nNVDA\n-0.010144\n\n\n97126\n2023-09-20\nNVDA\n-0.029435\nNVDA\n-0.029435\n\n\n97127\n2023-09-21\nNVDA\n-0.028931\nNVDA\n-0.028931\n\n\n\n\n97128 rows × 5 columns\n\n\n\n\n\nStep 2: Add Rolling Correlations with tk.augment_rolling()\nNext, let’s add rolling correlations.\n\nWe first .groupby() on the combination of our target assets “symbol” and our comparison asset “comp”.\nThen we use the familiar tk.augment_rolling().\n\n\n\n\n\n\n\nUse Independent Variables\n\n\n\n\n\nWhen performing rolling calculations that require independent variables such as rolling correlations and rolling regressions, set use_independent_variables = True. This instructs the rolling operation to give you access to all columns in the data frame rather than just the value column as a series. This allows you to do more complex calculations that depend on more than just a target value_column.\n\n\n\n\n\nCode\nreturn_corr_df = return_combinations_long_df \\\n    .groupby([\"symbol\", \"comp\"]) \\\n    .augment_rolling(\n        date_column = \"date\",\n        value_column = \"returns\",\n        # IMPORTANT: INCLUDE INDEPENDENT VARIABLES\n        use_independent_variables = True,\n        window = 90,\n        window_func = [('corr', lambda df: df['returns'].corr(df['returns_comp']))]\n    )\n\nreturn_corr_df\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nreturns\ncomp\nreturns_comp\nreturns_rolling_corr_win_90\n\n\n\n\n0\n2013-01-03\nAAPL\n-0.012622\nAAPL\n-0.012622\nNaN\n\n\n1\n2013-01-04\nAAPL\n-0.027854\nAAPL\n-0.027854\nNaN\n\n\n2\n2013-01-07\nAAPL\n-0.005883\nAAPL\n-0.005883\nNaN\n\n\n3\n2013-01-08\nAAPL\n0.002691\nAAPL\n0.002691\nNaN\n\n\n4\n2013-01-09\nAAPL\n-0.015629\nAAPL\n-0.015629\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n97123\n2023-09-15\nNVDA\n-0.036879\nNVDA\n-0.036879\n1.0\n\n\n97124\n2023-09-18\nNVDA\n0.001503\nNVDA\n0.001503\n1.0\n\n\n97125\n2023-09-19\nNVDA\n-0.010144\nNVDA\n-0.010144\n1.0\n\n\n97126\n2023-09-20\nNVDA\n-0.029435\nNVDA\n-0.029435\n1.0\n\n\n97127\n2023-09-21\nNVDA\n-0.028931\nNVDA\n-0.028931\n1.0\n\n\n\n\n97128 rows × 6 columns\n\n\n\n\n\nStep 3: Visualize the Rolling Correlation\nWe can use tk.plot_timeseries() to visualize the 90-day rolling correlation. It’s interesting to see that stock combinations such as AAPL | AMZN returns have a high positive correlation of 0.80, but this relationship actually was much lower 0.25 prior to 2015.\n\nThe blue smoother can help us detect trends\nThe y_intercept is useful in this case to draw lines at -1, 0, and 1\n\n\nPlotlyPlotnine\n\n\n\n\nCode\nreturn_corr_df \\\n    .dropna() \\\n    .groupby(['symbol', 'comp']) \\\n    .plot_timeseries(\n        date_column = \"date\",\n        value_column = \"returns_rolling_corr_win_90\",\n        facet_ncol = 6,\n        y_intercept = [-1,0,1],\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1500,\n        height = 1000,\n        title = \"90-Day Rolling Correlation\",\n        engine = \"plotly\"\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nreturn_corr_df \\\n    .dropna() \\\n    .groupby(['symbol', 'comp']) \\\n    .plot_timeseries(\n        date_column = \"date\",\n        value_column = \"returns_rolling_corr_win_90\",\n        facet_ncol = 6,\n        y_intercept = [-1,0,1],\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1500,\n        height = 1000,\n        title = \"90-Day Rolling Correlation\",\n        engine = \"plotnine\"\n    )\n\n\n\n\n\n&lt;Figure Size: (1500 x 1000)&gt;\n\n\n\n\n\nFor comparison, we can examine the corr_table_df from the Descriptive Statistics Analysis:\n\n\nCode\ncorr_table_df\n\n\n\n\n\n\n\n\nsymbol\nAAPL\nAMZN\nGOOG\nMETA\nNFLX\nNVDA\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\nAAPL\n1.000000\n0.497906\n0.566452\n0.479787\n0.321694\n0.526508\n\n\nAMZN\n0.497906\n1.000000\n0.628103\n0.544481\n0.475078\n0.490234\n\n\nGOOG\n0.566452\n0.628103\n1.000000\n0.595728\n0.428470\n0.531382\n\n\nMETA\n0.479787\n0.544481\n0.595728\n1.000000\n0.407417\n0.450586\n\n\nNFLX\n0.321694\n0.475078\n0.428470\n0.407417\n1.000000\n0.380153\n\n\nNVDA\n0.526508\n0.490234\n0.531382\n0.450586\n0.380153\n1.000000"
  },
  {
    "objectID": "tutorials/02_finance.html#about-rolling-regression",
    "href": "tutorials/02_finance.html#about-rolling-regression",
    "title": "Finance Analysis",
    "section": "5.3 About: Rolling Regression",
    "text": "5.3 About: Rolling Regression\nRolling regression involves running regression analyses over rolling windows of data points to assess the relationship between a dependent and one or more independent variables. In the context of stock analysis, it can be used to:\n\nBeta Estimation: It can be used to estimate the beta of a stock (a measure of market risk) against a market index over different time periods. A higher beta indicates higher market-related risk.\nMarket Timing: It can be useful in identifying changing relationships between stocks and market indicators, helping traders to adjust their positions accordingly.\nHedge Ratio Determination: It helps in determining the appropriate hedge ratios for pairs trading or other hedging strategies."
  },
  {
    "objectID": "tutorials/02_finance.html#application-90-day-rolling-regression",
    "href": "tutorials/02_finance.html#application-90-day-rolling-regression",
    "title": "Finance Analysis",
    "section": "5.4 Application: 90-Day Rolling Regression",
    "text": "5.4 Application: 90-Day Rolling Regression\n\n\n\n\n\n\nThis Application Requires Scikit Learn\n\n\n\n\n\nWe need to make a regression function that returns the Slope and Intercept. Scikit Learn has an easy to use modeling interface. You may need to pip install scikit-learn to use this applied tutorial.\n\n\n\n\nStep 1: Get Market Returns\nFor our purposes, we assume the market is the average returns of the 6 technology stocks.\n\nWe calculate an equal-weight portfolio as the “market returns”.\nThen we merge the market returns into the returns long data.\n\n\n\nCode\n# Assume Market Returns = Equal Weight Portfolio\nmarket_returns_df = returns_wide_df \\\n    .set_index(\"date\") \\\n    .assign(returns_market = lambda df: df.sum(axis = 1) * (1 / df.shape[1])) \\\n    .reset_index() \\\n    [['date', 'returns_market']]\n\n# Merge with returns long\nreturns_long_market_df = returns_long_df \\\n    .merge(market_returns_df, how='left', on='date')\n\nreturns_long_market_df\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nreturns\nreturns_market\n\n\n\n\n0\n2013-01-03\nAAPL\n-0.012622\n0.005809\n\n\n1\n2013-01-04\nAAPL\n-0.027854\n0.009471\n\n\n2\n2013-01-07\nAAPL\n-0.005883\n0.008880\n\n\n3\n2013-01-08\nAAPL\n0.002691\n-0.010293\n\n\n4\n2013-01-09\nAAPL\n-0.015629\n0.001366\n\n\n...\n...\n...\n...\n...\n\n\n16183\n2023-09-15\nNVDA\n-0.036879\n-0.020231\n\n\n16184\n2023-09-18\nNVDA\n0.001503\n0.003555\n\n\n16185\n2023-09-19\nNVDA\n-0.010144\n-0.001466\n\n\n16186\n2023-09-20\nNVDA\n-0.029435\n-0.023276\n\n\n16187\n2023-09-21\nNVDA\n-0.028931\n-0.020764\n\n\n\n\n16188 rows × 4 columns\n\n\n\n\n\nStep 2: Run a Rolling Regression\nNext, run the following code to perform a rolling regression:\n\nUse a custom regression function that will return the slope and intercept as a pandas series.\nRun the rolling regression with tk.augment_rolling().\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\ndef regression(df):\n    \n    model = LinearRegression()\n    X = df[['returns_market']]  # Extract X values (independent variables)\n    y = df['returns']  # Extract y values (dependent variable)\n    model.fit(X, y)\n    ret = pd.Series([model.intercept_, model.coef_[0]], index=['Intercept', 'Slope'])\n    \n    return ret # Return intercept and slope as a Series\n\nreturn_regression_df = returns_long_market_df \\\n    .groupby('symbol') \\\n    .augment_rolling(\n        date_column = \"date\",\n        value_column = \"returns\",\n        use_independent_variables = True,\n        window = 90,\n        window_func = [('regression', regression)]\n        \n    ) \\\n    .dropna()\n\nreturn_regression_df\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nreturns\nreturns_market\nreturns_rolling_regression_win_90\n\n\n\n\n89\n2013-05-13\nAAPL\n0.003908\n0.007082\nIntercept -0.001844 Slope 0.061629 dt...\n\n\n90\n2013-05-14\nAAPL\n-0.023926\n0.007583\nIntercept -0.001959 Slope 0.056540 dt...\n\n\n91\n2013-05-15\nAAPL\n-0.033817\n0.005381\nIntercept -0.002036 Slope 0.062330 dt...\n\n\n92\n2013-05-16\nAAPL\n0.013361\n-0.009586\nIntercept -0.001789 Slope 0.052348 dt...\n\n\n93\n2013-05-17\nAAPL\n-0.003037\n0.009005\nIntercept -0.001871 Slope 0.055661 dt...\n\n\n...\n...\n...\n...\n...\n...\n\n\n16183\n2023-09-15\nNVDA\n-0.036879\n-0.020231\nIntercept 0.000100 Slope 1.805479 dt...\n\n\n16184\n2023-09-18\nNVDA\n0.001503\n0.003555\nIntercept 0.000207 Slope 1.800813 dt...\n\n\n16185\n2023-09-19\nNVDA\n-0.010144\n-0.001466\nIntercept 0.000301 Slope 1.817878 dt...\n\n\n16186\n2023-09-20\nNVDA\n-0.029435\n-0.023276\nIntercept 0.000845 Slope 1.825818 dt...\n\n\n16187\n2023-09-21\nNVDA\n-0.028931\n-0.020764\nIntercept 0.000901 Slope 1.818710 dt...\n\n\n\n\n15654 rows × 5 columns\n\n\n\n\n\nStep 3: Extract the Slope Coefficient (Beta)\nThis is more of a hack than anything to extract the beta (slope) of the rolling regression.\n\n\nCode\nintercept_slope_df = pd.concat(return_regression_df['returns_rolling_regression_win_90'].to_list(), axis=1).T \n\nintercept_slope_df.index = return_regression_df.index\n\nreturn_beta_df = pd.concat([return_regression_df, intercept_slope_df], axis=1)\n\nreturn_beta_df\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nreturns\nreturns_market\nreturns_rolling_regression_win_90\nIntercept\nSlope\n\n\n\n\n89\n2013-05-13\nAAPL\n0.003908\n0.007082\nIntercept -0.001844 Slope 0.061629 dt...\n-0.001844\n0.061629\n\n\n90\n2013-05-14\nAAPL\n-0.023926\n0.007583\nIntercept -0.001959 Slope 0.056540 dt...\n-0.001959\n0.056540\n\n\n91\n2013-05-15\nAAPL\n-0.033817\n0.005381\nIntercept -0.002036 Slope 0.062330 dt...\n-0.002036\n0.062330\n\n\n92\n2013-05-16\nAAPL\n0.013361\n-0.009586\nIntercept -0.001789 Slope 0.052348 dt...\n-0.001789\n0.052348\n\n\n93\n2013-05-17\nAAPL\n-0.003037\n0.009005\nIntercept -0.001871 Slope 0.055661 dt...\n-0.001871\n0.055661\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16183\n2023-09-15\nNVDA\n-0.036879\n-0.020231\nIntercept 0.000100 Slope 1.805479 dt...\n0.000100\n1.805479\n\n\n16184\n2023-09-18\nNVDA\n0.001503\n0.003555\nIntercept 0.000207 Slope 1.800813 dt...\n0.000207\n1.800813\n\n\n16185\n2023-09-19\nNVDA\n-0.010144\n-0.001466\nIntercept 0.000301 Slope 1.817878 dt...\n0.000301\n1.817878\n\n\n16186\n2023-09-20\nNVDA\n-0.029435\n-0.023276\nIntercept 0.000845 Slope 1.825818 dt...\n0.000845\n1.825818\n\n\n16187\n2023-09-21\nNVDA\n-0.028931\n-0.020764\nIntercept 0.000901 Slope 1.818710 dt...\n0.000901\n1.818710\n\n\n\n\n15654 rows × 7 columns\n\n\n\n\n\nStep 4: Visualize the Rolling Beta\n\nPlotlyPlotnine\n\n\n\n\nCode\nreturn_beta_df \\\n    .groupby('symbol') \\\n    .plot_timeseries(\n        date_column = \"date\",\n        value_column = \"Slope\",\n        facet_ncol = 2,\n        facet_scales = \"free_x\",\n        y_intercept = [0, 3],\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 800,\n        height = 600,\n        title = \"90-Day Rolling Regression\",\n        engine = \"plotly\",\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nreturn_beta_df \\\n    .groupby('symbol') \\\n    .plot_timeseries(\n        date_column = \"date\",\n        value_column = \"Slope\",\n        facet_ncol = 2,\n        facet_scales = \"free_x\",\n        y_intercept = [0, 3],\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 800,\n        height = 600,\n        title = \"90-Day Rolling Regression\",\n        engine = \"plotnine\",\n    )\n\n\n\n\n\n&lt;Figure Size: (800 x 600)&gt;"
  },
  {
    "objectID": "tutorials/04_anomaly_detection.html",
    "href": "tutorials/04_anomaly_detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "tutorials/01_sales_crm.html#preliminary-data-exploration-with-tk.summarize_by_time",
    "href": "tutorials/01_sales_crm.html#preliminary-data-exploration-with-tk.summarize_by_time",
    "title": "Sales CRM Database Analysis",
    "section": "2.1 Preliminary Data Exploration with tk.summarize_by_time",
    "text": "2.1 Preliminary Data Exploration with tk.summarize_by_time\nCRM data is often bustling with activity, reflecting the myriad of transactions happening daily. Due to this high volume, the data can sometimes seem overwhelming or noisy. To derive meaningful insights, it’s essential to aggregate this data over specific time intervals. This is where tk.summarize_by_time() comes into play.\nThe tk.summarize_by_time() function offers a streamlined approach to time-based data aggregation. By defining a desired frequency and an aggregation method, this function seamlessly organizes your data. The beauty of it is its versatility; from a broad array of built-in aggregation methods and frequencies to the flexibility of integrating a custom function, it caters to a range of requirements.\n\n\n\n\n\n\nGetting to know tk.summarize_by_time()\n\n\n\n\n\nCurious about the various options it provides?\n\nClick here to see our Data Wrangling Guide\nUse help(tk.summarize_by_time) to review additional helpful documentation. And explore the plethora of possibilities!\n\n\n\n\n\nWeekly Totals\n\n\nCode\nweekly_totals = df.summarize_by_time(\n    date_column  = 'order_date',\n    value_column = 'total_price',\n    agg_func     = ['sum'],\n    freq         = 'W'\n)\n\nweekly_totals.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\n\n\n\n\n0\n2011-01-09\n12040\n\n\n1\n2011-01-16\n151460\n\n\n2\n2011-01-23\n143850\n\n\n3\n2011-01-30\n175665\n\n\n4\n2011-02-06\n105210\n\n\n5\n2011-02-13\n250390\n\n\n6\n2011-02-20\n410595\n\n\n7\n2011-02-27\n254045\n\n\n8\n2011-03-06\n308420\n\n\n9\n2011-03-13\n45450\n\n\n\n\n\n\n\n\n\nWeekly Totals by Group (Category 2)\nTo better understand your data, you might want to add groups to this summary. We can include a groupby before the summarize_by_time and then aggregate our data.\n\n\nCode\n sales_by_week = df \\\n    .groupby('category_2') \\\n    .summarize_by_time(\n        date_column = 'order_date',\n        value_column = 'total_price',\n        agg_func = ['sum'],\n        freq = 'W'\n    )\n\nsales_by_week.head(10)\n\n\n\n\n\n\n\n\n\ncategory_2\norder_date\ntotal_price_sum\n\n\n\n\n0\nCross Country Race\n2011-01-16\n61750\n\n\n1\nCross Country Race\n2011-01-23\n25050\n\n\n2\nCross Country Race\n2011-01-30\n56860\n\n\n3\nCross Country Race\n2011-02-06\n8740\n\n\n4\nCross Country Race\n2011-02-13\n78070\n\n\n5\nCross Country Race\n2011-02-20\n115010\n\n\n6\nCross Country Race\n2011-02-27\n64290\n\n\n7\nCross Country Race\n2011-03-06\n95070\n\n\n8\nCross Country Race\n2011-03-13\n3200\n\n\n9\nCross Country Race\n2011-03-20\n21170\n\n\n\n\n\n\n\nThis long format can make it a little hard to compare the different group values visually, so instead of long-format you might want to pivot wide to view the data.\n\n\nCode\nsales_by_week_wide = df \\\n    .groupby('category_2') \\\n    .summarize_by_time(\n        date_column = 'order_date',\n        value_column = 'total_price',\n        agg_func = ['sum'],\n        freq = 'W',\n        wide_format = True\n    )\n\nsales_by_week_wide.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum_Cross Country Race\ntotal_price_sum_Cyclocross\ntotal_price_sum_Elite Road\ntotal_price_sum_Endurance Road\ntotal_price_sum_Fat Bike\ntotal_price_sum_Over Mountain\ntotal_price_sum_Sport\ntotal_price_sum_Trail\ntotal_price_sum_Triathalon\n\n\n\n\n0\n2011-01-09\n0.0\n0.0\n0.0\n0.0\n0.0\n12040.0\n0.0\n0.0\n0.0\n\n\n1\n2011-01-16\n61750.0\n1960.0\n49540.0\n11110.0\n0.0\n9170.0\n4030.0\n7450.0\n6450.0\n\n\n2\n2011-01-23\n25050.0\n3500.0\n51330.0\n47930.0\n0.0\n3840.0\n0.0\n0.0\n12200.0\n\n\n3\n2011-01-30\n56860.0\n2450.0\n43895.0\n24160.0\n0.0\n10880.0\n3720.0\n26700.0\n7000.0\n\n\n4\n2011-02-06\n8740.0\n7000.0\n35640.0\n22680.0\n3730.0\n14270.0\n980.0\n10220.0\n1950.0\n\n\n5\n2011-02-13\n78070.0\n0.0\n83780.0\n24820.0\n2130.0\n17160.0\n6810.0\n17120.0\n20500.0\n\n\n6\n2011-02-20\n115010.0\n7910.0\n79770.0\n27650.0\n26100.0\n37830.0\n10925.0\n96250.0\n9150.0\n\n\n7\n2011-02-27\n64290.0\n6650.0\n86900.0\n31900.0\n5860.0\n22070.0\n6165.0\n16410.0\n13800.0\n\n\n8\n2011-03-06\n95070.0\n2450.0\n31990.0\n47660.0\n5860.0\n82060.0\n9340.0\n26790.0\n7200.0\n\n\n9\n2011-03-13\n3200.0\n4200.0\n23110.0\n7260.0\n0.0\n5970.0\n1710.0\n0.0\n0.0\n\n\n\n\n\n\n\nYou can now observe the total sales for each product side by side. This streamlined view facilitates easy comparison between product sales."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#plot-your-data-with-tk.plot_timeseries",
    "href": "tutorials/01_sales_crm.html#plot-your-data-with-tk.plot_timeseries",
    "title": "Sales CRM Database Analysis",
    "section": "2.2 Plot your data with tk.plot_timeseries",
    "text": "2.2 Plot your data with tk.plot_timeseries\nYou can now visualize the summarized data to gain a clearer insight into the prevailing trends.\n\nPlotlyPlotnine\n\n\n\n\nCode\nsales_by_week \\\n    .groupby('category_2') \\\n    .plot_timeseries(\n        date_column = 'order_date',  \n        value_column = 'total_price_sum',\n        title = 'Bike Sales by Category',\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        y_lab = 'Total Sales', \n        engine = 'plotly'\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nsales_by_week \\\n    .groupby('category_2') \\\n    .plot_timeseries(\n        date_column = 'order_date',  \n        value_column = 'total_price_sum',\n        title = 'Bike Sales by Category',\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        line_size = 0.35,\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        y_lab = 'Total Sales', \n        engine = 'plotnine'\n    )\n\n\n\n\n\n&lt;Figure Size: (1000 x 800)&gt;\n\n\n\n\n\nThe graph showcases a pronounced uptick in sales for most of the different bike products during the summer. It’s a natural trend, aligning with our understanding that people gravitate towards biking during the balmy summer days. Conversely, as the chill of winter sets in at the year’s start and end, we observe a corresponding dip in sales.\nIt’s worth highlighting the elegance of the plot_timeseries function. Beyond just plotting raw data, it introduces a smoother, accentuating underlying trends and making them more discernible. This enhancement ensures we can effortlessly capture and comprehend the cyclical nature of bike sales throughout the year."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#visualize-your-time-series-data-with-tk.plot_timeseries",
    "href": "tutorials/01_sales_crm.html#visualize-your-time-series-data-with-tk.plot_timeseries",
    "title": "Sales Customer Analysis",
    "section": "2.3 Visualize your time series data with tk.plot_timeseries",
    "text": "2.3 Visualize your time series data with tk.plot_timeseries\nYou can now visualize the summarized data to gain a clearer insight into the prevailing trends.\n\nPlotlyPlotnine\n\n\n\n\nCode\nsales_by_week \\\n    .groupby('category_2') \\\n    .plot_timeseries(\n        date_column = 'order_date',  \n        value_column = 'total_price_sum',\n        title = 'Bike Sales by Category',\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        y_lab = 'Total Sales', \n        engine = 'plotly'\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nsales_by_week \\\n    .groupby('category_2') \\\n    .plot_timeseries(\n        date_column = 'order_date',  \n        value_column = 'total_price_sum',\n        title = 'Bike Sales by Category',\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        y_lab = 'Total Sales', \n        engine = 'plotnine'\n    )\n\n\n\n\n\n&lt;Figure Size: (1000 x 800)&gt;\n\n\n\n\n\nThe graph showcases a pronounced uptick in sales for most of the different bike products during the summer. It’s a natural trend, aligning with our understanding that people gravitate towards biking during the balmy summer days. Conversely, as the chill of winter sets in at the year’s start and end, we observe a corresponding dip in sales.\nIt’s worth highlighting the elegance of the plot_timeseries function. Beyond just plotting raw data, it introduces a smoother, accentuating underlying trends and making them more discernible. This enhancement ensures we can effortlessly capture and comprehend the cyclical nature of bike sales throughout the year."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#making-irregular-data-regular-with-tk.pad_by_time",
    "href": "tutorials/01_sales_crm.html#making-irregular-data-regular-with-tk.pad_by_time",
    "title": "Sales Customer Analysis",
    "section": "3.1 Making irregular data regular with tk.pad_by_time",
    "text": "3.1 Making irregular data regular with tk.pad_by_time\nKicking off our journey, we’ll utilize pytimetk’s tk.pad_by_time() function. For this, grouping by the ‘category_1’ variable is recommended. Moreover, it’s prudent to establish a definitive end date. This ensures that all groups are equipped with training data up to the most recent date, accommodating scenarios where certain categories might have seen no sales in the final training week. By doing so, we create a representative observation for every group, capturing the nuances of each category’s sales pattern.\n\n\nCode\nsales_padded = sales_by_week \\\n    .groupby('category_2') \\\n    .pad_by_time(\n        date_column = 'order_date',\n        freq        = 'W',\n        end_date    = sales_by_week.order_date.max()\n    )\nsales_padded\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\n\n\n...\n...\n...\n...\n\n\n452\n2011-12-04\nTriathalon\n3200.0\n\n\n453\n2011-12-11\nTriathalon\n28350.0\n\n\n454\n2011-12-18\nTriathalon\n2700.0\n\n\n455\n2011-12-25\nTriathalon\n3900.0\n\n\n456\n2012-01-01\nTriathalon\nNaN\n\n\n\n\n457 rows × 3 columns"
  },
  {
    "objectID": "tutorials/01_sales_crm.html#future-frame",
    "href": "tutorials/01_sales_crm.html#future-frame",
    "title": "Sales Customer Analysis",
    "section": "3.2 Future Frame",
    "text": "3.2 Future Frame\nMoving on, let’s set up the future frame, which will serve as our test dataset. To achieve this, employ the tk.future_frame() method. This function allows for the specification of a grouping column and a forecast horizon.\nUpon invoking tk.future_frame(), you’ll observe that placeholders (null values) are added for each group, extending 12 weeks into the future.\n\n\nCode\ndf_with_futureframe = sales_padded \\\n    .groupby('category_2') \\\n    .future_frame(\n        date_column = 'order_date',\n        length_out  = 12\n    )\ndf_with_futureframe\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\n\n\n...\n...\n...\n...\n\n\n58\n2012-02-26\nTriathalon\nNaN\n\n\n59\n2012-03-04\nTriathalon\nNaN\n\n\n60\n2012-03-11\nTriathalon\nNaN\n\n\n61\n2012-03-18\nTriathalon\nNaN\n\n\n62\n2012-03-25\nTriathalon\nNaN\n\n\n\n\n565 rows × 3 columns\n\n\n\n\n5.3 Lag Values\nCrafting features from time series data can be intricate, but thanks to the suite of feature engineering tools in pytimetk, the process is streamlined and intuitive.\nIn this guide, we’ll focus on the basics: introducing a few lag variables and incorporating some date-related features.\nFirstly, let’s dive into creating lag features.\nGiven our forecasting objective of a 12-week horizon, to ensure we have lag data available for every future point, we should utilize a lag of 12 or more. The beauty of the toolkit is that it supports the addition of multiple lags simultaneously.\nLag features play a pivotal role in machine learning for time series. Often, recent data offers valuable insights into future trends. To capture this recency effect, it’s crucial to integrate lag values. For this purpose, tk.augment_lags() comes in handy.\n\n\nCode\ndf_with_lags = df_with_futureframe \\\n    .groupby('category_2') \\\n    .augment_lags(\n        date_column  = 'order_date',\n        value_column = 'total_price_sum',\n        lags         = [12,24]\n\n    )\ndf_with_lags.head(25)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\nNaN\nNaN\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\nNaN\nNaN\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\nNaN\nNaN\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\nNaN\nNaN\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\nNaN\nNaN\n\n\n5\n2011-02-20\nCross Country Race\n115010.0\nNaN\nNaN\n\n\n6\n2011-02-27\nCross Country Race\n64290.0\nNaN\nNaN\n\n\n7\n2011-03-06\nCross Country Race\n95070.0\nNaN\nNaN\n\n\n8\n2011-03-13\nCross Country Race\n3200.0\nNaN\nNaN\n\n\n9\n2011-03-20\nCross Country Race\n21170.0\nNaN\nNaN\n\n\n10\n2011-03-27\nCross Country Race\n28990.0\nNaN\nNaN\n\n\n11\n2011-04-03\nCross Country Race\n51860.0\nNaN\nNaN\n\n\n12\n2011-04-10\nCross Country Race\n85910.0\n61750.0\nNaN\n\n\n13\n2011-04-17\nCross Country Race\n138230.0\n25050.0\nNaN\n\n\n14\n2011-04-24\nCross Country Race\n138350.0\n56860.0\nNaN\n\n\n15\n2011-05-01\nCross Country Race\n136090.0\n8740.0\nNaN\n\n\n16\n2011-05-08\nCross Country Race\n32110.0\n78070.0\nNaN\n\n\n17\n2011-05-15\nCross Country Race\n139010.0\n115010.0\nNaN\n\n\n18\n2011-05-22\nCross Country Race\n2060.0\n64290.0\nNaN\n\n\n19\n2011-05-29\nCross Country Race\n26130.0\n95070.0\nNaN\n\n\n20\n2011-06-05\nCross Country Race\n30360.0\n3200.0\nNaN\n\n\n21\n2011-06-12\nCross Country Race\n88280.0\n21170.0\nNaN\n\n\n22\n2011-06-19\nCross Country Race\n109470.0\n28990.0\nNaN\n\n\n23\n2011-06-26\nCross Country Race\n107280.0\n51860.0\nNaN\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n\n\n\n\n\n\n\nObserve that lag values of 12 and 24 introduce missing entries at the dataset’s outset. This occurs because there isn’t available data from 12 or 24 weeks prior. To address these gaps, you can adopt one of two strategies:\n\nDiscard the Affected Rows: This is a recommended approach if your dataset is sufficiently large. Removing a few initial rows might not significantly impact the training process.\nBackfill Missing Values: In situations with limited data, you might consider backfilling these nulls using the first available values from lag 12 and 24. However, the appropriateness of this technique hinges on your specific context and objectives.\n\nFor the scope of this tutorial, we’ll opt to remove these rows. However, it’s worth pointing out that our dataset is actually quite small with limited historical data, so this might impact our model.\n\n\nCode\nlag_columns = [col for col in df_with_lags.columns if 'lag' in col]\ndf_no_nas = df_with_lags \\\n    .dropna(subset=lag_columns, inplace=False)\n\ndf_no_nas.head()\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n\n\n\n\n\n\n\n\n\n5.4 Date Features\nNow, let’s enrich our dataset with date-related features.\nWith the function tk.augment_timeseries_signature(), you can effortlessly append 29 date attributes to a timestamp. Given that our dataset captures weekly intervals, certain attributes like ‘hour’ may not be pertinent. Thus, it’s prudent to refine our columns, retaining only those that truly matter to our analysis.\n\n\nCode\ndf_with_datefeatures = df_no_nas \\\n    .augment_timeseries_signature(date_column='order_date')\n\ndf_with_datefeatures.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\n...\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n1309651200\n2011\n2011\n0\n0\n...\n3\n3\n184\n1\n0\n0\n0\n0\n0\nam\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n1310256000\n2011\n2011\n0\n0\n...\n10\n10\n191\n1\n0\n0\n0\n0\n0\nam\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n1310860800\n2011\n2011\n0\n0\n...\n17\n17\n198\n1\n0\n0\n0\n0\n0\nam\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n1311465600\n2011\n2011\n0\n0\n...\n24\n24\n205\n1\n0\n0\n0\n0\n0\nam\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n1312070400\n2011\n2011\n0\n0\n...\n31\n31\n212\n1\n0\n0\n0\n0\n0\nam\n\n\n29\n2011-08-07\nCross Country Race\n11620.0\n139010.0\n115010.0\n1312675200\n2011\n2011\n0\n0\n...\n7\n38\n219\n1\n0\n0\n0\n0\n0\nam\n\n\n30\n2011-08-14\nCross Country Race\n9730.0\n2060.0\n64290.0\n1313280000\n2011\n2011\n0\n0\n...\n14\n45\n226\n1\n0\n0\n0\n0\n0\nam\n\n\n31\n2011-08-21\nCross Country Race\n22780.0\n26130.0\n95070.0\n1313884800\n2011\n2011\n0\n0\n...\n21\n52\n233\n1\n0\n0\n0\n0\n0\nam\n\n\n32\n2011-08-28\nCross Country Race\n53680.0\n30360.0\n3200.0\n1314489600\n2011\n2011\n0\n0\n...\n28\n59\n240\n1\n0\n0\n0\n0\n0\nam\n\n\n33\n2011-09-04\nCross Country Race\n38360.0\n88280.0\n21170.0\n1315094400\n2011\n2011\n0\n0\n...\n4\n66\n247\n1\n0\n0\n0\n0\n0\nam\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\nCode\ndf_with_datefeatures.columns\n\n\nIndex(['order_date', 'category_2', 'total_price_sum', 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_index_num', 'order_date_year',\n       'order_date_year_iso', 'order_date_yearstart', 'order_date_yearend',\n       'order_date_leapyear', 'order_date_half', 'order_date_quarter',\n       'order_date_quarteryear', 'order_date_quarterstart',\n       'order_date_quarterend', 'order_date_month', 'order_date_month_lbl',\n       'order_date_monthstart', 'order_date_monthend', 'order_date_yweek',\n       'order_date_mweek', 'order_date_wday', 'order_date_wday_lbl',\n       'order_date_mday', 'order_date_qday', 'order_date_yday',\n       'order_date_weekend', 'order_date_hour', 'order_date_minute',\n       'order_date_second', 'order_date_msecond', 'order_date_nsecond',\n       'order_date_am_pm'],\n      dtype='object')\n\n\nLet’s subset to just a few of the relevant date features.\n\n\nCode\ndf_with_datefeatures_narrom = df_with_datefeatures[[\n    'order_date', \n    'category_2', \n    'total_price_sum',\n    'total_price_sum_lag_12',\n    'total_price_sum_lag_24',\n    'order_date_year',  \n    'order_date_half', \n    'order_date_quarter',      \n    'order_date_month',\n    'order_date_yweek'\n]]\n\ndf_with_datefeatures_narrom.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\n\n\n29\n2011-08-07\nCross Country Race\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\n\n\n30\n2011-08-14\nCross Country Race\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\n\n\n31\n2011-08-21\nCross Country Race\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\n\n\n32\n2011-08-28\nCross Country Race\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\n\n\n33\n2011-09-04\nCross Country Race\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\n\n\n\n\n\n\n\nThe final phase in our feature engineering journey is one-hot encoding our categorical variables. While certain machine learning models like CatBoost can natively handle categorical data, many cannot. Enter one-hot encoding, a technique that transforms each category within a column into its separate column, marking its presence with a ‘1’ or absence with a ‘0’.\nFor this transformation, the handy pd.get_dummies() function from pandas comes to the rescue.\n\n\nCode\ndf_encoded = pd.get_dummies(df_with_datefeatures_narrom, columns=['category_2'])\n\ndf_encoded.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nPytimetk offers an extensive array of feature engineering tools and augmentation functions, giving you a broad spectrum of possibilities. However, for the purposes of this tutorial, let’s shift our focus to modeling.\nLet’s proceed by segmenting our dataframe into training and future sets.\n\n\nCode\nfuture = df_encoded[df_encoded.total_price_sum.isnull()]\ntrain = df_encoded[df_encoded.total_price_sum.notnull()]\n\n\nLet’s focus on the columns essential for training. You’ll observe that we’ve excluded the ‘order_date’ column. This is because numerous machine learning models struggle with date data types. This is precisely why we utilized the tk.augment_timeseries_signature earlier—to transform date features into a format that’s compatible with ML models.\n\n\nCode\ntrain.columns\n\n\nIndex(['order_date', 'total_price_sum', 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_year', 'order_date_half',\n       'order_date_quarter', 'order_date_month', 'order_date_yweek',\n       'category_2_Cross Country Race', 'category_2_Cyclocross',\n       'category_2_Elite Road', 'category_2_Endurance Road',\n       'category_2_Fat Bike', 'category_2_Over Mountain', 'category_2_Sport',\n       'category_2_Trail', 'category_2_Triathalon'],\n      dtype='object')\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ntrain_columns = [ 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_year', 'order_date_half',\n       'order_date_quarter', 'order_date_month', 'order_date_yweek','category_2_Cross Country Race', 'category_2_Cyclocross',\n       'category_2_Elite Road', 'category_2_Endurance Road',\n       'category_2_Fat Bike', 'category_2_Over Mountain', 'category_2_Sport',\n       'category_2_Trail', 'category_2_Triathalon']\nX = train[train_columns]\ny = train[['total_price_sum']]\n\nmodel = RandomForestClassifier()\nmodel = model.fit(X, y)\n\n\nWe now have a fitted model, and can use this to predict sales from our future frame.\n\n\nCode\npredicted_values = model.predict(future[train_columns])\nfuture['y_pred'] = predicted_values\n\nfuture.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\ny_pred\n\n\n\n\n51\n2012-01-08\nNaN\n51820.0\n75720.0\n2012\n1\n1\n1\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n52\n2012-01-15\nNaN\n62940.0\n21240.0\n2012\n1\n1\n1\n2\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n53\n2012-01-22\nNaN\n9060.0\n11620.0\n2012\n1\n1\n1\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n2660.0\n\n\n54\n2012-01-29\nNaN\n15980.0\n9730.0\n2012\n1\n1\n1\n4\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n2660.0\n\n\n55\n2012-02-05\nNaN\n59180.0\n22780.0\n2012\n1\n1\n2\n5\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n56\n2012-02-12\nNaN\n132550.0\n53680.0\n2012\n1\n1\n2\n6\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n57\n2012-02-19\nNaN\n68430.0\n38360.0\n2012\n1\n1\n2\n7\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n58\n2012-02-26\nNaN\n29470.0\n90290.0\n2012\n1\n1\n2\n8\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n59\n2012-03-04\nNaN\n71080.0\n7380.0\n2012\n1\n1\n3\n9\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n56430.0\n\n\n60\n2012-03-11\nNaN\n9800.0\n0.0\n2012\n1\n1\n3\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n11410.0\n\n\n\n\n\n\n\nNow let us do a little cleanup. For ease in plotting later, let’s add a column to track the actuals vs. the predicted values.\n\n\nCode\ntrain['type'] = 'actuals'\nfuture['type'] = 'prediction'\n\nfull_df = pd.concat([train, future])\n\nfull_df.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\ntype\ny_pred\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n\n\n\n\n\nYou can get the grouping category back from the one-hot encoding for easier plotting.\nFor simplicity, we will search for any column with ‘category’ in its name.\n\n\nCode\n# Extract dummy columns\ndummy_cols = [col for col in full_df.columns if 'category' in col.lower() ]\nfull_df_reverted = full_df.copy()\n\n# Convert dummy columns back to categorical column\nfull_df_reverted['category'] = full_df_reverted[dummy_cols].idxmax(axis=1).str.replace(\"A_\", \"\")\n\n# Drop dummy columns\nfull_df_reverted = full_df_reverted.drop(columns=dummy_cols)\n\nfull_df_reverted.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ntype\ny_pred\ncategory\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n\n\n\n\n\nBefore we proceed to visualization, let’s streamline our dataset by aligning our predicted values with the actuals. This approach will simplify the plotting process. Given that our DataFrame columns are already labeled as ‘actuals’ and ‘predictions’, a brief conditional check will allow us to consolidate the necessary values.\n\n\nCode\nfull_df_reverted['total_price_sum'] = np.where(full_df_reverted.type =='actuals', full_df_reverted.total_price_sum, full_df_reverted.y_pred)\n\nfull_df_reverted.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ntype\ny_pred\ncategory\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n\n\n\n\n\n\nPlotlyPlotnine\n\n\n\n\nCode\nfull_df_reverted \\\n    .groupby('category') \\\n    .plot_timeseries(\n        date_column = 'order_date',\n        value_column = 'total_price_sum',\n        color_column = 'type',\n        smooth = False,\n        smooth_alpha = 0,\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 800,\n        height = 600,\n        engine = 'plotly'\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nfull_df_reverted \\\n    .groupby('category') \\\n    .plot_timeseries(\n        date_column = 'order_date',\n        value_column = 'total_price_sum',\n        color_column = 'type',\n        smooth = False,\n        smooth_alpha = 0,\n        facet_ncol = 2,    \n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        engine = 'plotnine'\n    )\n\n\n\n\n\n&lt;Figure Size: (1000 x 800)&gt;\n\n\n\n\n\nUpon examining the graph, our models look alright given the length of time for training. Important points:\n\nFor effective time series forecasting, having multiple years of data is pivotal. This provides the model ample opportunities to recognize and adapt to seasonal variations.\nGiven our dataset spanned less than a year, the model lacked the depth of historical context to discern such patterns.\nAlthough our feature engineering was kept basic to introduce various pytimetk capabilities, there’s room for enhancement.\nFor a more refined analysis, consider experimenting with different machine learning models and diving deeper into feature engineering.\nPytimetk’s tk.augment_fourier() might assist in discerning seasonal trends, but with the dataset’s limited historical scope, capturing intricate patterns could remain a challenge."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#inspect-with-tk.glimpse",
    "href": "tutorials/01_sales_crm.html#inspect-with-tk.glimpse",
    "title": "Sales CRM Database Analysis",
    "section": "1.3 Inspect with tk.glimpse",
    "text": "1.3 Inspect with tk.glimpse\nTo get a preliminary understanding of our data, let’s utilize the tk.glimpse() function from pytimetk. This will provide us with a snapshot of the available fields, their respective data types, and a sneak peek into the data entries.\n\n\nCode\ndf = tk.datasets.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf.glimpse()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;: 2466 rows of 13 columns\norder_id:        int64            [1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5 ...\norder_line:      int64            [1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3 ...\norder_date:      datetime64[ns]   [Timestamp('2011-01-07 00:00:00'), Tim ...\nquantity:        int64            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1 ...\nprice:           int64            [6070, 5970, 2770, 5970, 10660, 3200,  ...\ntotal_price:     int64            [6070, 5970, 2770, 5970, 10660, 3200,  ...\nmodel:           object           ['Jekyll Carbon 2', 'Trigger Carbon 2' ...\ncategory_1:      object           ['Mountain', 'Mountain', 'Mountain', ' ...\ncategory_2:      object           ['Over Mountain', 'Over Mountain', 'Tr ...\nframe_material:  object           ['Carbon', 'Carbon', 'Aluminum', 'Carb ...\nbikeshop_name:   object           ['Ithaca Mountain Climbers', 'Ithaca M ...\ncity:            object           ['Ithaca', 'Ithaca', 'Kansas City', 'K ...\nstate:           object           ['NY', 'NY', 'KS', 'KS', 'KY', 'KY', ' ..."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#initial-inspection-with-tk.glimpse",
    "href": "tutorials/01_sales_crm.html#initial-inspection-with-tk.glimpse",
    "title": "Sales Customer Analysis",
    "section": "2.1 Initial Inspection with tk.glimpse",
    "text": "2.1 Initial Inspection with tk.glimpse\nTo get a preliminary understanding of our data, let’s utilize the tk.glimpse() function from pytimetk. This will provide us with a snapshot of the available fields, their respective data types, and a sneak peek into the data entries.\n\n\nCode\ndf = tk.datasets.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf.glimpse()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;: 2466 rows of 13 columns\norder_id:        int64            [1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5 ...\norder_line:      int64            [1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3 ...\norder_date:      datetime64[ns]   [Timestamp('2011-01-07 00:00:00'), Tim ...\nquantity:        int64            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1 ...\nprice:           int64            [6070, 5970, 2770, 5970, 10660, 3200,  ...\ntotal_price:     int64            [6070, 5970, 2770, 5970, 10660, 3200,  ...\nmodel:           object           ['Jekyll Carbon 2', 'Trigger Carbon 2' ...\ncategory_1:      object           ['Mountain', 'Mountain', 'Mountain', ' ...\ncategory_2:      object           ['Over Mountain', 'Over Mountain', 'Tr ...\nframe_material:  object           ['Carbon', 'Carbon', 'Aluminum', 'Carb ...\nbikeshop_name:   object           ['Ithaca Mountain Climbers', 'Ithaca M ...\ncity:            object           ['Ithaca', 'Ithaca', 'Kansas City', 'K ...\nstate:           object           ['NY', 'NY', 'KS', 'KS', 'KY', 'KY', ' ..."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#data-exploration-with-tk.summarize_by_time",
    "href": "tutorials/01_sales_crm.html#data-exploration-with-tk.summarize_by_time",
    "title": "Sales Customer Analysis",
    "section": "2.2 Data Exploration with tk.summarize_by_time",
    "text": "2.2 Data Exploration with tk.summarize_by_time\nCRM data is often bustling with activity, reflecting the myriad of transactions happening daily. Due to this high volume, the data can sometimes seem overwhelming or noisy. To derive meaningful insights, it’s essential to aggregate this data over specific time intervals. This is where tk.summarize_by_time() comes into play.\nThe tk.summarize_by_time() function offers a streamlined approach to time-based data aggregation. By defining a desired frequency and an aggregation method, this function seamlessly organizes your data. The beauty of it is its versatility; from a broad array of built-in aggregation methods and frequencies to the flexibility of integrating a custom function, it caters to a range of requirements.\n\n\n\n\n\n\nGetting to know tk.summarize_by_time()\n\n\n\n\n\nCurious about the various options it provides?\n\nClick here to see our Data Wrangling Guide\nUse help(tk.summarize_by_time) to review additional helpful documentation. And explore the plethora of possibilities!\n\n\n\n\n\nGetting Weekly Totals\nWe can quickly get totals by week with summarize_byt_time.\n\n\nCode\nweekly_totals = df.summarize_by_time(\n    date_column  = 'order_date',\n    value_column = 'total_price',\n    agg_func     = ['sum'],\n    freq         = 'W'\n)\n\nweekly_totals.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\n\n\n\n\n0\n2011-01-09\n12040\n\n\n1\n2011-01-16\n151460\n\n\n2\n2011-01-23\n143850\n\n\n3\n2011-01-30\n175665\n\n\n4\n2011-02-06\n105210\n\n\n5\n2011-02-13\n250390\n\n\n6\n2011-02-20\n410595\n\n\n7\n2011-02-27\n254045\n\n\n8\n2011-03-06\n308420\n\n\n9\n2011-03-13\n45450\n\n\n\n\n\n\n\n\n\nGet Weekly Totals by Group (Category 2)\nTo better understand your data, you might want to add groups to this summary. We can include a groupby before the summarize_by_time and then aggregate our data.\n\n\nCode\n sales_by_week = df \\\n    .groupby('category_2') \\\n    .summarize_by_time(\n        date_column = 'order_date',\n        value_column = 'total_price',\n        agg_func = ['sum'],\n        freq = 'W'\n    )\n\nsales_by_week.head(10)\n\n\n\n\n\n\n\n\n\ncategory_2\norder_date\ntotal_price_sum\n\n\n\n\n0\nCross Country Race\n2011-01-16\n61750\n\n\n1\nCross Country Race\n2011-01-23\n25050\n\n\n2\nCross Country Race\n2011-01-30\n56860\n\n\n3\nCross Country Race\n2011-02-06\n8740\n\n\n4\nCross Country Race\n2011-02-13\n78070\n\n\n5\nCross Country Race\n2011-02-20\n115010\n\n\n6\nCross Country Race\n2011-02-27\n64290\n\n\n7\nCross Country Race\n2011-03-06\n95070\n\n\n8\nCross Country Race\n2011-03-13\n3200\n\n\n9\nCross Country Race\n2011-03-20\n21170\n\n\n\n\n\n\n\n\n\nLong vs Wide Format\nThis long format can make it a little hard to compare the different group values visually, so instead of long-format you might want to pivot wide to view the data.\n\n\nCode\nsales_by_week_wide = df \\\n    .groupby('category_2') \\\n    .summarize_by_time(\n        date_column = 'order_date',\n        value_column = 'total_price',\n        agg_func = ['sum'],\n        freq = 'W',\n        wide_format = True\n    )\n\nsales_by_week_wide.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum_Cross Country Race\ntotal_price_sum_Cyclocross\ntotal_price_sum_Elite Road\ntotal_price_sum_Endurance Road\ntotal_price_sum_Fat Bike\ntotal_price_sum_Over Mountain\ntotal_price_sum_Sport\ntotal_price_sum_Trail\ntotal_price_sum_Triathalon\n\n\n\n\n0\n2011-01-09\n0.0\n0.0\n0.0\n0.0\n0.0\n12040.0\n0.0\n0.0\n0.0\n\n\n1\n2011-01-16\n61750.0\n1960.0\n49540.0\n11110.0\n0.0\n9170.0\n4030.0\n7450.0\n6450.0\n\n\n2\n2011-01-23\n25050.0\n3500.0\n51330.0\n47930.0\n0.0\n3840.0\n0.0\n0.0\n12200.0\n\n\n3\n2011-01-30\n56860.0\n2450.0\n43895.0\n24160.0\n0.0\n10880.0\n3720.0\n26700.0\n7000.0\n\n\n4\n2011-02-06\n8740.0\n7000.0\n35640.0\n22680.0\n3730.0\n14270.0\n980.0\n10220.0\n1950.0\n\n\n5\n2011-02-13\n78070.0\n0.0\n83780.0\n24820.0\n2130.0\n17160.0\n6810.0\n17120.0\n20500.0\n\n\n6\n2011-02-20\n115010.0\n7910.0\n79770.0\n27650.0\n26100.0\n37830.0\n10925.0\n96250.0\n9150.0\n\n\n7\n2011-02-27\n64290.0\n6650.0\n86900.0\n31900.0\n5860.0\n22070.0\n6165.0\n16410.0\n13800.0\n\n\n8\n2011-03-06\n95070.0\n2450.0\n31990.0\n47660.0\n5860.0\n82060.0\n9340.0\n26790.0\n7200.0\n\n\n9\n2011-03-13\n3200.0\n4200.0\n23110.0\n7260.0\n0.0\n5970.0\n1710.0\n0.0\n0.0\n\n\n\n\n\n\n\nYou can now observe the total sales for each product side by side. This streamlined view facilitates easy comparison between product sales."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#making-future-dates-easier-with-tk.future_frame",
    "href": "tutorials/01_sales_crm.html#making-future-dates-easier-with-tk.future_frame",
    "title": "Sales Customer Analysis",
    "section": "3.2 Making Future Dates Easier with tk.future_frame",
    "text": "3.2 Making Future Dates Easier with tk.future_frame\nMoving on, let’s set up the future frame, which will serve as our test dataset. To achieve this, employ the tk.future_frame() method. This function allows for the specification of a grouping column and a forecast horizon.\nUpon invoking tk.future_frame(), you’ll observe that placeholders (null values) are added for each group, extending 12 weeks into the future.\n\n\nCode\ndf_with_futureframe = sales_padded \\\n    .groupby('category_2') \\\n    .future_frame(\n        date_column = 'order_date',\n        length_out  = 12\n    )\ndf_with_futureframe\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\n\n\n...\n...\n...\n...\n\n\n58\n2012-02-26\nTriathalon\nNaN\n\n\n59\n2012-03-04\nTriathalon\nNaN\n\n\n60\n2012-03-11\nTriathalon\nNaN\n\n\n61\n2012-03-18\nTriathalon\nNaN\n\n\n62\n2012-03-25\nTriathalon\nNaN\n\n\n\n\n565 rows × 3 columns"
  },
  {
    "objectID": "tutorials/01_sales_crm.html#lag-values-with-tk.augment_lags",
    "href": "tutorials/01_sales_crm.html#lag-values-with-tk.augment_lags",
    "title": "Sales Customer Analysis",
    "section": "3.3 Lag Values with tk.augment_lags",
    "text": "3.3 Lag Values with tk.augment_lags\nCrafting features from time series data can be intricate, but thanks to the suite of feature engineering tools in pytimetk, the process is streamlined and intuitive.\nIn this guide, we’ll focus on the basics: introducing a few lag variables and incorporating some date-related features.\nFirstly, let’s dive into creating lag features.\nGiven our forecasting objective of a 12-week horizon, to ensure we have lag data available for every future point, we should utilize a lag of 12 or more. The beauty of the toolkit is that it supports the addition of multiple lags simultaneously.\nLag features play a pivotal role in machine learning for time series. Often, recent data offers valuable insights into future trends. To capture this recency effect, it’s crucial to integrate lag values. For this purpose, tk.augment_lags() comes in handy.\n\n\nCode\ndf_with_lags = df_with_futureframe \\\n    .groupby('category_2') \\\n    .augment_lags(\n        date_column  = 'order_date',\n        value_column = 'total_price_sum',\n        lags         = [12,24]\n\n    )\ndf_with_lags.head(25)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\n\n\n\n\n0\n2011-01-16\nCross Country Race\n61750.0\nNaN\nNaN\n\n\n1\n2011-01-23\nCross Country Race\n25050.0\nNaN\nNaN\n\n\n2\n2011-01-30\nCross Country Race\n56860.0\nNaN\nNaN\n\n\n3\n2011-02-06\nCross Country Race\n8740.0\nNaN\nNaN\n\n\n4\n2011-02-13\nCross Country Race\n78070.0\nNaN\nNaN\n\n\n5\n2011-02-20\nCross Country Race\n115010.0\nNaN\nNaN\n\n\n6\n2011-02-27\nCross Country Race\n64290.0\nNaN\nNaN\n\n\n7\n2011-03-06\nCross Country Race\n95070.0\nNaN\nNaN\n\n\n8\n2011-03-13\nCross Country Race\n3200.0\nNaN\nNaN\n\n\n9\n2011-03-20\nCross Country Race\n21170.0\nNaN\nNaN\n\n\n10\n2011-03-27\nCross Country Race\n28990.0\nNaN\nNaN\n\n\n11\n2011-04-03\nCross Country Race\n51860.0\nNaN\nNaN\n\n\n12\n2011-04-10\nCross Country Race\n85910.0\n61750.0\nNaN\n\n\n13\n2011-04-17\nCross Country Race\n138230.0\n25050.0\nNaN\n\n\n14\n2011-04-24\nCross Country Race\n138350.0\n56860.0\nNaN\n\n\n15\n2011-05-01\nCross Country Race\n136090.0\n8740.0\nNaN\n\n\n16\n2011-05-08\nCross Country Race\n32110.0\n78070.0\nNaN\n\n\n17\n2011-05-15\nCross Country Race\n139010.0\n115010.0\nNaN\n\n\n18\n2011-05-22\nCross Country Race\n2060.0\n64290.0\nNaN\n\n\n19\n2011-05-29\nCross Country Race\n26130.0\n95070.0\nNaN\n\n\n20\n2011-06-05\nCross Country Race\n30360.0\n3200.0\nNaN\n\n\n21\n2011-06-12\nCross Country Race\n88280.0\n21170.0\nNaN\n\n\n22\n2011-06-19\nCross Country Race\n109470.0\n28990.0\nNaN\n\n\n23\n2011-06-26\nCross Country Race\n107280.0\n51860.0\nNaN\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n\n\n\n\n\n\n\nObserve that lag values of 12 and 24 introduce missing entries at the dataset’s outset. This occurs because there isn’t available data from 12 or 24 weeks prior. To address these gaps, you can adopt one of two strategies:\n\nDiscard the Affected Rows: This is a recommended approach if your dataset is sufficiently large. Removing a few initial rows might not significantly impact the training process.\nBackfill Missing Values: In situations with limited data, you might consider backfilling these nulls using the first available values from lag 12 and 24. However, the appropriateness of this technique hinges on your specific context and objectives.\n\nFor the scope of this tutorial, we’ll opt to remove these rows. However, it’s worth pointing out that our dataset is quite small with limited historical data, so this might impact our model.\n\n\nCode\nlag_columns = [col for col in df_with_lags.columns if 'lag' in col]\ndf_no_nas = df_with_lags \\\n    .dropna(subset=lag_columns, inplace=False)\n\ndf_no_nas.head()\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0"
  },
  {
    "objectID": "tutorials/01_sales_crm.html#date-features-with-tk.augment_timeseries_signature",
    "href": "tutorials/01_sales_crm.html#date-features-with-tk.augment_timeseries_signature",
    "title": "Sales Customer Analysis",
    "section": "3.4 Date Features with tk.augment_timeseries_signature",
    "text": "3.4 Date Features with tk.augment_timeseries_signature\nNow, let’s enrich our dataset with date-related features.\nWith the function tk.augment_timeseries_signature(), you can effortlessly append 29 date attributes to a timestamp. Given that our dataset captures weekly intervals, certain attributes like ‘hour’ may not be pertinent. Thus, it’s prudent to refine our columns, retaining only those that truly matter to our analysis.\n\n\nCode\ndf_with_datefeatures = df_no_nas \\\n    .augment_timeseries_signature(date_column='order_date')\n\ndf_with_datefeatures.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\n...\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n1309651200\n2011\n2011\n0\n0\n...\n3\n3\n184\n1\n0\n0\n0\n0\n0\nam\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n1310256000\n2011\n2011\n0\n0\n...\n10\n10\n191\n1\n0\n0\n0\n0\n0\nam\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n1310860800\n2011\n2011\n0\n0\n...\n17\n17\n198\n1\n0\n0\n0\n0\n0\nam\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n1311465600\n2011\n2011\n0\n0\n...\n24\n24\n205\n1\n0\n0\n0\n0\n0\nam\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n1312070400\n2011\n2011\n0\n0\n...\n31\n31\n212\n1\n0\n0\n0\n0\n0\nam\n\n\n29\n2011-08-07\nCross Country Race\n11620.0\n139010.0\n115010.0\n1312675200\n2011\n2011\n0\n0\n...\n7\n38\n219\n1\n0\n0\n0\n0\n0\nam\n\n\n30\n2011-08-14\nCross Country Race\n9730.0\n2060.0\n64290.0\n1313280000\n2011\n2011\n0\n0\n...\n14\n45\n226\n1\n0\n0\n0\n0\n0\nam\n\n\n31\n2011-08-21\nCross Country Race\n22780.0\n26130.0\n95070.0\n1313884800\n2011\n2011\n0\n0\n...\n21\n52\n233\n1\n0\n0\n0\n0\n0\nam\n\n\n32\n2011-08-28\nCross Country Race\n53680.0\n30360.0\n3200.0\n1314489600\n2011\n2011\n0\n0\n...\n28\n59\n240\n1\n0\n0\n0\n0\n0\nam\n\n\n33\n2011-09-04\nCross Country Race\n38360.0\n88280.0\n21170.0\n1315094400\n2011\n2011\n0\n0\n...\n4\n66\n247\n1\n0\n0\n0\n0\n0\nam\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\nCode\ndf_with_datefeatures.columns\n\n\nIndex(['order_date', 'category_2', 'total_price_sum', 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_index_num', 'order_date_year',\n       'order_date_year_iso', 'order_date_yearstart', 'order_date_yearend',\n       'order_date_leapyear', 'order_date_half', 'order_date_quarter',\n       'order_date_quarteryear', 'order_date_quarterstart',\n       'order_date_quarterend', 'order_date_month', 'order_date_month_lbl',\n       'order_date_monthstart', 'order_date_monthend', 'order_date_yweek',\n       'order_date_mweek', 'order_date_wday', 'order_date_wday_lbl',\n       'order_date_mday', 'order_date_qday', 'order_date_yday',\n       'order_date_weekend', 'order_date_hour', 'order_date_minute',\n       'order_date_second', 'order_date_msecond', 'order_date_nsecond',\n       'order_date_am_pm'],\n      dtype='object')\n\n\nLet’s subset to just a few of the relevant date features.\n\n\nCode\ndf_with_datefeatures_narrom = df_with_datefeatures[[\n    'order_date', \n    'category_2', \n    'total_price_sum',\n    'total_price_sum_lag_12',\n    'total_price_sum_lag_24',\n    'order_date_year',  \n    'order_date_half', \n    'order_date_quarter',      \n    'order_date_month',\n    'order_date_yweek'\n]]\n\ndf_with_datefeatures_narrom.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ncategory_2\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\n\n\n\n\n24\n2011-07-03\nCross Country Race\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\n\n\n25\n2011-07-10\nCross Country Race\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\n\n\n26\n2011-07-17\nCross Country Race\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\n\n\n27\n2011-07-24\nCross Country Race\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\n\n\n28\n2011-07-31\nCross Country Race\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\n\n\n29\n2011-08-07\nCross Country Race\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\n\n\n30\n2011-08-14\nCross Country Race\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\n\n\n31\n2011-08-21\nCross Country Race\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\n\n\n32\n2011-08-28\nCross Country Race\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\n\n\n33\n2011-09-04\nCross Country Race\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\n\n\n\n\n\n\n\n\nOne-Hot Encoding\nThe final phase in our feature engineering journey is one-hot encoding our categorical variables. While certain machine learning models like CatBoost can natively handle categorical data, many cannot. Enter one-hot encoding, a technique that transforms each category within a column into its separate column, marking its presence with a ‘1’ or absence with a ‘0’.\nFor this transformation, the handy pd.get_dummies() function from pandas comes to the rescue.\n\n\nCode\ndf_encoded = pd.get_dummies(df_with_datefeatures_narrom, columns=['category_2'])\n\ndf_encoded.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\nTraining and Future Feature Sets\nPytimetk offers an extensive array of feature engineering tools and augmentation functions, giving you a broad spectrum of possibilities. However, for the purposes of this tutorial, let’s shift our focus to modeling.\nLet’s proceed by segmenting our dataframe into training and future sets.\n\n\nCode\nfuture = df_encoded[df_encoded.total_price_sum.isnull()]\ntrain = df_encoded[df_encoded.total_price_sum.notnull()]\n\n\nLet’s focus on the columns essential for training. You’ll observe that we’ve excluded the ‘order_date’ column. This is because numerous machine learning models struggle with date data types. This is precisely why we utilized the tk.augment_timeseries_signature earlier—to transform date features into a format that’s compatible with ML models.\nWe can quickly see what features we have available with tk.glimpse().\n\n\nCode\ntrain.glimpse()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;: 233 rows of 18 columns\norder_date:                     datetime64[ns]   [Timestamp('2011-07-03  ...\ntotal_price_sum:                float64          [56430.0, 62320.0, 1416 ...\ntotal_price_sum_lag_12:         float64          [85910.0, 138230.0, 138 ...\ntotal_price_sum_lag_24:         float64          [61750.0, 25050.0, 5686 ...\norder_date_year:                int32            [2011, 2011, 2011, 2011 ...\norder_date_half:                int32            [2, 2, 2, 2, 2, 2, 2, 2 ...\norder_date_quarter:             int32            [3, 3, 3, 3, 3, 3, 3, 3 ...\norder_date_month:               int32            [7, 7, 7, 7, 7, 8, 8, 8 ...\norder_date_yweek:               UInt32           [26, 27, 28, 29, 30, 31 ...\ncategory_2_Cross Country Race:  bool             [True, True, True, True ...\ncategory_2_Cyclocross:          bool             [False, False, False, F ...\ncategory_2_Elite Road:          bool             [False, False, False, F ...\ncategory_2_Endurance Road:      bool             [False, False, False, F ...\ncategory_2_Fat Bike:            bool             [False, False, False, F ...\ncategory_2_Over Mountain:       bool             [False, False, False, F ...\ncategory_2_Sport:               bool             [False, False, False, F ...\ncategory_2_Trail:               bool             [False, False, False, F ...\ncategory_2_Triathalon:          bool             [False, False, False, F ..."
  },
  {
    "objectID": "tutorials/01_sales_crm.html#scikit-learn-model",
    "href": "tutorials/01_sales_crm.html#scikit-learn-model",
    "title": "Sales Customer Analysis",
    "section": "3.5 Scikit Learn Model",
    "text": "3.5 Scikit Learn Model\nNow for some machine learning.\n\nFitting a Random Forest Regressor\nLet’s create a RandomForestRegressor to predict future sales patterns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\ntrain_columns = [ 'total_price_sum_lag_12',\n       'total_price_sum_lag_24', 'order_date_year', 'order_date_half',\n       'order_date_quarter', 'order_date_month', 'order_date_yweek','category_2_Cross Country Race', 'category_2_Cyclocross',\n       'category_2_Elite Road', 'category_2_Endurance Road',\n       'category_2_Fat Bike', 'category_2_Over Mountain', 'category_2_Sport',\n       'category_2_Trail', 'category_2_Triathalon']\nX = train[train_columns]\ny = train[['total_price_sum']]\n\nmodel = RandomForestRegressor(random_state=123)\nmodel = model.fit(X, y)\n\n\n\nPrediction\nWe now have a fitted model, and can use this to predict sales from our future frame.\n\n\nCode\npredicted_values = model.predict(future[train_columns])\nfuture['y_pred'] = predicted_values\n\nfuture.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\ny_pred\n\n\n\n\n51\n2012-01-08\nNaN\n51820.0\n75720.0\n2012\n1\n1\n1\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n59462.00\n\n\n52\n2012-01-15\nNaN\n62940.0\n21240.0\n2012\n1\n1\n1\n2\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n59149.45\n\n\n53\n2012-01-22\nNaN\n9060.0\n11620.0\n2012\n1\n1\n1\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n20458.40\n\n\n54\n2012-01-29\nNaN\n15980.0\n9730.0\n2012\n1\n1\n1\n4\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n31914.00\n\n\n55\n2012-02-05\nNaN\n59180.0\n22780.0\n2012\n1\n1\n2\n5\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n59128.95\n\n\n56\n2012-02-12\nNaN\n132550.0\n53680.0\n2012\n1\n1\n2\n6\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n76397.50\n\n\n57\n2012-02-19\nNaN\n68430.0\n38360.0\n2012\n1\n1\n2\n7\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n63497.80\n\n\n58\n2012-02-26\nNaN\n29470.0\n90290.0\n2012\n1\n1\n2\n8\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n57332.00\n\n\n59\n2012-03-04\nNaN\n71080.0\n7380.0\n2012\n1\n1\n3\n9\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n60981.30\n\n\n60\n2012-03-11\nNaN\n9800.0\n0.0\n2012\n1\n1\n3\n10\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n18738.15\n\n\n\n\n\n\n\n\n\nCleaning Up\nNow let us do a little cleanup. For ease in plotting later, let’s add a column to track the actuals vs. the predicted values.\n\n\nCode\ntrain['type'] = 'actuals'\nfuture['type'] = 'prediction'\n\nfull_df = pd.concat([train, future])\n\nfull_df.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ncategory_2_Cross Country Race\ncategory_2_Cyclocross\ncategory_2_Elite Road\ncategory_2_Endurance Road\ncategory_2_Fat Bike\ncategory_2_Over Mountain\ncategory_2_Sport\ncategory_2_Trail\ncategory_2_Triathalon\ntype\ny_pred\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nactuals\nNaN\n\n\n\n\n\n\n\nYou can get the grouping category back from the one-hot encoding for easier plotting. For simplicity, we will search for any column with ‘category’ in its name.\n\n\nCode\n# Extract dummy columns\ndummy_cols = [col for col in full_df.columns if 'category' in col.lower() ]\nfull_df_reverted = full_df.copy()\n\n# Convert dummy columns back to categorical column\nfull_df_reverted['category'] = full_df_reverted[dummy_cols].idxmax(axis=1).str.replace(\"A_\", \"\")\n\n# Drop dummy columns\nfull_df_reverted = full_df_reverted.drop(columns=dummy_cols)\n\nfull_df_reverted.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ntype\ny_pred\ncategory\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n\n\n\n\n\n\n\nPre-Visualization Wrangling\nBefore we proceed to visualization, let’s streamline our dataset by aligning our predicted values with the actuals. This approach will simplify the plotting process. Given that our DataFrame columns are already labeled as ‘actuals’ and ‘predictions’, a brief conditional check will allow us to consolidate the necessary values.\n\n\nCode\nfull_df_reverted['total_price_sum'] = np.where(full_df_reverted.type =='actuals', full_df_reverted.total_price_sum, full_df_reverted.y_pred)\n\nfull_df_reverted.head(10)\n\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum\ntotal_price_sum_lag_12\ntotal_price_sum_lag_24\norder_date_year\norder_date_half\norder_date_quarter\norder_date_month\norder_date_yweek\ntype\ny_pred\ncategory\n\n\n\n\n24\n2011-07-03\n56430.0\n85910.0\n61750.0\n2011\n2\n3\n7\n26\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n25\n2011-07-10\n62320.0\n138230.0\n25050.0\n2011\n2\n3\n7\n27\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n26\n2011-07-17\n141620.0\n138350.0\n56860.0\n2011\n2\n3\n7\n28\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n27\n2011-07-24\n75720.0\n136090.0\n8740.0\n2011\n2\n3\n7\n29\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n28\n2011-07-31\n21240.0\n32110.0\n78070.0\n2011\n2\n3\n7\n30\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n29\n2011-08-07\n11620.0\n139010.0\n115010.0\n2011\n2\n3\n8\n31\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n30\n2011-08-14\n9730.0\n2060.0\n64290.0\n2011\n2\n3\n8\n32\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n31\n2011-08-21\n22780.0\n26130.0\n95070.0\n2011\n2\n3\n8\n33\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n32\n2011-08-28\n53680.0\n30360.0\n3200.0\n2011\n2\n3\n8\n34\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n33\n2011-09-04\n38360.0\n88280.0\n21170.0\n2011\n2\n3\n9\n35\nactuals\nNaN\ncategory_2_Cross Country Race\n\n\n\n\n\n\n\n\n\nVisualize the Forecast\nLet’s again use tk.plot_timeseries() to visually inspect the forecasts.\n\nPlotlyPlotnine\n\n\n\n\nCode\nfull_df_reverted \\\n    .groupby('category') \\\n    .plot_timeseries(\n        date_column = 'order_date',\n        value_column = 'total_price_sum',\n        color_column = 'type',\n        smooth = False,\n        smooth_alpha = 0,\n        facet_ncol = 2,\n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 800,\n        height = 600,\n        engine = 'plotly'\n    )\n\n\n\n                                                \n\n\n\n\n\n\nCode\nfull_df_reverted \\\n    .groupby('category') \\\n    .plot_timeseries(\n        date_column = 'order_date',\n        value_column = 'total_price_sum',\n        color_column = 'type',\n        smooth = False,\n        smooth_alpha = 0,\n        facet_ncol = 2,    \n        facet_scales = \"free\",\n        y_intercept_color = tk.palette_timetk()['steel_blue'],\n        width = 1000,\n        height = 800,\n        engine = 'plotnine'\n    )\n\n\n\n\n\n&lt;Figure Size: (1000 x 800)&gt;\n\n\n\n\n\nUpon examining the graph, our models look alright given the length of time for training. Important points:\n\nFor effective time series forecasting, having multiple years of data is pivotal. This provides the model ample opportunities to recognize and adapt to seasonal variations.\nGiven our dataset spanned less than a year, the model lacked the depth of historical context to discern such patterns.\nAlthough our feature engineering was kept basic to introduce various pytimetk capabilities, there’s room for enhancement.\nFor a more refined analysis, consider experimenting with different machine learning models and diving deeper into feature engineering.\nPytimetk’s tk.augment_fourier() might assist in discerning seasonal trends, but with the dataset’s limited historical scope, capturing intricate patterns could remain a challenge."
  }
]