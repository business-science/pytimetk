[
  {
    "objectID": "reference/floor_date.html",
    "href": "reference/floor_date.html",
    "title": "floor_date",
    "section": "",
    "text": "floor_date(idx, unit='D')\nRound a date down to the specified unit (e.g. Flooring).\nThe floor_date function takes a pandas Series of dates and returns a new Series with the dates rounded down to the specified unit."
  },
  {
    "objectID": "reference/floor_date.html#parameters",
    "href": "reference/floor_date.html#parameters",
    "title": "floor_date",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter is a pandas Series or pandas DatetimeIndex object that contains datetime values. It represents the dates that you want to round down.\nrequired\n\n\nunit\nstr\nThe unit parameter in the floor_date function is a string that specifies the time unit to which the dates in the idx series should be rounded down. It has a default value of “D”, which stands for day. Other possible values for the unit parameter could be\n'D'"
  },
  {
    "objectID": "reference/floor_date.html#returns",
    "href": "reference/floor_date.html#returns",
    "title": "floor_date",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe floor_date function returns a pandas Series object containing datetime64[ns] values."
  },
  {
    "objectID": "reference/floor_date.html#examples",
    "href": "reference/floor_date.html#examples",
    "title": "floor_date",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-01-10\", freq=\"1H\")\ndates\n\nDatetimeIndex(['2020-01-01 00:00:00', '2020-01-01 01:00:00',\n               '2020-01-01 02:00:00', '2020-01-01 03:00:00',\n               '2020-01-01 04:00:00', '2020-01-01 05:00:00',\n               '2020-01-01 06:00:00', '2020-01-01 07:00:00',\n               '2020-01-01 08:00:00', '2020-01-01 09:00:00',\n               ...\n               '2020-01-09 15:00:00', '2020-01-09 16:00:00',\n               '2020-01-09 17:00:00', '2020-01-09 18:00:00',\n               '2020-01-09 19:00:00', '2020-01-09 20:00:00',\n               '2020-01-09 21:00:00', '2020-01-09 22:00:00',\n               '2020-01-09 23:00:00', '2020-01-10 00:00:00'],\n              dtype='datetime64[ns]', length=217, freq='H')\n\n\n\n# Works on DateTimeIndex\ntk.floor_date(dates, unit=\"D\")\n\n0     2020-01-01\n1     2020-01-01\n2     2020-01-01\n3     2020-01-01\n4     2020-01-01\n         ...    \n212   2020-01-09\n213   2020-01-09\n214   2020-01-09\n215   2020-01-09\n216   2020-01-10\nName: idx, Length: 217, dtype: datetime64[ns]\n\n\n\n# Works on Pandas Series\ndates.to_series().floor_date(unit=\"D\")\n\n2020-01-01 00:00:00   2020-01-01\n2020-01-01 01:00:00   2020-01-01\n2020-01-01 02:00:00   2020-01-01\n2020-01-01 03:00:00   2020-01-01\n2020-01-01 04:00:00   2020-01-01\n                         ...    \n2020-01-09 20:00:00   2020-01-09\n2020-01-09 21:00:00   2020-01-09\n2020-01-09 22:00:00   2020-01-09\n2020-01-09 23:00:00   2020-01-09\n2020-01-10 00:00:00   2020-01-10\nFreq: H, Length: 217, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/get_timeseries_signature.html",
    "href": "reference/get_timeseries_signature.html",
    "title": "get_timeseries_signature",
    "section": "",
    "text": "get_timeseries_signature(idx)\nConvert a timestamp to a set of 29 time series features.\nThe function tk_get_timeseries_signature engineers 29 different date and time based features from a single datetime index idx:"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#parameters",
    "href": "reference/get_timeseries_signature.html#parameters",
    "title": "get_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nidx is a pandas Series object containing datetime values. Alternatively a pd.DatetimeIndex can be passed.\nrequired"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#returns",
    "href": "reference/get_timeseries_signature.html#returns",
    "title": "get_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function tk_get_timeseries_signature returns a pandas DataFrame that contains 29 different date and time based features derived from a single datetime column."
  },
  {
    "objectID": "reference/get_timeseries_signature.html#examples",
    "href": "reference/get_timeseries_signature.html#examples",
    "title": "get_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\ndates = pd.date_range(start = '2019-01', end = '2019-03', freq = 'D')\n\n# Makes 29 new time series features from the dates\ntk.get_timeseries_signature(dates).head()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\nquarterend\nmonth\nmonth_lbl\nmonthstart\nmonthend\nyweek\nmweek\nwday\nwday_lbl\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1546300800\n2019\n2019\n1\n0\n0\n1\n1\n2019Q1\n1\n0\n1\nJanuary\n1\n0\n1\n1\n2\nTuesday\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1546387200\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n3\nWednesday\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1546473600\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n4\nThursday\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1546560000\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1546646400\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n6\nSaturday\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/make_future_timeseries.html",
    "href": "reference/make_future_timeseries.html",
    "title": "make_future_timeseries",
    "section": "",
    "text": "make_future_timeseries(idx, length_out, force_regular=False)\nMake future dates for a time series.\nThe function make_future_timeseries takes a pandas Series or DateTimeIndex and generates a future sequence of dates based on the frequency of the input series."
  },
  {
    "objectID": "reference/make_future_timeseries.html#parameters",
    "href": "reference/make_future_timeseries.html#parameters",
    "title": "make_future_timeseries",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe idx parameter is the input time series data. It can be either a pandas Series or a pandas DateTimeIndex. It represents the existing dates in the time series.\nrequired\n\n\nlength_out\nint\nThe parameter length_out is an integer that represents the number of future dates to generate for the time series.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether the frequency of the future dates should be forced to be regular. If force_regular is set to True, the frequency of the future dates will be forced to be regular. If force_regular is set to False, the frequency of the future dates will be inferred from the input data (e.g. business calendars might be used). The default value is False.\nFalse"
  },
  {
    "objectID": "reference/make_future_timeseries.html#returns",
    "href": "reference/make_future_timeseries.html#returns",
    "title": "make_future_timeseries",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA pandas Series object containing future dates."
  },
  {
    "objectID": "reference/make_future_timeseries.html#examples",
    "href": "reference/make_future_timeseries.html#examples",
    "title": "make_future_timeseries",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndates = pd.Series(pd.to_datetime(['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04']))\ndates\n\n0   2022-01-01\n1   2022-01-02\n2   2022-01-03\n3   2022-01-04\ndtype: datetime64[ns]\n\n\n\n# DateTimeIndex: Generate 5 future dates\nfuture_dates_dt = tk.make_future_timeseries(dates, 5)\nfuture_dates_dt\n\n0   2022-01-05\n1   2022-01-06\n2   2022-01-07\n3   2022-01-08\n4   2022-01-09\ndtype: datetime64[ns]\n\n\n\n# Series: Generate 5 future dates\npd.Series(future_dates_dt).make_future_timeseries(5)\n\n0   2022-01-10\n1   2022-01-11\n2   2022-01-12\n3   2022-01-13\n4   2022-01-14\ndtype: datetime64[ns]\n\n\n\ntimestamps = [\"2023-01-01 01:00\", \"2023-01-01 02:00\", \"2023-01-01 03:00\", \"2023-01-01 04:00\", \"2023-01-01 05:00\"]\n\ndates = pd.to_datetime(timestamps)\n\ntk.make_future_timeseries(dates, 5)\n\n0   2023-01-01 06:00:00\n1   2023-01-01 07:00:00\n2   2023-01-01 08:00:00\n3   2023-01-01 09:00:00\n4   2023-01-01 10:00:00\ndtype: datetime64[ns]\n\n\n\n# Monthly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-02-01\", \"2021-03-01\", \"2021-04-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2021-05-01\n1   2021-06-01\n2   2021-07-01\n3   2021-08-01\ndtype: datetime64[ns]\n\n\n\n# Quarterly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-04-01\", \"2021-07-01\", \"2021-10-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2022-01-01\n1   2022-04-01\n2   2022-07-01\n3   2022-10-01\ndtype: datetime64[ns]\n\n\n\n# Irregular Dates: Business Days\ndates = pd.to_datetime([\"2021-01-01\", \"2021-01-04\", \"2021-01-05\", \"2021-01-06\"])\n\ntk.get_pandas_frequency(dates)\n\ntk.make_future_timeseries(dates, 4)\n\n0   2021-01-07\n1   2021-01-08\n2   2021-01-11\n3   2021-01-12\ndtype: datetime64[ns]\n\n\n\n# Irregular Dates: Business Days (Force Regular)    \ntk.make_future_timeseries(dates, 4, force_regular=True)\n\n0   2021-01-07\n1   2021-01-08\n2   2021-01-09\n3   2021-01-10\ndtype: datetime64[ns]"
  },
  {
    "objectID": "reference/future_frame.html",
    "href": "reference/future_frame.html",
    "title": "future_frame",
    "section": "",
    "text": "future_frame(data, date_column, length_out, force_regular=False, bind_data=True)\nExtend a DataFrame or GroupBy object with future dates.\nThe future_frame function extends a given DataFrame or GroupBy object with future dates based on a specified length, optionally binding the original data."
  },
  {
    "objectID": "reference/future_frame.html#parameters",
    "href": "reference/future_frame.html#parameters",
    "title": "future_frame",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to extend with future dates.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to generate future dates.\nrequired\n\n\nlength_out\nint\nThe length_out parameter specifies the number of future dates to be added to the DataFrame.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether the frequency of the future dates should be forced to be regular. If force_regular is set to True, the frequency of the future dates will be forced to be regular. If force_regular is set to False, the frequency of the future dates will be inferred from the input data (e.g. business calendars might be used). The default value is False.\nFalse\n\n\nbind_data\nbool\nThe bind_data parameter is a boolean flag that determines whether the extended data should be concatenated with the original data or returned separately. If bind_data is set to True, the extended data will be concatenated with the original data using pd.concat. If bind_data is set to False, the extended data will be returned separately. The default value is True.\nTrue"
  },
  {
    "objectID": "reference/future_frame.html#returns",
    "href": "reference/future_frame.html#returns",
    "title": "future_frame",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nAn extended DataFrame with future dates."
  },
  {
    "objectID": "reference/future_frame.html#see-also",
    "href": "reference/future_frame.html#see-also",
    "title": "future_frame",
    "section": "See Also",
    "text": "See Also\nmake_future_timeseries: Generate future dates for a time series."
  },
  {
    "objectID": "reference/future_frame.html#examples",
    "href": "reference/future_frame.html#examples",
    "title": "future_frame",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndf = tk.load_dataset('m4_hourly', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490\n\n\n...\n...\n...\n...\n\n\n3055\nH410\n2017-02-10 07:00:00+00:00\n108\n\n\n3056\nH410\n2017-02-10 08:00:00+00:00\n70\n\n\n3057\nH410\n2017-02-10 09:00:00+00:00\n72\n\n\n3058\nH410\n2017-02-10 10:00:00+00:00\n79\n\n\n3059\nH410\n2017-02-10 11:00:00+00:00\n77\n\n\n\n\n3060 rows × 3 columns\n\n\n\n\n# Extend the data for a single time series group by 12 hours\nextended_df = (\n    df\n        .query('id == \"H10\"')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12\n        )\n        .assign(id = lambda x: x['id'].ffill())\n)\nextended_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513.0\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512.0\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506.0\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500.0\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490.0\n\n\n...\n...\n...\n...\n\n\n707\nH10\n2015-07-30 23:00:00\nNaN\n\n\n708\nH10\n2015-07-31 00:00:00\nNaN\n\n\n709\nH10\n2015-07-31 01:00:00\nNaN\n\n\n710\nH10\n2015-07-31 02:00:00\nNaN\n\n\n711\nH10\n2015-07-31 03:00:00\nNaN\n\n\n\n\n712 rows × 3 columns\n\n\n\n\n# Extend the data for each group by 12 hours\nextended_df = (\n    df\n        .groupby('id')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513.0\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512.0\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506.0\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500.0\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490.0\n\n\n...\n...\n...\n...\n\n\n707\nH50\n2015-07-30 23:00:00\nNaN\n\n\n708\nH50\n2015-07-31 00:00:00\nNaN\n\n\n709\nH50\n2015-07-31 01:00:00\nNaN\n\n\n710\nH50\n2015-07-31 02:00:00\nNaN\n\n\n711\nH50\n2015-07-31 03:00:00\nNaN\n\n\n\n\n3108 rows × 3 columns\n\n\n\n\n# Same as above, but just return the extended data with bind_data=False\nextended_df = (\n    df\n        .groupby('id')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            bind_data   = False # Returns just future data\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nid\n\n\n\n\n0\n2015-07-30 16:00:00\nH10\n\n\n1\n2015-07-30 17:00:00\nH10\n\n\n2\n2015-07-30 18:00:00\nH10\n\n\n3\n2015-07-30 19:00:00\nH10\n\n\n4\n2015-07-30 20:00:00\nH10\n\n\n5\n2015-07-30 21:00:00\nH10\n\n\n6\n2015-07-30 22:00:00\nH10\n\n\n7\n2015-07-30 23:00:00\nH10\n\n\n8\n2015-07-31 00:00:00\nH10\n\n\n9\n2015-07-31 01:00:00\nH10\n\n\n10\n2015-07-31 02:00:00\nH10\n\n\n11\n2015-07-31 03:00:00\nH10\n\n\n0\n2013-09-30 16:00:00\nH150\n\n\n1\n2013-09-30 17:00:00\nH150\n\n\n2\n2013-09-30 18:00:00\nH150\n\n\n3\n2013-09-30 19:00:00\nH150\n\n\n4\n2013-09-30 20:00:00\nH150\n\n\n5\n2013-09-30 21:00:00\nH150\n\n\n6\n2013-09-30 22:00:00\nH150\n\n\n7\n2013-09-30 23:00:00\nH150\n\n\n8\n2013-10-01 00:00:00\nH150\n\n\n9\n2013-10-01 01:00:00\nH150\n\n\n10\n2013-10-01 02:00:00\nH150\n\n\n11\n2013-10-01 03:00:00\nH150\n\n\n0\n2017-02-10 12:00:00\nH410\n\n\n1\n2017-02-10 13:00:00\nH410\n\n\n2\n2017-02-10 14:00:00\nH410\n\n\n3\n2017-02-10 15:00:00\nH410\n\n\n4\n2017-02-10 16:00:00\nH410\n\n\n5\n2017-02-10 17:00:00\nH410\n\n\n6\n2017-02-10 18:00:00\nH410\n\n\n7\n2017-02-10 19:00:00\nH410\n\n\n8\n2017-02-10 20:00:00\nH410\n\n\n9\n2017-02-10 21:00:00\nH410\n\n\n10\n2017-02-10 22:00:00\nH410\n\n\n11\n2017-02-10 23:00:00\nH410\n\n\n0\n2015-07-30 16:00:00\nH50\n\n\n1\n2015-07-30 17:00:00\nH50\n\n\n2\n2015-07-30 18:00:00\nH50\n\n\n3\n2015-07-30 19:00:00\nH50\n\n\n4\n2015-07-30 20:00:00\nH50\n\n\n5\n2015-07-30 21:00:00\nH50\n\n\n6\n2015-07-30 22:00:00\nH50\n\n\n7\n2015-07-30 23:00:00\nH50\n\n\n8\n2015-07-31 00:00:00\nH50\n\n\n9\n2015-07-31 01:00:00\nH50\n\n\n10\n2015-07-31 02:00:00\nH50\n\n\n11\n2015-07-31 03:00:00\nH50\n\n\n\n\n\n\n\n\n # Working with irregular dates: Business Days (Stocks Data)\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Allow irregular future dates (i.e. business days)\nextended_df = (\n    df\n        .groupby('symbol')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            force_regular = False, # Allow irregular future dates (i.e. business days)),\n            bind_data   = False\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nsymbol\n\n\n\n\n0\n2023-09-22\nAAPL\n\n\n1\n2023-09-25\nAAPL\n\n\n2\n2023-09-26\nAAPL\n\n\n3\n2023-09-27\nAAPL\n\n\n4\n2023-09-28\nAAPL\n\n\n...\n...\n...\n\n\n7\n2023-10-03\nNVDA\n\n\n8\n2023-10-04\nNVDA\n\n\n9\n2023-10-05\nNVDA\n\n\n10\n2023-10-06\nNVDA\n\n\n11\n2023-10-09\nNVDA\n\n\n\n\n72 rows × 2 columns\n\n\n\n\n# Force regular: Include Weekends\nextended_df = (\n    df\n        .groupby('symbol')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            force_regular = True, # Force regular future dates (i.e. include weekends)),\n            bind_data   = False\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nsymbol\n\n\n\n\n0\n2023-09-22\nAAPL\n\n\n1\n2023-09-23\nAAPL\n\n\n2\n2023-09-24\nAAPL\n\n\n3\n2023-09-25\nAAPL\n\n\n4\n2023-09-26\nAAPL\n\n\n...\n...\n...\n\n\n7\n2023-09-29\nNVDA\n\n\n8\n2023-09-30\nNVDA\n\n\n9\n2023-10-01\nNVDA\n\n\n10\n2023-10-02\nNVDA\n\n\n11\n2023-10-03\nNVDA\n\n\n\n\n72 rows × 2 columns"
  },
  {
    "objectID": "reference/summarize_by_time.html",
    "href": "reference/summarize_by_time.html",
    "title": "summarize_by_time",
    "section": "",
    "text": "summarize_by_time(data, date_column, value_column, freq='D', agg_func='sum', kind='timestamp', wide_format=False, fillna=0, *args, **kwargs)\nSummarize a DataFrame or GroupBy object by time.\nThe summarize_by_time function aggregates data by a specified time period and one or more numeric columns, allowing for grouping and customization of the time-based aggregation."
  },
  {
    "objectID": "reference/summarize_by_time.html#parameters",
    "href": "reference/summarize_by_time.html#parameters",
    "title": "summarize_by_time",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nA pandas DataFrame or a pandas GroupBy object. This is the data that you want to summarize by time.\nrequired\n\n\ndate_column\nstr\nThe name of the column in the data frame that contains the dates or timestamps to be aggregated by. This column must be of type datetime64.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the name of one or more columns in the DataFrame that you want to aggregate by. It can be either a string representing a single column name, or a list of strings representing multiple column names.\nrequired\n\n\nfreq\nstr\nThe freq parameter specifies the frequency at which the data should be aggregated. It accepts a string representing a pandas frequency offset, such as “D” for daily or “MS” for month start. The default value is “D”, which means the data will be aggregated on a daily basis. Some common frequency aliases include: - S: secondly frequency - min: minute frequency - H: hourly frequency - D: daily frequency - W: weekly frequency - M: month end frequency - MS: month start frequency - Q: quarter end frequency - QS: quarter start frequency - Y: year end frequency - YS: year start frequency\n'D'\n\n\nagg_func\nlist\nThe agg_func parameter is used to specify one or more aggregating functions to apply to the value column(s) during the summarization process. It can be a single function or a list of functions. The default value is \"sum\", which represents the sum function. Some common aggregating functions include: - “sum”: Sum of values - “mean”: Mean of values - “median”: Median of values - “min”: Minimum of values - “max”: Maximum of values - “std”: Standard deviation of values - “var”: Variance of values - “first”: First value in group - “last”: Last value in group - “count”: Count of values - “nunique”: Number of unique values - “corr”: Correlation between values Custom lambda aggregating functions can be used too. Here are several common examples: - (“q25”, lambda x: x.quantile(0.25)): 25th percentile of values - (“q75”, lambda x: x.quantile(0.75)): 75th percentile of values - (“iqr”, lambda x: x.quantile(0.75) - x.quantile(0.25)): Interquartile range of values - (“range”, lambda x: x.max() - x.min()): Range of values\n'sum'\n\n\nwide_format\nbool\nA boolean parameter that determines whether the output should be in “wide” or “long” format. If set to True, the output will be in wide format, where each group is represented by a separate column. If set to False, the output will be in long format, where each group is represented by a separate row. The default value is False.\nFalse\n\n\nfillna\nint\nThe fillna parameter is used to specify the value to fill missing data with. By default, it is set to 0. If you want to keep missing values as NaN, you can use np.nan as the value for fillna.\n0"
  },
  {
    "objectID": "reference/summarize_by_time.html#returns",
    "href": "reference/summarize_by_time.html#returns",
    "title": "summarize_by_time",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame that is summarized by time."
  },
  {
    "objectID": "reference/summarize_by_time.html#examples",
    "href": "reference/summarize_by_time.html#examples",
    "title": "summarize_by_time",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Summarize by time with a DataFrame object\n( \n    df \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price',\n            freq         = \"MS\",\n            agg_func     = ['mean', 'sum']\n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_mean\ntotal_price_sum\n\n\n\n\n0\n2011-01-01\n4600.142857\n483015\n\n\n1\n2011-02-01\n4611.408730\n1162075\n\n\n2\n2011-03-01\n5196.653543\n659975\n\n\n3\n2011-04-01\n4533.846154\n1827140\n\n\n4\n2011-05-01\n4097.912621\n844170\n\n\n5\n2011-06-01\n4544.839228\n1413445\n\n\n6\n2011-07-01\n4976.791667\n1194430\n\n\n7\n2011-08-01\n4961.970803\n679790\n\n\n8\n2011-09-01\n4682.298851\n814720\n\n\n9\n2011-10-01\n3930.053476\n734920\n\n\n10\n2011-11-01\n4768.175355\n1006085\n\n\n11\n2011-12-01\n4186.902655\n473120\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Long Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = False, \n        )\n)\n\n\n\n\n\n\n\n\ncategory_1\norder_date\ntotal_price\n\n\n\n\n0\nMountain\n2011-01-01\n221490\n\n\n1\nMountain\n2011-02-01\n660555\n\n\n2\nMountain\n2011-03-01\n358855\n\n\n3\nMountain\n2011-04-01\n1075975\n\n\n4\nMountain\n2011-05-01\n450440\n\n\n5\nMountain\n2011-06-01\n723040\n\n\n6\nMountain\n2011-07-01\n767740\n\n\n7\nMountain\n2011-08-01\n361255\n\n\n8\nMountain\n2011-09-01\n401125\n\n\n9\nMountain\n2011-10-01\n377335\n\n\n10\nMountain\n2011-11-01\n549345\n\n\n11\nMountain\n2011-12-01\n276055\n\n\n12\nRoad\n2011-01-01\n261525\n\n\n13\nRoad\n2011-02-01\n501520\n\n\n14\nRoad\n2011-03-01\n301120\n\n\n15\nRoad\n2011-04-01\n751165\n\n\n16\nRoad\n2011-05-01\n393730\n\n\n17\nRoad\n2011-06-01\n690405\n\n\n18\nRoad\n2011-07-01\n426690\n\n\n19\nRoad\n2011-08-01\n318535\n\n\n20\nRoad\n2011-09-01\n413595\n\n\n21\nRoad\n2011-10-01\n357585\n\n\n22\nRoad\n2011-11-01\n456740\n\n\n23\nRoad\n2011-12-01\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object and multiple summaries (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = ['sum', 'mean', ('q25', lambda x: x.quantile(0.25)), ('q75', lambda x: x.quantile(0.75))],\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum_Mountain\ntotal_price_sum_Road\ntotal_price_mean_Mountain\ntotal_price_mean_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n4922.000000\n4358.750000\n2060.0\n1950.0\n6070.0\n5605.0\n\n\n1\n2011-02-01\n660555\n501520\n4374.536424\n4965.544554\n2060.0\n1950.0\n5330.0\n5860.0\n\n\n2\n2011-03-01\n358855\n301120\n5882.868852\n4562.424242\n2130.0\n2240.0\n6390.0\n5875.0\n\n\n3\n2011-04-01\n1075975\n751165\n4890.795455\n4104.726776\n2060.0\n1950.0\n5970.0\n4800.0\n\n\n4\n2011-05-01\n450440\n393730\n4549.898990\n3679.719626\n2010.0\n1570.0\n6020.0\n3500.0\n\n\n5\n2011-06-01\n723040\n690405\n5021.111111\n4134.161677\n1950.0\n1840.0\n5647.5\n4500.0\n\n\n6\n2011-07-01\n767740\n426690\n5444.964539\n4310.000000\n2130.0\n1895.0\n6400.0\n5330.0\n\n\n7\n2011-08-01\n361255\n318535\n5734.206349\n4304.527027\n2235.0\n1950.0\n6400.0\n4987.5\n\n\n8\n2011-09-01\n401125\n413595\n5077.531646\n4353.631579\n1620.0\n1950.0\n6390.0\n5330.0\n\n\n9\n2011-10-01\n377335\n357585\n4439.235294\n3505.735294\n2160.0\n1750.0\n6070.0\n4260.0\n\n\n10\n2011-11-01\n549345\n456740\n5282.163462\n4268.598131\n2340.0\n1950.0\n7460.0\n4370.0\n\n\n11\n2011-12-01\n276055\n197065\n5208.584906\n3284.416667\n2060.0\n1652.5\n6400.0\n3200.0"
  },
  {
    "objectID": "reference/get_pandas_frequency.html",
    "href": "reference/get_pandas_frequency.html",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "get_pandas_frequency(idx, force_regular=False)\nGet the frequency of a pandas Series or DatetimeIndex.\nThe function get_pandas_frequency takes a Pandas Series or DatetimeIndex as input and returns the inferred frequency of the index, with an option to force regular frequency.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/get_pandas_frequency.html#parameters",
    "href": "reference/get_pandas_frequency.html#parameters",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse"
  },
  {
    "objectID": "reference/get_pandas_frequency.html#returns",
    "href": "reference/get_pandas_frequency.html#returns",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/pad_by_time.html",
    "href": "reference/pad_by_time.html",
    "title": "pad_by_time",
    "section": "",
    "text": "pad_by_time(data, date_column, freq='auto', force_regular=True)\nMake irregular time series regular by padding with missing dates.\nThe pad_by_time function inserts missing dates into a Pandas DataFrame or DataFrameGroupBy object, through the process making an irregularly spaced time series regularly spaced."
  },
  {
    "objectID": "reference/pad_by_time.html#parameters",
    "href": "reference/pad_by_time.html#parameters",
    "title": "pad_by_time",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter can be either a Pandas DataFrame or a Pandas DataFrameGroupBy object. It represents the data that you want to pad with missing dates.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to determine the minimum and maximum dates in theDataFrame, and to generate the regular date range for padding.\nrequired\n\n\nfreq\nstr\nThe freq parameter specifies the frequency at which the missing timestamps should be generated. It accepts a string representing a pandas frequency alias. Automatic Frequency Detection: - \"auto\": Automatically detect the frequency of the data. This will default allow regular frequencies (i.e. no business days). This is the default value. This can be changed with the force_regular parameter. You can override this with a pandas frequency alias. Some common frequency aliases include: - S: secondly frequency - min: minute frequency - H: hourly frequency - B: business day frequency - D: daily frequency - W: weekly frequency - M: month end frequency - MS: month start frequency - BMS: Business month start - Q: quarter end frequency - QS: quarter start frequency - Y: year end frequency - YS: year start frequency\n'auto'\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean that specifies whether the frequency should be forced to be regular. This parameter is only used when the freq parameter is set to \"auto\". It has a default value of True. If force_regular is True, then the freq parameter will be forced to be a regular frequency. If force_regular is False, then the freq parameter will be allowed to be irregular (i.e. business calendars can be used).\nTrue"
  },
  {
    "objectID": "reference/pad_by_time.html#returns",
    "href": "reference/pad_by_time.html#returns",
    "title": "pad_by_time",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe function pad_by_time returns a Pandas DataFrame that has been extended with future dates."
  },
  {
    "objectID": "reference/pad_by_time.html#examples",
    "href": "reference/pad_by_time.html#examples",
    "title": "pad_by_time",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Pad Single Time Series: Fill missing dates\npadded_df = (\n    df\n        .query('symbol == \"AAPL\"')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'auto'\n        )\n        .assign(id = lambda x: x['symbol'].ffill())\n)\npadded_df \n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\nid\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\nAAPL\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\nAAPL\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\nAAPL\n\n\n3\n2013-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n4\n2013-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3910\n2023-09-17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n3911\n2023-09-18\nAAPL\n176.479996\n179.380005\n176.169998\n177.970001\n67257600.0\n177.970001\nAAPL\n\n\n3912\n2023-09-19\nAAPL\n177.520004\n179.630005\n177.130005\n179.070007\n51826900.0\n179.070007\nAAPL\n\n\n3913\n2023-09-20\nAAPL\n179.259995\n179.699997\n175.399994\n175.490005\n58436200.0\n175.490005\nAAPL\n\n\n3914\n2023-09-21\nAAPL\n174.550003\n176.300003\n173.860001\n173.929993\n63047900.0\n173.929993\nAAPL\n\n\n\n\n3915 rows × 9 columns\n\n\n\n\n# Pad Single Time Series: Fill missing dates\npadded_df = (\n    df\n        .query('symbol == \"AAPL\"')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'D'\n        )\n        .assign(id = lambda x: x['symbol'].ffill())\n)\npadded_df \n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\nid\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\nAAPL\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\nAAPL\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\nAAPL\n\n\n3\n2013-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n4\n2013-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3910\n2023-09-17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n3911\n2023-09-18\nAAPL\n176.479996\n179.380005\n176.169998\n177.970001\n67257600.0\n177.970001\nAAPL\n\n\n3912\n2023-09-19\nAAPL\n177.520004\n179.630005\n177.130005\n179.070007\n51826900.0\n179.070007\nAAPL\n\n\n3913\n2023-09-20\nAAPL\n179.259995\n179.699997\n175.399994\n175.490005\n58436200.0\n175.490005\nAAPL\n\n\n3914\n2023-09-21\nAAPL\n174.550003\n176.300003\n173.860001\n173.929993\n63047900.0\n173.929993\nAAPL\n\n\n\n\n3915 rows × 9 columns\n\n\n\n\n# Pad by Group: Pad each group with missing dates\npadded_df = (\n    df\n        .groupby('symbol')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'D'\n        )\n)\npadded_df\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\n\n\n3\n2013-01-05\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2013-01-06\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n23485\n2023-09-17\nNVDA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n23486\n2023-09-18\nNVDA\n427.480011\n442.420013\n420.000000\n439.660004\n50027100.0\n439.660004\n\n\n23487\n2023-09-19\nNVDA\n438.329987\n439.660004\n430.019989\n435.200012\n37306400.0\n435.200012\n\n\n23488\n2023-09-20\nNVDA\n436.000000\n439.029999\n422.230011\n422.390015\n36710800.0\n422.390015\n\n\n23489\n2023-09-21\nNVDA\n415.829987\n421.000000\n409.799988\n410.170013\n44893000.0\n410.170013\n\n\n\n\n23490 rows × 8 columns"
  },
  {
    "objectID": "guides/04_wrangling.html",
    "href": "guides/04_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "guides/01_visualization.html",
    "href": "guides/01_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "guides/03_pandas_frequency.html",
    "href": "guides/03_pandas_frequency.html",
    "title": "Pandas Frequencies",
    "section": "",
    "text": "How this guide benefits you\n\n\n\n\n\nThis guide covers how to use the pandas frequency strings within timetk. Once you understand key frequencies, you can apply them to manipulate time series data like a pro.\n\n\n\n\n1 Pandas Frequencies\nPandas offers a variety of frequency strings, also known as offset aliases, to define the frequency of a time series. Here are some common frequency strings used in pandas:\n\n‘B’: Business Day\n‘D’: Calendar day\n‘W’: Weekly\n‘M’: Month end\n‘BM’: Business month end\n‘MS’: Month start\n‘BMS’: Business month start\n‘Q’: Quarter end\n‘BQ’: Business quarter end\n‘QS’: Quarter start\n‘BQS’: Business quarter start\n‘A’ or ‘Y’: Year end\n‘BA’ or ‘BY’: Business year end\n‘AS’ or ‘YS’: Year start\n‘BAS’ or ‘BYS’: Business year start\n‘H’: Hourly\n‘T’ or ‘min’: Minutely\n‘S’: Secondly\n‘L’ or ‘ms’: Milliseconds\n‘U’: Microseconds\n‘N’: Nanoseconds\n\n\nCustom Frequencies:\n\nYou can also create custom frequencies by combining base frequencies, like:\n\n‘2D’: Every 2 days\n‘3W’: Every 3 weeks\n‘4H’: Every 4 hours\n‘1H30T’: Every 1 hour and 30 minutes\n\n\n\n\nCompound Frequencies:\n\nYou can combine multiple frequencies by adding them together.\n\n‘1D1H’: 1 day and 1 hour\n‘1H30T’: 1 hour and 30 minutes\n\n\n\n\nExample:\n\nimport pandas as pd\n\n# Creating a date range with daily frequency\ndate_range_daily = pd.date_range(start='2023-01-01', end='2023-01-10', freq='D')\n\ndate_range_daily\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n# Creating a date range with 2 days frequency\ndate_range_two_days = pd.date_range(start='2023-01-01', end='2023-01-10', freq='2D')\n\ndate_range_two_days\n\nDatetimeIndex(['2023-01-01', '2023-01-03', '2023-01-05', '2023-01-07',\n               '2023-01-09'],\n              dtype='datetime64[ns]', freq='2D')\n\n\nThese frequency strings help in resampling, creating date ranges, and handling time-series data efficiently in pandas.\n\n\n\n2 Timetk Incorporates Pandas Frequencies\nNow that you’ve seen pandas frequencies, you’ll see them pop up in many of the timetk functions.\n\nExample: Padding Dates\nThis example shows how to use Pandas frequencies inside of timetk functions.\nWe’ll use pad_by_time to show how to use freq to fill in missing dates.\n\n# DataFrame with missing dates\nimport pandas as pd\n\ndata = {\n    # '2023-09-05' is missing\n    'datetime': ['2023-09-01', '2023-09-02', '2023-09-03', '2023-09-04', '2023-09-06'],  \n    'value': [10, 30, 40, 50, 60]\n}\n\ndf = pd.DataFrame(data)\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01\n10\n\n\n1\n2023-09-02\n30\n\n\n2\n2023-09-03\n40\n\n\n3\n2023-09-04\n50\n\n\n4\n2023-09-06\n60\n\n\n\n\n\n\n\nWe can resample to fill in the missing day using pad_by_time with freq = 'D'.\n\nimport timetk as tk\n\ndf.pad_by_time('datetime', freq = 'D')\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01\n10.0\n\n\n1\n2023-09-02\n30.0\n\n\n2\n2023-09-03\n40.0\n\n\n3\n2023-09-04\n50.0\n\n\n4\n2023-09-05\nNaN\n\n\n5\n2023-09-06\n60.0\n\n\n\n\n\n\n\nWhat about resampling every 12 hours? Just set `freq = ‘12H’.\n\nimport timetk as tk\n\ndf.pad_by_time('datetime', freq = '12H')\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01 00:00:00\n10.0\n\n\n1\n2023-09-01 12:00:00\nNaN\n\n\n2\n2023-09-02 00:00:00\n30.0\n\n\n3\n2023-09-02 12:00:00\nNaN\n\n\n4\n2023-09-03 00:00:00\n40.0\n\n\n5\n2023-09-03 12:00:00\nNaN\n\n\n6\n2023-09-04 00:00:00\n50.0\n\n\n7\n2023-09-04 12:00:00\nNaN\n\n\n8\n2023-09-05 00:00:00\nNaN\n\n\n9\n2023-09-05 12:00:00\nNaN\n\n\n10\n2023-09-06 00:00:00\n60.0\n\n\n\n\n\n\n\nYou’ll see these pandas frequencies come up as the parameter freq in many timetk functions."
  },
  {
    "objectID": "getting-started/01_installation.html",
    "href": "getting-started/01_installation.html",
    "title": "Install",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/01_installation.html#installation",
    "href": "getting-started/01_installation.html#installation",
    "title": "Install",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npip install poetry\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "timetk for Python ",
    "section": "",
    "text": "The Time Series Toolkit for Python\nTimetk’s Mission: To make time series analysis easier, faster, and more enjoyable in Python."
  },
  {
    "objectID": "index.html#quick-start-a-monthly-sales-analysis",
    "href": "index.html#quick-start-a-monthly-sales-analysis",
    "title": "timetk for Python ",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        freq         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "timetk for Python ",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npip install poetry\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "getting-started/02_quick_start.html",
    "href": "getting-started/02_quick_start.html",
    "title": "Quick Start",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "href": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "title": "Quick Start",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        freq         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "getting-started/02_quick_start.html#more-coming-soon",
    "href": "getting-started/02_quick_start.html#more-coming-soon",
    "title": "Quick Start",
    "section": "More coming soon…",
    "text": "More coming soon…\nThere’s a lot more coming in timetk for Python. You can check out our Project Roadmap here."
  },
  {
    "objectID": "guides/02_timetk_concepts.html",
    "href": "guides/02_timetk_concepts.html",
    "title": "Timetk Basics",
    "section": "",
    "text": "Timetk has one mission: To make time series analysis simpler, easier, and faster in Python. This goal requires some opinionated ways of treating time series in Python. We will conceptually lay out how timetk can help.\nLet’s first start with how to think about time series data conceptually. Time series data has 3 core properties."
  },
  {
    "objectID": "guides/02_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "href": "guides/02_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "title": "Timetk Basics",
    "section": "2.1 Type 1: Pandas DataFrame Operations",
    "text": "2.1 Type 1: Pandas DataFrame Operations\nBefore we start using timetk, let’s make sure our data is set up properly.\n\nTimetk Data Format Compliance\n\n\n\n\n\n\n3 Core Properties Must Be Upheald\n\n\n\n\n\nA Timetk-Compliant Pandas DataFrame must have:\n\nTime Series Index: A Time Stamp column containing datetime64 values\nValue Column(s): The value column(s) containing float or int values\nGroup Column(s): Optionally for grouped time series analysis, one or more columns containg str or categorical values (shown as an object)\n\nIf these are NOT upheld, this will impact your ability to use timetk DataFrame operations.\n\n\n\n\n\n\n\n\n\nInspect the DataFrame\n\n\n\n\n\nUse Pandas info() method to check compliance.\n\n\n\nUsing pandas info() method, we can see that we have a compliant data frame with a date column containing datetime64 and a value column containing float64. For grouped analysis we have the id column containing object dtype.\n\n# Tip: Inspect for compliance with info()\nm4_daily_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9743 entries, 0 to 9742\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   id      9743 non-null   object        \n 1   date    9743 non-null   datetime64[ns]\n 2   value   9743 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 228.5+ KB\n\n\n\n\nGrouped Time Series Analysis with Summarize By Time\nFirst, inspect how the summarize_by_time function works by calling help().\n\n# Review the summarize_by_time documentation (output not shown)\nhelp(tk.summarize_by_time)\n\n\n\n\n\n\n\nHelp Doc Info: summarize_by_time()\n\n\n\n\n\n\nThe first parameter is data, indicating this is a DataFrame operation.\nThe Examples show different use cases for how to apply the function on a DataFrame\n\n\n\n\nLet’s test the summarize_by_time() DataFrame operation out using the grouped approach with method chaining. DataFrame operations can be used as Pandas methods with method-chaining, which allows us to more succinctly apply time series operations.\n\n# Grouped Summarize By Time with Method Chaining\ndf_summarized = (\n    m4_daily_df\n        .groupby('id')\n        .summarize_by_time(\n            date_column  = 'date',\n            value_column = 'value',\n            freq         = 'QS', # QS = Quarter Start\n            agg_func     = [\n                'mean', \n                'median', \n                'min',\n                ('q25', lambda x: np.quantile(x, 0.25)),\n                ('q75', lambda x: np.quantile(x, 0.75)),\n                'max',\n                ('range',lambda x: x.max() - x.min()),\n            ],\n        )\n)\n\ndf_summarized\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_q25\nvalue_q75\nvalue_max\nvalue_range\n\n\n\n\n0\nD10\n2014-07-01\n1960.078889\n1979.90\n1781.6\n1915.225\n2002.575\n2076.2\n294.6\n\n\n1\nD10\n2014-10-01\n2184.586957\n2154.05\n2022.8\n2125.075\n2274.150\n2344.9\n322.1\n\n\n2\nD10\n2015-01-01\n2309.830000\n2312.30\n2209.6\n2284.575\n2342.150\n2392.4\n182.8\n\n\n3\nD10\n2015-04-01\n2344.481319\n2333.00\n2185.1\n2301.750\n2391.000\n2499.8\n314.7\n\n\n4\nD10\n2015-07-01\n2156.754348\n2186.70\n1856.6\n1997.250\n2289.425\n2368.1\n511.5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n105\nD500\n2011-07-01\n9727.321739\n9745.55\n8964.5\n9534.125\n10003.900\n10463.9\n1499.4\n\n\n106\nD500\n2011-10-01\n8175.565217\n7897.00\n6755.0\n7669.875\n8592.575\n9860.0\n3105.0\n\n\n107\nD500\n2012-01-01\n8291.317582\n8412.60\n7471.5\n7814.800\n8677.850\n8980.7\n1509.2\n\n\n108\nD500\n2012-04-01\n8654.020879\n8471.10\n8245.6\n8389.850\n9017.250\n9349.2\n1103.6\n\n\n109\nD500\n2012-07-01\n8770.502353\n8690.50\n8348.1\n8604.400\n8846.000\n9545.3\n1197.2\n\n\n\n\n110 rows × 9 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: summarize_by_time()\n\n\n\n\n\n\nThe data must comply with the 3 core properties (date column, value column(s), and group column(s))\nThe aggregation functions were applied by combination of group (id) and resample (Quarter Start)\nThe result was a pandas DataFrame with group column, resampled date column, and summary values (mean, median, min, 25th-quantile, etc)\n\n\n\n\n\n\nAnother DataFrame Example: Creating 29 Engineered Features\nLet’s examine another DataFrame function, tk.augment_timeseries_signature(). Feel free to inspect the documentation with help(tk.augment_timeseries_signature).\n\n# Creating 29 engineered features from the date column\n# Not run: help(tk.augment_timeseries_signature)\ndf_augmented = (\n    m4_daily_df\n        .augment_timeseries_signature(date_column = 'date')\n)\n\ndf_augmented.head()\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: augment_timeseries_signature()\n\n\n\n\n\n\nThe data must comply with the 1 of the 3 core properties (date column)\nThe result was a pandas DataFrame with 29 time series features that can be used for Machine Learning and Forecasting\n\n\n\n\n\n\nMaking Future Dates with Future Frame\nA common time series task before forecasting with machine learning models is to make a future DataFrame some length_out into the future. You can do this with tk.future_frame(). Here’s how.\n\n# Preparing a time series data set for Machine Learning Forecasting\nfull_augmented_df = (\n    m4_daily_df \n        .groupby('id')\n        .future_frame('date', length_out = 365)\n        .augment_timeseries_signature('date')\n)\nfull_augmented_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4556\nD500\n2013-09-19\nNaN\n1379548800\n2013\n2013\n0\n0\n0\n2\n...\n19\n81\n262\n0\n0\n0\n0\n0\n0\nam\n\n\n4557\nD500\n2013-09-20\nNaN\n1379635200\n2013\n2013\n0\n0\n0\n2\n...\n20\n82\n263\n0\n0\n0\n0\n0\n0\nam\n\n\n4558\nD500\n2013-09-21\nNaN\n1379721600\n2013\n2013\n0\n0\n0\n2\n...\n21\n83\n264\n0\n0\n0\n0\n0\n0\nam\n\n\n4559\nD500\n2013-09-22\nNaN\n1379808000\n2013\n2013\n0\n0\n0\n2\n...\n22\n84\n265\n1\n0\n0\n0\n0\n0\nam\n\n\n4560\nD500\n2013-09-23\nNaN\n1379894400\n2013\n2013\n0\n0\n0\n2\n...\n23\n85\n266\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n11203 rows × 32 columns\n\n\n\nWe can then get the future data by keying in on the data with value column that is missing (np.nan).\n\n# Get the future data (just the observations that haven't happened yet)\nfuture_df = (\n    full_augmented_df\n        .query('value.isna()')\n)\nfuture_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n674\nD10\n2016-05-07\nNaN\n1462579200\n2016\n2016\n0\n0\n1\n1\n...\n7\n37\n128\n0\n0\n0\n0\n0\n0\nam\n\n\n675\nD10\n2016-05-08\nNaN\n1462665600\n2016\n2016\n0\n0\n1\n1\n...\n8\n38\n129\n1\n0\n0\n0\n0\n0\nam\n\n\n676\nD10\n2016-05-09\nNaN\n1462752000\n2016\n2016\n0\n0\n1\n1\n...\n9\n39\n130\n0\n0\n0\n0\n0\n0\nam\n\n\n677\nD10\n2016-05-10\nNaN\n1462838400\n2016\n2016\n0\n0\n1\n1\n...\n10\n40\n131\n0\n0\n0\n0\n0\n0\nam\n\n\n678\nD10\n2016-05-11\nNaN\n1462924800\n2016\n2016\n0\n0\n1\n1\n...\n11\n41\n132\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4556\nD500\n2013-09-19\nNaN\n1379548800\n2013\n2013\n0\n0\n0\n2\n...\n19\n81\n262\n0\n0\n0\n0\n0\n0\nam\n\n\n4557\nD500\n2013-09-20\nNaN\n1379635200\n2013\n2013\n0\n0\n0\n2\n...\n20\n82\n263\n0\n0\n0\n0\n0\n0\nam\n\n\n4558\nD500\n2013-09-21\nNaN\n1379721600\n2013\n2013\n0\n0\n0\n2\n...\n21\n83\n264\n0\n0\n0\n0\n0\n0\nam\n\n\n4559\nD500\n2013-09-22\nNaN\n1379808000\n2013\n2013\n0\n0\n0\n2\n...\n22\n84\n265\n1\n0\n0\n0\n0\n0\nam\n\n\n4560\nD500\n2013-09-23\nNaN\n1379894400\n2013\n2013\n0\n0\n0\n2\n...\n23\n85\n266\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n1460 rows × 32 columns"
  },
  {
    "objectID": "guides/02_timetk_concepts.html#type-2-pandas-series-operations",
    "href": "guides/02_timetk_concepts.html#type-2-pandas-series-operations",
    "title": "Timetk Basics",
    "section": "2.2 Type 2: Pandas Series Operations",
    "text": "2.2 Type 2: Pandas Series Operations\nThe main difference between a DataFrame operation and a Series operation is that we are operating on an array of values from typically one of the following dtypes:\n\nTimestamps (datetime64)\nNumeric (float64 or int64)\n\nThe first argument of Series operations that operate on Timestamps will always be idx.\nLet’s take a look at one shall we? We’ll start with a common action: Making future time series from an existing time series with a regular frequency.\n\nThe Make Future Time Series Function\nSay we have a monthly sequence of timestamps. What if we want to create a forecast where we predict 12 months into the future? Well, we will need to create 12 future timestamps. Here’s how.\nFirst create a pd.date_range() with dates starting at the beginning of each month.\n\n# Make a monthly date range\ndates_dt = pd.date_range(\"2023-01\", \"2024-01\", freq=\"MS\")\ndates_dt\n\nDatetimeIndex(['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01',\n               '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01',\n               '2023-09-01', '2023-10-01', '2023-11-01', '2023-12-01',\n               '2024-01-01'],\n              dtype='datetime64[ns]', freq='MS')\n\n\nNext, use tk.make_future_timeseries() to create the next 12 timestamps in the sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas Series: Future Dates\nfuture_series = pd.Series(dates_dt).make_future_timeseries(12)\nfuture_series\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\n# DateTimeIndex: Future Dates\nfuture_dt = tk.make_future_timeseries(\n    idx      = dates_dt,\n    length_out = 12\n)\nfuture_dt\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\nWe can combine the actual and future timestamps into one combined timeseries.\n\n# Combining the 2 series and resetting the index\ncombined_timeseries = (\n    pd.concat(\n        [pd.Series(dates_dt), pd.Series(future_dt)],\n        axis=0\n    )\n        .reset_index(drop = True)\n)\n\ncombined_timeseries\n\n0    2023-01-01\n1    2023-02-01\n2    2023-03-01\n3    2023-04-01\n4    2023-05-01\n5    2023-06-01\n6    2023-07-01\n7    2023-08-01\n8    2023-09-01\n9    2023-10-01\n10   2023-11-01\n11   2023-12-01\n12   2024-01-01\n13   2024-02-01\n14   2024-03-01\n15   2024-04-01\n16   2024-05-01\n17   2024-06-01\n18   2024-07-01\n19   2024-08-01\n20   2024-09-01\n21   2024-10-01\n22   2024-11-01\n23   2024-12-01\n24   2025-01-01\ndtype: datetime64[ns]\n\n\nNext, we’ll take a look at how to go from an irregular time series to a regular time series.\n\n\nFlooring Dates\nAn example is tk.floor_date, which is used to round down dates. See help(tk.floor_date).\nFlooring dates is often used as part of a strategy to go from an irregular time series to regular by combining with an aggregation. Often summarize_by_time() is used (I’ll share why shortly). But conceptually, date flooring is the secret.\n\nWith FlooringWithout Flooring\n\n\n\n# Monthly flooring rounds dates down to 1st of the month\nm4_daily_df['date'].floor_date(unit = \"M\")\n\n0      2014-07-01\n1      2014-07-01\n2      2014-07-01\n3      2014-07-01\n4      2014-07-01\n          ...    \n9738   2012-09-01\n9739   2012-09-01\n9740   2012-09-01\n9741   2012-09-01\n9742   2012-09-01\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\n# Before Flooring\nm4_daily_df['date']\n\n0      2014-07-03\n1      2014-07-04\n2      2014-07-05\n3      2014-07-06\n4      2014-07-07\n          ...    \n9738   2012-09-19\n9739   2012-09-20\n9740   2012-09-21\n9741   2012-09-22\n9742   2012-09-23\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\nThis “date flooring” operation can be useful for creating date groupings.\n\n# Adding a date group with floor_date()\ndates_grouped_by_month = (\n    m4_daily_df\n        .assign(date_group = lambda x: x['date'].floor_date(\"M\"))\n)\n\ndates_grouped_by_month\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_group\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2014-07-01\n\n\n1\nD10\n2014-07-04\n2073.4\n2014-07-01\n\n\n2\nD10\n2014-07-05\n2048.7\n2014-07-01\n\n\n3\nD10\n2014-07-06\n2048.9\n2014-07-01\n\n\n4\nD10\n2014-07-07\n2006.4\n2014-07-01\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n2012-09-01\n\n\n9739\nD500\n2012-09-20\n9365.7\n2012-09-01\n\n\n9740\nD500\n2012-09-21\n9445.9\n2012-09-01\n\n\n9741\nD500\n2012-09-22\n9497.9\n2012-09-01\n\n\n9742\nD500\n2012-09-23\n9545.3\n2012-09-01\n\n\n\n\n9743 rows × 4 columns\n\n\n\nWe can then do grouped operations.\n\n# Example of a grouped operation with floored dates\nsummary_df = (\n    dates_grouped_by_month\n        .drop('date', axis=1) \\\n        .groupby(['id', 'date_group'])\n        .mean() \\\n        .reset_index()\n)\n\nsummary_df\n\n\n\n\n\n\n\n\nid\ndate_group\nvalue\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n\n\n1\nD10\n2014-08-01\n1985.548387\n\n\n2\nD10\n2014-09-01\n1926.593333\n\n\n3\nD10\n2014-10-01\n2100.077419\n\n\n4\nD10\n2014-11-01\n2155.326667\n\n\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n\n\n319\nD500\n2012-06-01\n9124.903333\n\n\n320\nD500\n2012-07-01\n8674.551613\n\n\n321\nD500\n2012-08-01\n8666.054839\n\n\n322\nD500\n2012-09-01\n9040.604348\n\n\n\n\n323 rows × 3 columns\n\n\n\nOf course for this operation, we can do it faster with summarize_by_time() (and it’s much more flexible).\n\n# Summarize by time is less code and more flexible\n(\n    m4_daily_df \n        .groupby('id')\n        .summarize_by_time(\n            'date', 'value', \n            freq = \"MS\",\n            agg_func = ['mean', 'median', 'min', 'max']\n        )\n)\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_max\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n1978.80\n1876.0\n2076.2\n\n\n1\nD10\n2014-08-01\n1985.548387\n1995.60\n1914.7\n2027.5\n\n\n2\nD10\n2014-09-01\n1926.593333\n1920.95\n1781.6\n2023.5\n\n\n3\nD10\n2014-10-01\n2100.077419\n2107.60\n2022.8\n2154.9\n\n\n4\nD10\n2014-11-01\n2155.326667\n2149.30\n2083.5\n2245.4\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n8430.80\n8245.6\n8578.1\n\n\n319\nD500\n2012-06-01\n9124.903333\n9163.85\n8686.1\n9349.2\n\n\n320\nD500\n2012-07-01\n8674.551613\n8673.60\n8407.5\n9091.1\n\n\n321\nD500\n2012-08-01\n8666.054839\n8667.40\n8348.1\n8939.6\n\n\n322\nD500\n2012-09-01\n9040.604348\n9091.40\n8500.0\n9545.3\n\n\n\n\n323 rows × 6 columns\n\n\n\nAnd that’s the core idea behind timetk, writing less code and getting more.\nNext, let’s do one more function. The brother of augment_timeseries_signature()…\n\n\nThe Get Time Series Signature Function\nThis function takes a pandas Series or DateTimeIndex and returns a DataFrame containing the 29 engineered features.\nStart with either a DateTimeIndex…\n\ntimestamps_dt = pd.date_range(\"2023\", \"2024\", freq = \"D\")\ntimestamps_dt\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10',\n               ...\n               '2023-12-23', '2023-12-24', '2023-12-25', '2023-12-26',\n               '2023-12-27', '2023-12-28', '2023-12-29', '2023-12-30',\n               '2023-12-31', '2024-01-01'],\n              dtype='datetime64[ns]', length=366, freq='D')\n\n\n… Or a Pandas Series.\n\ntimestamps_series = pd.Series(timestamps_dt)\ntimestamps_series\n\n0     2023-01-01\n1     2023-01-02\n2     2023-01-03\n3     2023-01-04\n4     2023-01-05\n         ...    \n361   2023-12-28\n362   2023-12-29\n363   2023-12-30\n364   2023-12-31\n365   2024-01-01\nLength: 366, dtype: datetime64[ns]\n\n\nAnd you can use the pandas Series function, tk.get_timeseries_signature() to create 29 features from the date sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas series: get_timeseries_signature\ntimestamps_series.get_timeseries_signature()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns\n\n\n\n\n\n\n# DateTimeIndex: get_timeseries_signature\ntk.get_timeseries_signature(timestamps_dt)\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns"
  },
  {
    "objectID": "guides/05_augmenting.html",
    "href": "guides/05_augmenting.html",
    "title": "Adding Features (Augmenting)",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "reference/get_available_datasets.html",
    "href": "reference/get_available_datasets.html",
    "title": "get_available_datasets",
    "section": "",
    "text": "datasets.get_datasets.get_available_datasets()\nGet a list of 12 datasets that can be loaded with timetk.load_dataset.\nThe get_available_datasets function returns a sorted list of available dataset names from the timetk.datasets module. The available datasets are:"
  },
  {
    "objectID": "reference/get_available_datasets.html#returns",
    "href": "reference/get_available_datasets.html#returns",
    "title": "get_available_datasets",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nThe function get_available_datasets returns a sorted list of available dataset names from the timetk.datasets module."
  },
  {
    "objectID": "reference/get_available_datasets.html#examples",
    "href": "reference/get_available_datasets.html#examples",
    "title": "get_available_datasets",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\n\ntk.get_available_datasets()\n\n['bike_sales_sample',\n 'bike_sharing_daily',\n 'm4_daily',\n 'm4_hourly',\n 'm4_monthly',\n 'm4_quarterly',\n 'm4_weekly',\n 'm4_yearly',\n 'stocks_daily',\n 'taylor_30_min',\n 'walmart_sales_weekly',\n 'wikipedia_traffic_daily']"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Bend time series data to your will.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\npad_by_time\nMake irregular time series regular by padding with missing dates.\n\n\nfuture_frame\nExtend a DataFrame or GroupBy object with future dates.\n\n\n\n\n\n\nAdd one or more feature columns to time series data.\n\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame.\n\n\naugment_lags\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\n\n\n\n\n\n\nTime series functions that manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nget_pandas_frequency\nGet the frequency of a pandas Series or DatetimeIndex.\n\n\n\n\n\n\nHelper functions to make your life easier.\n\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month.\n\n\n\n\n\n\nPractice timetk with 12 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 12 datasets that can be loaded with timetk.load_dataset.\n\n\nload_dataset\nLoad one of 12 Time Series Datasets."
  },
  {
    "objectID": "reference/index.html#wrangling-pandas-time-series-dataframes",
    "href": "reference/index.html#wrangling-pandas-time-series-dataframes",
    "title": "Function reference",
    "section": "",
    "text": "Bend time series data to your will.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\npad_by_time\nMake irregular time series regular by padding with missing dates.\n\n\nfuture_frame\nExtend a DataFrame or GroupBy object with future dates."
  },
  {
    "objectID": "reference/index.html#adding-features-to-time-series-dataframes-augmenting",
    "href": "reference/index.html#adding-features-to-time-series-dataframes-augmenting",
    "title": "Function reference",
    "section": "",
    "text": "Add one or more feature columns to time series data.\n\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame.\n\n\naugment_lags\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object."
  },
  {
    "objectID": "reference/index.html#time-series-for-pandas-series",
    "href": "reference/index.html#time-series-for-pandas-series",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nget_pandas_frequency\nGet the frequency of a pandas Series or DatetimeIndex."
  },
  {
    "objectID": "reference/index.html#utilities",
    "href": "reference/index.html#utilities",
    "title": "Function reference",
    "section": "",
    "text": "Helper functions to make your life easier.\n\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month."
  },
  {
    "objectID": "reference/index.html#datasets",
    "href": "reference/index.html#datasets",
    "title": "Function reference",
    "section": "",
    "text": "Practice timetk with 12 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 12 datasets that can be loaded with timetk.load_dataset.\n\n\nload_dataset\nLoad one of 12 Time Series Datasets."
  },
  {
    "objectID": "reference/week_of_month.html",
    "href": "reference/week_of_month.html",
    "title": "week_of_month",
    "section": "",
    "text": "week_of_month(idx)\nThe “week_of_month” function calculates the week number of a given date within its month."
  },
  {
    "objectID": "reference/week_of_month.html#parameters",
    "href": "reference/week_of_month.html#parameters",
    "title": "week_of_month",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe parameter “idx” is a pandas Series object that represents a specific date for which you want to determine the week of the month.\nrequired"
  },
  {
    "objectID": "reference/week_of_month.html#returns",
    "href": "reference/week_of_month.html#returns",
    "title": "week_of_month",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe week of the month for a given date."
  },
  {
    "objectID": "reference/week_of_month.html#examples",
    "href": "reference/week_of_month.html#examples",
    "title": "week_of_month",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"1D\")\ndates\n\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10', '2020-01-11', '2020-01-12',\n               '2020-01-13', '2020-01-14', '2020-01-15', '2020-01-16',\n               '2020-01-17', '2020-01-18', '2020-01-19', '2020-01-20',\n               '2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24',\n               '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28',\n               '2020-01-29', '2020-01-30', '2020-01-31', '2020-02-01',\n               '2020-02-02', '2020-02-03', '2020-02-04', '2020-02-05',\n               '2020-02-06', '2020-02-07', '2020-02-08', '2020-02-09',\n               '2020-02-10', '2020-02-11', '2020-02-12', '2020-02-13',\n               '2020-02-14', '2020-02-15', '2020-02-16', '2020-02-17',\n               '2020-02-18', '2020-02-19', '2020-02-20', '2020-02-21',\n               '2020-02-22', '2020-02-23', '2020-02-24', '2020-02-25',\n               '2020-02-26', '2020-02-27', '2020-02-28'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n# Works on DateTimeIndex\ntk.week_of_month(dates)\n\n0     1\n1     1\n2     1\n3     1\n4     1\n5     1\n6     1\n7     2\n8     2\n9     2\n10    2\n11    2\n12    2\n13    2\n14    3\n15    3\n16    3\n17    3\n18    3\n19    3\n20    3\n21    4\n22    4\n23    4\n24    4\n25    4\n26    4\n27    4\n28    5\n29    5\n30    5\n31    1\n32    1\n33    1\n34    1\n35    1\n36    1\n37    1\n38    2\n39    2\n40    2\n41    2\n42    2\n43    2\n44    2\n45    3\n46    3\n47    3\n48    3\n49    3\n50    3\n51    3\n52    4\n53    4\n54    4\n55    4\n56    4\n57    4\n58    4\nName: week_of_month, dtype: int32\n\n\n\n# Works on Pandas Series\ndates.to_series().week_of_month()\n\n2020-01-01    1\n2020-01-02    1\n2020-01-03    1\n2020-01-04    1\n2020-01-05    1\n2020-01-06    1\n2020-01-07    1\n2020-01-08    2\n2020-01-09    2\n2020-01-10    2\n2020-01-11    2\n2020-01-12    2\n2020-01-13    2\n2020-01-14    2\n2020-01-15    3\n2020-01-16    3\n2020-01-17    3\n2020-01-18    3\n2020-01-19    3\n2020-01-20    3\n2020-01-21    3\n2020-01-22    4\n2020-01-23    4\n2020-01-24    4\n2020-01-25    4\n2020-01-26    4\n2020-01-27    4\n2020-01-28    4\n2020-01-29    5\n2020-01-30    5\n2020-01-31    5\n2020-02-01    1\n2020-02-02    1\n2020-02-03    1\n2020-02-04    1\n2020-02-05    1\n2020-02-06    1\n2020-02-07    1\n2020-02-08    2\n2020-02-09    2\n2020-02-10    2\n2020-02-11    2\n2020-02-12    2\n2020-02-13    2\n2020-02-14    2\n2020-02-15    3\n2020-02-16    3\n2020-02-17    3\n2020-02-18    3\n2020-02-19    3\n2020-02-20    3\n2020-02-21    3\n2020-02-22    4\n2020-02-23    4\n2020-02-24    4\n2020-02-25    4\n2020-02-26    4\n2020-02-27    4\n2020-02-28    4\nFreq: D, Name: week_of_month, dtype: int32"
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "datasets.get_datasets.load_dataset(name='m4_daily', verbose=False, **kwargs)\nLoad one of 12 Time Series Datasets.\nThe load_dataset function is used to load various time series datasets by name, with options to print the available datasets and pass additional arguments to pandas.read_csv. The available datasets are:\nThe datasets can be loaded with timetk.load_dataset(name), where name is the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned above."
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name parameter is used to specify the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned in the function’s docstring.\n'm4_daily'\n\n\nverbose\nbool\nThe verbose parameter is a boolean flag that determines whether or not to print the names of the available datasets. If verbose is set to True, the function will print the names of the available datasets. If verbose is set to False, the function will not print anything.\nFalse\n\n\n**kwargs\n\nThe **kwargs parameter is used to pass additional arguments to pandas.read_csv.\n{}"
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe load_dataset function returns the requested dataset as a pandas DataFrame."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\n\n# Stocks Daily Dataset: META, APPL, AMZN, NFLX, NVDA, GOOG\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Bike Sales CRM Sample Dataset\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Taylor 30-Minute Power Demand Dataset\ndf = tk.load_dataset('taylor_30_min', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2000-06-05 00:00:00+00:00\n22262\n\n\n1\n2000-06-05 00:30:00+00:00\n21756\n\n\n2\n2000-06-05 01:00:00+00:00\n22247\n\n\n3\n2000-06-05 01:30:00+00:00\n22759\n\n\n4\n2000-06-05 02:00:00+00:00\n22549\n\n\n...\n...\n...\n\n\n4027\n2000-08-27 21:30:00+00:00\n27946\n\n\n4028\n2000-08-27 22:00:00+00:00\n27133\n\n\n4029\n2000-08-27 22:30:00+00:00\n25996\n\n\n4030\n2000-08-27 23:00:00+00:00\n24610\n\n\n4031\n2000-08-27 23:30:00+00:00\n23132\n\n\n\n\n4032 rows × 2 columns"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html",
    "href": "reference/augment_timeseries_signature.html",
    "title": "augment_timeseries_signature",
    "section": "",
    "text": "augment_timeseries_signature(data, date_column)\nAdd 29 time series features to a DataFrame.\nThe function augment_timeseries_signature takes a DataFrame and a date column as input and returns the original DataFrame with the 29 different date and time based features added as new columns:"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#parameters",
    "href": "reference/augment_timeseries_signature.html#parameters",
    "title": "augment_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe data parameter is a pandas DataFrame that contains the time series data.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that represents the name of the date column in the data DataFrame.\nrequired"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#returns",
    "href": "reference/augment_timeseries_signature.html#returns",
    "title": "augment_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA pandas DataFrame that is the concatenation of the original data DataFrame and the ts_signature_df DataFrame."
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#examples",
    "href": "reference/augment_timeseries_signature.html#examples",
    "title": "augment_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\n# Adds 29 new time series features as columns to the original DataFrame\n( \n    tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n        .augment_timeseries_signature(date_column = 'order_date')\n        .head()\n)\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\norder_date_leapyear\norder_date_half\norder_date_quarter\norder_date_quarteryear\norder_date_quarterstart\norder_date_quarterend\norder_date_month\norder_date_month_lbl\norder_date_monthstart\norder_date_monthend\norder_date_yweek\norder_date_mweek\norder_date_wday\norder_date_wday_lbl\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/augment_lags.html",
    "href": "reference/augment_lags.html",
    "title": "augment_lags",
    "section": "",
    "text": "augment_lags(data, date_column, value_column, lags=1)\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\nThe augment_lags function takes a Pandas DataFrame or GroupBy object, a date column, a value column or list of value columns, and a lag or list of lags, and adds lagged versions of the value columns to the DataFrame."
  },
  {
    "objectID": "reference/augment_lags.html#parameters",
    "href": "reference/augment_lags.html#parameters",
    "title": "augment_lags",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to add lagged columns to.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to sort the data before adding the lagged values.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the column(s) in the DataFrame that you want to add lagged values for. It can be either a single column name (string) or a list of column names.\nrequired\n\n\nlags\nint or list\nThe lags parameter is an integer or a list of integers that specifies the number of lagged values to add to the dataframe. If it is an integer, the function will add that number of lagged values for each column specified in the value_column parameter. If it is a list\n1"
  },
  {
    "objectID": "reference/augment_lags.html#returns",
    "href": "reference/augment_lags.html#returns",
    "title": "augment_lags",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame with lagged columns added to it."
  },
  {
    "objectID": "reference/augment_lags.html#examples",
    "href": "reference/augment_lags.html#examples",
    "title": "augment_lags",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndf = tk.load_dataset('m4_daily', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n\n\n1\nD10\n2014-07-04\n2073.4\n\n\n2\nD10\n2014-07-05\n2048.7\n\n\n3\nD10\n2014-07-06\n2048.9\n\n\n4\nD10\n2014-07-07\n2006.4\n\n\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n\n\n9739\nD500\n2012-09-20\n9365.7\n\n\n9740\nD500\n2012-09-21\n9445.9\n\n\n9741\nD500\n2012-09-22\n9497.9\n\n\n9742\nD500\n2012-09-23\n9545.3\n\n\n\n\n9743 rows × 3 columns\n\n\n\n\n# Add 7 lagged values for each grouped time series\nlagged_df = (\n    df \n        .groupby('id')\n        .augment_lags(\n            date_column  = 'date',\n            value_column = 'value',\n            lags         = range(1, 8)\n        )\n)\n\nlagged_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_1\nvalue_lag_2\nvalue_lag_3\nvalue_lag_4\nvalue_lag_5\nvalue_lag_6\nvalue_lag_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n9286.9\n9265.4\n9091.4\n\n\n9739\nD500\n2012-09-20\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n9286.9\n9265.4\n\n\n9740\nD500\n2012-09-21\n9445.9\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n9286.9\n\n\n9741\nD500\n2012-09-22\n9497.9\n9445.9\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n9359.2\n\n\n9742\nD500\n2012-09-23\n9545.3\n9497.9\n9445.9\n9365.7\n9418.8\n9431.9\n9437.7\n9474.6\n\n\n\n\n9743 rows × 10 columns\n\n\n\n\n# Add 7 lagged values for a single time series\n(\n    df \n        .query('id == \"D10\"')\n        .augment_lags(\n            date_column  = 'date',\n            value_column = 'value',\n            lags         = range(1, 8)\n        )\n)\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_1\nvalue_lag_2\nvalue_lag_3\nvalue_lag_4\nvalue_lag_5\nvalue_lag_6\nvalue_lag_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n2542.0\n2534.2\n\n\n670\nD10\n2016-05-03\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n2542.0\n\n\n671\nD10\n2016-05-04\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n\n\n672\nD10\n2016-05-05\n2622.5\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n\n\n673\nD10\n2016-05-06\n2620.1\n2622.5\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n\n\n\n\n674 rows × 10 columns"
  }
]