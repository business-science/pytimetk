[
  {
    "objectID": "reference/ts_features.html",
    "href": "reference/ts_features.html",
    "title": "ts_features",
    "section": "",
    "text": "ts_features(data, date_column, value_column, features=None, freq=None, scale=True, threads=None)\nExtracts aggregated time series features from a DataFrame or DataFrameGroupBy object using the tsfeatures package.\nNote: Requires the tsfeatures package to be installed.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input data that can be either a Pandas DataFrame or a grouped DataFrame. It contains the time series data that you want to extract features from.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is the name of the column in the input data that contains the dates or timestamps of the time series data.\nrequired\n\n\nvalue_column\nstr\nThe value_column parameter is the name of the column in the DataFrame that contains the time series values.\nrequired\n\n\nfeatures\nlist\nThe features parameter is a list of functions that represent the time series features to be extracted. Each function should take a time series as input and return a scalar value as output. When None, uses the default list of features: - acf_features - arch_stat - crossing_points - entropy - flat_spots - heterogeneity - holt_parameters - lumpiness - nonlinearity - pacf_features - stl_features - stability - hw_parameters - unitroot_kpss - unitroot_pp - series_length - hurst\nNone\n\n\nfreq\nstr\nThe freq parameter specifies the frequency of the time series data. It is used to calculate features that are dependent on the frequency, such as seasonal features. - The frequency can be specified as a string, such as ‘D’ for daily, ‘W’ for weekly, ‘M’ for monthly. - The frequency can be a numeric value representing the number of observations per year, such as 365 for daily, 52 for weekly, 12 for monthly.\nNone\n\n\nscale\nbool\nThe scale parameter in the ts_features function determines whether or not to scale the extracted features. - If scale is set to True, the features will be scaled using z-score normalization. - If scale is set to False, the features will not be scaled.\nTrue\n\n\nthreads\nOptional[int]\nThe threads parameter is an optional parameter that specifies the number of threads to use for parallel processing. If not specified, the function will use the default number of threads available on the system.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function ts_features returns a pandas DataFrame containing the extracted time series features.\n\n\n\n\n\n\n\n# REQUIRES tsfeatures: pip install tsfeatures\nimport pandas as pd\nimport timetk as tk\n\n# tsfeatures comes with these features:\nfrom tsfeatures import (\n    acf_features, arch_stat, crossing_points,\n    entropy, flat_spots, heterogeneity,\n    holt_parameters, lumpiness, nonlinearity,\n    pacf_features, stl_features, stability,\n    hw_parameters, unitroot_kpss, unitroot_pp,\n    series_length, hurst\n)\n\ndf = tk.load_dataset('m4_daily', parse_dates = ['date'])\ndf\n# Feature Extraction\nfeature_df = (\n    df\n        .groupby('id')\n        .ts_features(    \n            date_column  = 'date', \n            value_column = 'value',\n            features     = [acf_features, hurst],\n            freq         = 7\n        )\n) \nfeature_df"
  },
  {
    "objectID": "reference/ts_features.html#parameters",
    "href": "reference/ts_features.html#parameters",
    "title": "ts_features",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input data that can be either a Pandas DataFrame or a grouped DataFrame. It contains the time series data that you want to extract features from.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is the name of the column in the input data that contains the dates or timestamps of the time series data.\nrequired\n\n\nvalue_column\nstr\nThe value_column parameter is the name of the column in the DataFrame that contains the time series values.\nrequired\n\n\nfeatures\nlist\nThe features parameter is a list of functions that represent the time series features to be extracted. Each function should take a time series as input and return a scalar value as output. When None, uses the default list of features: - acf_features - arch_stat - crossing_points - entropy - flat_spots - heterogeneity - holt_parameters - lumpiness - nonlinearity - pacf_features - stl_features - stability - hw_parameters - unitroot_kpss - unitroot_pp - series_length - hurst\nNone\n\n\nfreq\nstr\nThe freq parameter specifies the frequency of the time series data. It is used to calculate features that are dependent on the frequency, such as seasonal features. - The frequency can be specified as a string, such as ‘D’ for daily, ‘W’ for weekly, ‘M’ for monthly. - The frequency can be a numeric value representing the number of observations per year, such as 365 for daily, 52 for weekly, 12 for monthly.\nNone\n\n\nscale\nbool\nThe scale parameter in the ts_features function determines whether or not to scale the extracted features. - If scale is set to True, the features will be scaled using z-score normalization. - If scale is set to False, the features will not be scaled.\nTrue\n\n\nthreads\nOptional[int]\nThe threads parameter is an optional parameter that specifies the number of threads to use for parallel processing. If not specified, the function will use the default number of threads available on the system.\nNone"
  },
  {
    "objectID": "reference/ts_features.html#returns",
    "href": "reference/ts_features.html#returns",
    "title": "ts_features",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nThe function ts_features returns a pandas DataFrame containing the extracted time series features."
  },
  {
    "objectID": "reference/ts_features.html#examples",
    "href": "reference/ts_features.html#examples",
    "title": "ts_features",
    "section": "",
    "text": "# REQUIRES tsfeatures: pip install tsfeatures\nimport pandas as pd\nimport timetk as tk\n\n# tsfeatures comes with these features:\nfrom tsfeatures import (\n    acf_features, arch_stat, crossing_points,\n    entropy, flat_spots, heterogeneity,\n    holt_parameters, lumpiness, nonlinearity,\n    pacf_features, stl_features, stability,\n    hw_parameters, unitroot_kpss, unitroot_pp,\n    series_length, hurst\n)\n\ndf = tk.load_dataset('m4_daily', parse_dates = ['date'])\ndf\n# Feature Extraction\nfeature_df = (\n    df\n        .groupby('id')\n        .ts_features(    \n            date_column  = 'date', \n            value_column = 'value',\n            features     = [acf_features, hurst],\n            freq         = 7\n        )\n) \nfeature_df"
  },
  {
    "objectID": "reference/augment_holiday_signature.html",
    "href": "reference/augment_holiday_signature.html",
    "title": "augment_holiday_signature",
    "section": "",
    "text": "augment_holiday_signature(data, date_column, country_name='UnitedStates')\nEngineers 4 different holiday features from a single datetime for 80+ countries.\nNote: Requires the holidays package to be installed. See https://pypi.org/project/holidays/ for more information."
  },
  {
    "objectID": "reference/augment_holiday_signature.html#parameters",
    "href": "reference/augment_holiday_signature.html#parameters",
    "title": "augment_holiday_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe input DataFrame.\nrequired\n\n\ndate_column\nstr\nThe name of the datetime-like column in the DataFrame.\nrequired\n\n\ncountry_name\nstr\nThe name of the country for which to generate holiday features. Defaults to United States holidays, but the following countries are currently available and accessible by the full name or ISO code: Any of the following are acceptable keys for country_name: Available Countries: Full Country, Abrv. #1, #2, #3 Angola: Angola, AO, AGO, Argentina: Argentina, AR, ARG, Aruba: Aruba, AW, ABW, Australia: Australia, AU, AUS, Austria: Austria, AT, AUT, Bangladesh: Bangladesh, BD, BGD, Belarus: Belarus, BY, BLR, Belgium: Belgium, BE, BEL, Botswana: Botswana, BW, BWA, Brazil: Brazil, BR, BRA, Bulgaria: Bulgaria, BG, BLG, Burundi: Burundi, BI, BDI, Canada: Canada, CA, CAN, Chile: Chile, CL, CHL, Colombia: Colombia, CO, COL, Croatia: Croatia, HR, HRV, Curacao: Curacao, CW, CUW, Czechia: Czechia, CZ, CZE, Denmark: Denmark, DK, DNK, Djibouti: Djibouti, DJ, DJI, Dominican Republic: DominicanRepublic, DO, DOM, Egypt: Egypt, EG, EGY, England: England, Estonia: Estonia, EE, EST, European Central Bank: EuropeanCentralBank, Finland: Finland, FI, FIN, France: France, FR, FRA, Georgia: Georgia, GE, GEO, Germany: Germany, DE, DEU, Greece: Greece, GR, GRC, Honduras: Honduras, HN, HND, Hong Kong: HongKong, HK, HKG, Hungary: Hungary, HU, HUN, Iceland: Iceland, IS, ISL, India: India, IN, IND, Ireland: Ireland, IE, IRL, Isle Of Man: IsleOfMan, Israel: Israel, IL, ISR, Italy: Italy, IT, ITA, Jamaica: Jamaica, JM, JAM, Japan: Japan, JP, JPN, Kenya: Kenya, KE, KEN, Korea: Korea, KR, KOR, Latvia: Latvia, LV, LVA, Lithuania: Lithuania, LT, LTU, Luxembourg: Luxembourg, LU, LUX, Malaysia: Malaysia, MY, MYS, Malawi: Malawi, MW, MWI, Mexico: Mexico, MX, MEX, Morocco: Morocco, MA, MOR, Mozambique: Mozambique, MZ, MOZ, Netherlands: Netherlands, NL, NLD, NewZealand: NewZealand, NZ, NZL, Nicaragua: Nicaragua, NI, NIC, Nigeria: Nigeria, NG, NGA, Northern Ireland: NorthernIreland, Norway: Norway, NO, NOR, Paraguay: Paraguay, PY, PRY, Peru: Peru, PE, PER, Poland: Poland, PL, POL, Portugal: Portugal, PT, PRT, Portugal Ext: PortugalExt, PTE, Romania: Romania, RO, ROU, Russia: Russia, RU, RUS, Saudi Arabia: SaudiArabia, SA, SAU, Scotland: Scotland, Serbia: Serbia, RS, SRB, Singapore: Singapore, SG, SGP, Slovokia: Slovokia, SK, SVK, Slovenia: Slovenia, SI, SVN, South Africa: SouthAfrica, ZA, ZAF, Spain: Spain, ES, ESP, Sweden: Sweden, SE, SWE, Switzerland: Switzerland, CH, CHE, Turkey: Turkey, TR, TUR, Ukraine: Ukraine, UA, UKR, United Arab Emirates: UnitedArabEmirates, AE, ARE, United Kingdom: UnitedKingdom, GB, GBR, UK, United States: UnitedStates, US, USA, Venezuela: Venezuela, YV, VEN, Vietnam: Vietnam, VN, VNM, Wales: Wales\n'UnitedStates'"
  },
  {
    "objectID": "reference/augment_holiday_signature.html#returns",
    "href": "reference/augment_holiday_signature.html#returns",
    "title": "augment_holiday_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame:\nA pandas DataFrame with three holiday-specific features: - is_holiday: (0, 1) indicator for holiday - before_holiday: (0, 1) indicator for day before holiday - after_holiday: (0, 1) indicator for day after holiday - holiday_name: name of the holiday"
  },
  {
    "objectID": "reference/augment_holiday_signature.html#example",
    "href": "reference/augment_holiday_signature.html#example",
    "title": "augment_holiday_signature",
    "section": "Example",
    "text": "Example\n\nimport pandas as pd\nimport timetk as tk\n\n# Make a DataFrame with a date column\nstart_date = '2023-01-01'\nend_date = '2023-01-10'\ndf = pd.DataFrame(pd.date_range(start=start_date, end=end_date), columns=['date'])\n\n# Add holiday features for US\ntk.augment_holiday_signature(df, 'date', 'UnitedStates')\n\n\n\n\n\n\n\n\ndate\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n1\n0\nNew Year's Day\n\n\n1\n2023-01-02\n1\n0\n1\nNew Year's Day (Observed)\n\n\n2\n2023-01-03\n0\n0\n1\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN\n\n\n\n\n\n\n\n\n# Add holiday features for France\ntk.augment_holiday_signature(df, 'date', 'France')\n\n\n\n\n\n\n\n\ndate\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n0\n0\nNew Year's Day\n\n\n1\n2023-01-02\n0\n0\n1\nNaN\n\n\n2\n2023-01-03\n0\n0\n0\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN"
  },
  {
    "objectID": "reference/get_timeseries_signature.html",
    "href": "reference/get_timeseries_signature.html",
    "title": "get_timeseries_signature",
    "section": "",
    "text": "get_timeseries_signature(idx)\nConvert a timestamp to a set of 29 time series features.\nThe function tk_get_timeseries_signature engineers 29 different date and time based features from a single datetime index idx:"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#parameters",
    "href": "reference/get_timeseries_signature.html#parameters",
    "title": "get_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nidx is a pandas Series object containing datetime values. Alternatively a pd.DatetimeIndex can be passed.\nrequired"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#returns",
    "href": "reference/get_timeseries_signature.html#returns",
    "title": "get_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function tk_get_timeseries_signature returns a pandas DataFrame that contains 29 different date and time based features derived from a single datetime column."
  },
  {
    "objectID": "reference/get_timeseries_signature.html#examples",
    "href": "reference/get_timeseries_signature.html#examples",
    "title": "get_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\ndates = pd.date_range(start = '2019-01', end = '2019-03', freq = 'D')\n\n# Makes 29 new time series features from the dates\ntk.get_timeseries_signature(dates).head()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\nquarterend\nmonth\nmonth_lbl\nmonthstart\nmonthend\nyweek\nmweek\nwday\nwday_lbl\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1546300800\n2019\n2019\n1\n0\n0\n1\n1\n2019Q1\n1\n0\n1\nJanuary\n1\n0\n1\n1\n2\nTuesday\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1546387200\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n3\nWednesday\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1546473600\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n4\nThursday\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1546560000\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1546646400\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n6\nSaturday\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/get_holiday_signature.html",
    "href": "reference/get_holiday_signature.html",
    "title": "get_holiday_signature",
    "section": "",
    "text": "get_holiday_signature(idx, country_name='UnitedStates')\nEngineers 4 different holiday features from a single datetime for 80+ countries.\nNote: Requires the holidays package to be installed. See https://pypi.org/project/holidays/ for more information."
  },
  {
    "objectID": "reference/get_holiday_signature.html#parameters",
    "href": "reference/get_holiday_signature.html#parameters",
    "title": "get_holiday_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.DatetimeIndex or pd.Series\nA pandas DatetimeIndex or Series containing the dates for which you want to get the holiday signature.\nrequired\n\n\ncountry_name\nstr\nThe name of the country for which to generate holiday features. Defaults to United States holidays, but the following countries are currently available and accessible by the full name or ISO code: Any of the following are acceptable keys for country_name: Available Countries: Full Country, Abrv. #1, #2, #3 Angola: Angola, AO, AGO, Argentina: Argentina, AR, ARG, Aruba: Aruba, AW, ABW, Australia: Australia, AU, AUS, Austria: Austria, AT, AUT, Bangladesh: Bangladesh, BD, BGD, Belarus: Belarus, BY, BLR, Belgium: Belgium, BE, BEL, Botswana: Botswana, BW, BWA, Brazil: Brazil, BR, BRA, Bulgaria: Bulgaria, BG, BLG, Burundi: Burundi, BI, BDI, Canada: Canada, CA, CAN, Chile: Chile, CL, CHL, Colombia: Colombia, CO, COL, Croatia: Croatia, HR, HRV, Curacao: Curacao, CW, CUW, Czechia: Czechia, CZ, CZE, Denmark: Denmark, DK, DNK, Djibouti: Djibouti, DJ, DJI, Dominican Republic: DominicanRepublic, DO, DOM, Egypt: Egypt, EG, EGY, England: England, Estonia: Estonia, EE, EST, European Central Bank: EuropeanCentralBank, Finland: Finland, FI, FIN, France: France, FR, FRA, Georgia: Georgia, GE, GEO, Germany: Germany, DE, DEU, Greece: Greece, GR, GRC, Honduras: Honduras, HN, HND, Hong Kong: HongKong, HK, HKG, Hungary: Hungary, HU, HUN, Iceland: Iceland, IS, ISL, India: India, IN, IND, Ireland: Ireland, IE, IRL, Isle Of Man: IsleOfMan, Israel: Israel, IL, ISR, Italy: Italy, IT, ITA, Jamaica: Jamaica, JM, JAM, Japan: Japan, JP, JPN, Kenya: Kenya, KE, KEN, Korea: Korea, KR, KOR, Latvia: Latvia, LV, LVA, Lithuania: Lithuania, LT, LTU, Luxembourg: Luxembourg, LU, LUX, Malaysia: Malaysia, MY, MYS, Malawi: Malawi, MW, MWI, Mexico: Mexico, MX, MEX, Morocco: Morocco, MA, MOR, Mozambique: Mozambique, MZ, MOZ, Netherlands: Netherlands, NL, NLD, NewZealand: NewZealand, NZ, NZL, Nicaragua: Nicaragua, NI, NIC, Nigeria: Nigeria, NG, NGA, Northern Ireland: NorthernIreland, Norway: Norway, NO, NOR, Paraguay: Paraguay, PY, PRY, Peru: Peru, PE, PER, Poland: Poland, PL, POL, Portugal: Portugal, PT, PRT, Portugal Ext: PortugalExt, PTE, Romania: Romania, RO, ROU, Russia: Russia, RU, RUS, Saudi Arabia: SaudiArabia, SA, SAU, Scotland: Scotland, Serbia: Serbia, RS, SRB, Singapore: Singapore, SG, SGP, Slovokia: Slovokia, SK, SVK, Slovenia: Slovenia, SI, SVN, South Africa: SouthAfrica, ZA, ZAF, Spain: Spain, ES, ESP, Sweden: Sweden, SE, SWE, Switzerland: Switzerland, CH, CHE, Turkey: Turkey, TR, TUR, Ukraine: Ukraine, UA, UKR, United Arab Emirates: UnitedArabEmirates, AE, ARE, United Kingdom: UnitedKingdom, GB, GBR, UK, United States: UnitedStates, US, USA, Venezuela: Venezuela, YV, VEN, Vietnam: Vietnam, VN, VNM, Wales: Wales\n'UnitedStates'"
  },
  {
    "objectID": "reference/get_holiday_signature.html#returns",
    "href": "reference/get_holiday_signature.html#returns",
    "title": "get_holiday_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame:\nA pandas DataFrame with three holiday-specific features: - is_holiday: (0, 1) indicator for holiday - before_holiday: (0, 1) indicator for day before holiday - after_holiday: (0, 1) indicator for day after holiday - holiday_name: name of the holiday"
  },
  {
    "objectID": "reference/get_holiday_signature.html#example",
    "href": "reference/get_holiday_signature.html#example",
    "title": "get_holiday_signature",
    "section": "Example",
    "text": "Example\n\nimport pandas as pd\nimport timetk as tk\n\n# Make a DataFrame with a date column\nstart_date = '2023-01-01'\nend_date = '2023-01-10'\ndates = pd.date_range(start=start_date, end=end_date)\n\n# Get holiday features for US\ntk.get_holiday_signature(dates, 'UnitedStates')\n\n\n\n\n\n\n\n\nidx\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n1\n0\nNew Year's Day\n\n\n1\n2023-01-02\n1\n0\n1\nNew Year's Day (Observed)\n\n\n2\n2023-01-03\n0\n0\n1\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN\n\n\n\n\n\n\n\n\n# Get holiday features for France\ntk.get_holiday_signature(dates, 'France')\n\n\n\n\n\n\n\n\nidx\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n0\n0\nNew Year's Day\n\n\n1\n2023-01-02\n0\n0\n1\nNaN\n\n\n2\n2023-01-03\n0\n0\n0\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN\n\n\n\n\n\n\n\n\n# Pandas Series\npd.Series(dates, name='dates').get_holiday_signature('UnitedStates')\n\n\n\n\n\n\n\n\ndates\nis_holiday\nbefore_holiday\nafter_holiday\nholiday_name\n\n\n\n\n0\n2023-01-01\n1\n1\n0\nNew Year's Day\n\n\n1\n2023-01-02\n1\n0\n1\nNew Year's Day (Observed)\n\n\n2\n2023-01-03\n0\n0\n1\nNaN\n\n\n3\n2023-01-04\n0\n0\n0\nNaN\n\n\n4\n2023-01-05\n0\n0\n0\nNaN\n\n\n5\n2023-01-06\n0\n0\n0\nNaN\n\n\n6\n2023-01-07\n0\n0\n0\nNaN\n\n\n7\n2023-01-08\n0\n0\n0\nNaN\n\n\n8\n2023-01-09\n0\n0\n0\nNaN\n\n\n9\n2023-01-10\n0\n0\n0\nNaN"
  },
  {
    "objectID": "reference/make_future_timeseries.html",
    "href": "reference/make_future_timeseries.html",
    "title": "make_future_timeseries",
    "section": "",
    "text": "make_future_timeseries(idx, length_out, force_regular=False)\nMake future dates for a time series.\nThe function make_future_timeseries takes a pandas Series or DateTimeIndex and generates a future sequence of dates based on the frequency of the input series."
  },
  {
    "objectID": "reference/make_future_timeseries.html#parameters",
    "href": "reference/make_future_timeseries.html#parameters",
    "title": "make_future_timeseries",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe idx parameter is the input time series data. It can be either a pandas Series or a pandas DateTimeIndex. It represents the existing dates in the time series.\nrequired\n\n\nlength_out\nint\nThe parameter length_out is an integer that represents the number of future dates to generate for the time series.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether the frequency of the future dates should be forced to be regular. If force_regular is set to True, the frequency of the future dates will be forced to be regular. If force_regular is set to False, the frequency of the future dates will be inferred from the input data (e.g. business calendars might be used). The default value is False.\nFalse"
  },
  {
    "objectID": "reference/make_future_timeseries.html#returns",
    "href": "reference/make_future_timeseries.html#returns",
    "title": "make_future_timeseries",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA pandas Series object containing future dates."
  },
  {
    "objectID": "reference/make_future_timeseries.html#examples",
    "href": "reference/make_future_timeseries.html#examples",
    "title": "make_future_timeseries",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndates = pd.Series(pd.to_datetime(['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04']))\ndates\n\n0   2022-01-01\n1   2022-01-02\n2   2022-01-03\n3   2022-01-04\ndtype: datetime64[ns]\n\n\n\n# DateTimeIndex: Generate 5 future dates\nfuture_dates_dt = tk.make_future_timeseries(dates, 5)\nfuture_dates_dt\n\n0   2022-01-05\n1   2022-01-06\n2   2022-01-07\n3   2022-01-08\n4   2022-01-09\ndtype: datetime64[ns]\n\n\n\n# Series: Generate 5 future dates\npd.Series(future_dates_dt).make_future_timeseries(5)\n\n0   2022-01-10\n1   2022-01-11\n2   2022-01-12\n3   2022-01-13\n4   2022-01-14\ndtype: datetime64[ns]\n\n\n\ntimestamps = [\"2023-01-01 01:00\", \"2023-01-01 02:00\", \"2023-01-01 03:00\", \"2023-01-01 04:00\", \"2023-01-01 05:00\"]\n\ndates = pd.to_datetime(timestamps)\n\ntk.make_future_timeseries(dates, 5)\n\n0   2023-01-01 06:00:00\n1   2023-01-01 07:00:00\n2   2023-01-01 08:00:00\n3   2023-01-01 09:00:00\n4   2023-01-01 10:00:00\ndtype: datetime64[ns]\n\n\n\n# Monthly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-02-01\", \"2021-03-01\", \"2021-04-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2021-05-01\n1   2021-06-01\n2   2021-07-01\n3   2021-08-01\ndtype: datetime64[ns]\n\n\n\n# Quarterly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-04-01\", \"2021-07-01\", \"2021-10-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2022-01-01\n1   2022-04-01\n2   2022-07-01\n3   2022-10-01\ndtype: datetime64[ns]\n\n\n\n# Irregular Dates: Business Days\ndates = pd.to_datetime([\"2021-01-01\", \"2021-01-04\", \"2021-01-05\", \"2021-01-06\"])\n\ntk.get_pandas_frequency(dates)\n\ntk.make_future_timeseries(dates, 4)\n\n0   2021-01-07\n1   2021-01-08\n2   2021-01-11\n3   2021-01-12\ndtype: datetime64[ns]\n\n\n\n# Irregular Dates: Business Days (Force Regular)    \ntk.make_future_timeseries(dates, 4, force_regular=True)\n\n0   2021-01-07\n1   2021-01-08\n2   2021-01-09\n3   2021-01-10\ndtype: datetime64[ns]"
  },
  {
    "objectID": "reference/augment_leads.html",
    "href": "reference/augment_leads.html",
    "title": "augment_leads",
    "section": "",
    "text": "augment_leads(data, date_column, value_column, leads=1)\nAdds leads to a Pandas DataFrame or DataFrameGroupBy object.\nThe augment_leads function takes a Pandas DataFrame or GroupBy object, a date column, a value column or list of value columns, and a lead or list of leads, and adds leaded versions of the value columns to the DataFrame."
  },
  {
    "objectID": "reference/augment_leads.html#parameters",
    "href": "reference/augment_leads.html#parameters",
    "title": "augment_leads",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to add leaded columns to.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to sort the data before adding the leaded values.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the column(s) in the DataFrame that you want to add leaded values for. It can be either a single column name (string) or a list of column names.\nrequired\n\n\nleads\nint or tuple or list\nThe leads parameter is an integer, tuple, or list that specifies the number of leaded values to add to the DataFrame. If it is an integer, the function will add that number of leaded values for each column specified in the value_column parameter. If it is a tuple, it will generate leads from the first to the second value (inclusive). If it is a list, it will generate leads based on the values in the list.\n1"
  },
  {
    "objectID": "reference/augment_leads.html#returns",
    "href": "reference/augment_leads.html#returns",
    "title": "augment_leads",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame with leaded columns added to it."
  },
  {
    "objectID": "reference/augment_leads.html#examples",
    "href": "reference/augment_leads.html#examples",
    "title": "augment_leads",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndf = tk.load_dataset('m4_daily', parse_dates=['date'])\n\n\n# Add a leaded value of 2 for each grouped time series\nleaded_df = (\n    df \n        .groupby('id')\n        .augment_leads(\n            date_column='date',\n            value_column='value',\n            leads=2\n        )\n)\nleaded_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lead_2\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2048.7\n\n\n1\nD10\n2014-07-04\n2073.4\n2048.9\n\n\n2\nD10\n2014-07-05\n2048.7\n2006.4\n\n\n3\nD10\n2014-07-06\n2048.9\n2017.6\n\n\n4\nD10\n2014-07-07\n2006.4\n2019.1\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9445.9\n\n\n9739\nD500\n2012-09-20\n9365.7\n9497.9\n\n\n9740\nD500\n2012-09-21\n9445.9\n9545.3\n\n\n9741\nD500\n2012-09-22\n9497.9\nNaN\n\n\n9742\nD500\n2012-09-23\n9545.3\nNaN\n\n\n\n\n9743 rows × 4 columns\n\n\n\n\n# Add 7 leaded values for a single time series\nleaded_df_single = (\n    df \n        .query('id == \"D10\"')\n        .augment_leads(\n            date_column='date',\n            value_column='value',\n            leads=(1, 7)\n        )\n)\nleaded_df_single \n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lead_1\nvalue_lead_2\nvalue_lead_3\nvalue_lead_4\nvalue_lead_5\nvalue_lead_6\nvalue_lead_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2073.4\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n\n\n1\nD10\n2014-07-04\n2073.4\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n\n\n2\nD10\n2014-07-05\n2048.7\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n\n\n3\nD10\n2014-07-06\n2048.9\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n1978.8\n\n\n4\nD10\n2014-07-07\n2006.4\n2017.6\n2019.1\n2007.4\n2010.0\n2001.5\n1978.8\n1988.3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2649.3\n2631.8\n2622.5\n2620.1\nNaN\nNaN\nNaN\n\n\n670\nD10\n2016-05-03\n2649.3\n2631.8\n2622.5\n2620.1\nNaN\nNaN\nNaN\nNaN\n\n\n671\nD10\n2016-05-04\n2631.8\n2622.5\n2620.1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n672\nD10\n2016-05-05\n2622.5\n2620.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n673\nD10\n2016-05-06\n2620.1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n674 rows × 10 columns\n\n\n\n\n# Add 2 leaded values, 2 and 4, for a single time series\nleaded_df_single_two = (\n    df \n        .query('id == \"D10\"')\n        .augment_leads(\n            date_column='date',\n            value_column='value',\n            leads=[2, 4]\n        )\n)\nleaded_df_single_two\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lead_2\nvalue_lead_4\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2048.7\n2006.4\n\n\n1\nD10\n2014-07-04\n2073.4\n2048.9\n2017.6\n\n\n2\nD10\n2014-07-05\n2048.7\n2006.4\n2019.1\n\n\n3\nD10\n2014-07-06\n2048.9\n2017.6\n2007.4\n\n\n4\nD10\n2014-07-07\n2006.4\n2019.1\n2010.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2631.8\n2620.1\n\n\n670\nD10\n2016-05-03\n2649.3\n2622.5\nNaN\n\n\n671\nD10\n2016-05-04\n2631.8\n2620.1\nNaN\n\n\n672\nD10\n2016-05-05\n2622.5\nNaN\nNaN\n\n\n673\nD10\n2016-05-06\n2620.1\nNaN\nNaN\n\n\n\n\n674 rows × 5 columns"
  },
  {
    "objectID": "reference/future_frame.html",
    "href": "reference/future_frame.html",
    "title": "future_frame",
    "section": "",
    "text": "future_frame(data, date_column, length_out, force_regular=False, bind_data=True)\nExtend a DataFrame or GroupBy object with future dates.\nThe future_frame function extends a given DataFrame or GroupBy object with future dates based on a specified length, optionally binding the original data."
  },
  {
    "objectID": "reference/future_frame.html#parameters",
    "href": "reference/future_frame.html#parameters",
    "title": "future_frame",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to extend with future dates.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to generate future dates.\nrequired\n\n\nlength_out\nint\nThe length_out parameter specifies the number of future dates to be added to the DataFrame.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether the frequency of the future dates should be forced to be regular. If force_regular is set to True, the frequency of the future dates will be forced to be regular. If force_regular is set to False, the frequency of the future dates will be inferred from the input data (e.g. business calendars might be used). The default value is False.\nFalse\n\n\nbind_data\nbool\nThe bind_data parameter is a boolean flag that determines whether the extended data should be concatenated with the original data or returned separately. If bind_data is set to True, the extended data will be concatenated with the original data using pd.concat. If bind_data is set to False, the extended data will be returned separately. The default value is True.\nTrue"
  },
  {
    "objectID": "reference/future_frame.html#returns",
    "href": "reference/future_frame.html#returns",
    "title": "future_frame",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nAn extended DataFrame with future dates."
  },
  {
    "objectID": "reference/future_frame.html#see-also",
    "href": "reference/future_frame.html#see-also",
    "title": "future_frame",
    "section": "See Also",
    "text": "See Also\nmake_future_timeseries: Generate future dates for a time series."
  },
  {
    "objectID": "reference/future_frame.html#examples",
    "href": "reference/future_frame.html#examples",
    "title": "future_frame",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndf = tk.load_dataset('m4_hourly', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490\n\n\n...\n...\n...\n...\n\n\n3055\nH410\n2017-02-10 07:00:00+00:00\n108\n\n\n3056\nH410\n2017-02-10 08:00:00+00:00\n70\n\n\n3057\nH410\n2017-02-10 09:00:00+00:00\n72\n\n\n3058\nH410\n2017-02-10 10:00:00+00:00\n79\n\n\n3059\nH410\n2017-02-10 11:00:00+00:00\n77\n\n\n\n\n3060 rows × 3 columns\n\n\n\n\n# Extend the data for a single time series group by 12 hours\nextended_df = (\n    df\n        .query('id == \"H10\"')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12\n        )\n        .assign(id = lambda x: x['id'].ffill())\n)\nextended_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513.0\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512.0\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506.0\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500.0\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490.0\n\n\n...\n...\n...\n...\n\n\n707\nH10\n2015-07-30 23:00:00\nNaN\n\n\n708\nH10\n2015-07-31 00:00:00\nNaN\n\n\n709\nH10\n2015-07-31 01:00:00\nNaN\n\n\n710\nH10\n2015-07-31 02:00:00\nNaN\n\n\n711\nH10\n2015-07-31 03:00:00\nNaN\n\n\n\n\n712 rows × 3 columns\n\n\n\n\n# Extend the data for each group by 12 hours\nextended_df = (\n    df\n        .groupby('id')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nH10\n2015-07-01 12:00:00+00:00\n513.0\n\n\n1\nH10\n2015-07-01 13:00:00+00:00\n512.0\n\n\n2\nH10\n2015-07-01 14:00:00+00:00\n506.0\n\n\n3\nH10\n2015-07-01 15:00:00+00:00\n500.0\n\n\n4\nH10\n2015-07-01 16:00:00+00:00\n490.0\n\n\n...\n...\n...\n...\n\n\n707\nH50\n2015-07-30 23:00:00\nNaN\n\n\n708\nH50\n2015-07-31 00:00:00\nNaN\n\n\n709\nH50\n2015-07-31 01:00:00\nNaN\n\n\n710\nH50\n2015-07-31 02:00:00\nNaN\n\n\n711\nH50\n2015-07-31 03:00:00\nNaN\n\n\n\n\n3108 rows × 3 columns\n\n\n\n\n# Same as above, but just return the extended data with bind_data=False\nextended_df = (\n    df\n        .groupby('id')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            bind_data   = False # Returns just future data\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nid\n\n\n\n\n0\n2015-07-30 16:00:00\nH10\n\n\n1\n2015-07-30 17:00:00\nH10\n\n\n2\n2015-07-30 18:00:00\nH10\n\n\n3\n2015-07-30 19:00:00\nH10\n\n\n4\n2015-07-30 20:00:00\nH10\n\n\n5\n2015-07-30 21:00:00\nH10\n\n\n6\n2015-07-30 22:00:00\nH10\n\n\n7\n2015-07-30 23:00:00\nH10\n\n\n8\n2015-07-31 00:00:00\nH10\n\n\n9\n2015-07-31 01:00:00\nH10\n\n\n10\n2015-07-31 02:00:00\nH10\n\n\n11\n2015-07-31 03:00:00\nH10\n\n\n0\n2013-09-30 16:00:00\nH150\n\n\n1\n2013-09-30 17:00:00\nH150\n\n\n2\n2013-09-30 18:00:00\nH150\n\n\n3\n2013-09-30 19:00:00\nH150\n\n\n4\n2013-09-30 20:00:00\nH150\n\n\n5\n2013-09-30 21:00:00\nH150\n\n\n6\n2013-09-30 22:00:00\nH150\n\n\n7\n2013-09-30 23:00:00\nH150\n\n\n8\n2013-10-01 00:00:00\nH150\n\n\n9\n2013-10-01 01:00:00\nH150\n\n\n10\n2013-10-01 02:00:00\nH150\n\n\n11\n2013-10-01 03:00:00\nH150\n\n\n0\n2017-02-10 12:00:00\nH410\n\n\n1\n2017-02-10 13:00:00\nH410\n\n\n2\n2017-02-10 14:00:00\nH410\n\n\n3\n2017-02-10 15:00:00\nH410\n\n\n4\n2017-02-10 16:00:00\nH410\n\n\n5\n2017-02-10 17:00:00\nH410\n\n\n6\n2017-02-10 18:00:00\nH410\n\n\n7\n2017-02-10 19:00:00\nH410\n\n\n8\n2017-02-10 20:00:00\nH410\n\n\n9\n2017-02-10 21:00:00\nH410\n\n\n10\n2017-02-10 22:00:00\nH410\n\n\n11\n2017-02-10 23:00:00\nH410\n\n\n0\n2015-07-30 16:00:00\nH50\n\n\n1\n2015-07-30 17:00:00\nH50\n\n\n2\n2015-07-30 18:00:00\nH50\n\n\n3\n2015-07-30 19:00:00\nH50\n\n\n4\n2015-07-30 20:00:00\nH50\n\n\n5\n2015-07-30 21:00:00\nH50\n\n\n6\n2015-07-30 22:00:00\nH50\n\n\n7\n2015-07-30 23:00:00\nH50\n\n\n8\n2015-07-31 00:00:00\nH50\n\n\n9\n2015-07-31 01:00:00\nH50\n\n\n10\n2015-07-31 02:00:00\nH50\n\n\n11\n2015-07-31 03:00:00\nH50\n\n\n\n\n\n\n\n\n # Working with irregular dates: Business Days (Stocks Data)\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Allow irregular future dates (i.e. business days)\nextended_df = (\n    df\n        .groupby('symbol')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            force_regular = False, # Allow irregular future dates (i.e. business days)),\n            bind_data   = False\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nsymbol\n\n\n\n\n0\n2023-09-22\nAAPL\n\n\n1\n2023-09-25\nAAPL\n\n\n2\n2023-09-26\nAAPL\n\n\n3\n2023-09-27\nAAPL\n\n\n4\n2023-09-28\nAAPL\n\n\n...\n...\n...\n\n\n7\n2023-10-03\nNVDA\n\n\n8\n2023-10-04\nNVDA\n\n\n9\n2023-10-05\nNVDA\n\n\n10\n2023-10-06\nNVDA\n\n\n11\n2023-10-09\nNVDA\n\n\n\n\n72 rows × 2 columns\n\n\n\n\n# Force regular: Include Weekends\nextended_df = (\n    df\n        .groupby('symbol')\n        .future_frame(\n            date_column = 'date', \n            length_out  = 12,\n            force_regular = True, # Force regular future dates (i.e. include weekends)),\n            bind_data   = False\n        )\n)    \nextended_df\n\n\n\n\n\n\n\n\ndate\nsymbol\n\n\n\n\n0\n2023-09-22\nAAPL\n\n\n1\n2023-09-23\nAAPL\n\n\n2\n2023-09-24\nAAPL\n\n\n3\n2023-09-25\nAAPL\n\n\n4\n2023-09-26\nAAPL\n\n\n...\n...\n...\n\n\n7\n2023-09-29\nNVDA\n\n\n8\n2023-09-30\nNVDA\n\n\n9\n2023-10-01\nNVDA\n\n\n10\n2023-10-02\nNVDA\n\n\n11\n2023-10-03\nNVDA\n\n\n\n\n72 rows × 2 columns"
  },
  {
    "objectID": "reference/summarize_by_time.html",
    "href": "reference/summarize_by_time.html",
    "title": "summarize_by_time",
    "section": "",
    "text": "summarize_by_time(data, date_column, value_column, freq='D', agg_func='sum', kind='timestamp', wide_format=False, fillna=0, *args, **kwargs)\nSummarize a DataFrame or GroupBy object by time.\nThe summarize_by_time function aggregates data by a specified time period and one or more numeric columns, allowing for grouping and customization of the time-based aggregation."
  },
  {
    "objectID": "reference/summarize_by_time.html#parameters",
    "href": "reference/summarize_by_time.html#parameters",
    "title": "summarize_by_time",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nA pandas DataFrame or a pandas GroupBy object. This is the data that you want to summarize by time.\nrequired\n\n\ndate_column\nstr\nThe name of the column in the data frame that contains the dates or timestamps to be aggregated by. This column must be of type datetime64.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the name of one or more columns in the DataFrame that you want to aggregate by. It can be either a string representing a single column name, or a list of strings representing multiple column names.\nrequired\n\n\nfreq\nstr\nThe freq parameter specifies the frequency at which the data should be aggregated. It accepts a string representing a pandas frequency offset, such as “D” for daily or “MS” for month start. The default value is “D”, which means the data will be aggregated on a daily basis. Some common frequency aliases include: - S: secondly frequency - min: minute frequency - H: hourly frequency - D: daily frequency - W: weekly frequency - M: month end frequency - MS: month start frequency - Q: quarter end frequency - QS: quarter start frequency - Y: year end frequency - YS: year start frequency\n'D'\n\n\nagg_func\nlist\nThe agg_func parameter is used to specify one or more aggregating functions to apply to the value column(s) during the summarization process. It can be a single function or a list of functions. The default value is \"sum\", which represents the sum function. Some common aggregating functions include: - “sum”: Sum of values - “mean”: Mean of values - “median”: Median of values - “min”: Minimum of values - “max”: Maximum of values - “std”: Standard deviation of values - “var”: Variance of values - “first”: First value in group - “last”: Last value in group - “count”: Count of values - “nunique”: Number of unique values - “corr”: Correlation between values Custom lambda aggregating functions can be used too. Here are several common examples: - (“q25”, lambda x: x.quantile(0.25)): 25th percentile of values - (“q75”, lambda x: x.quantile(0.75)): 75th percentile of values - (“iqr”, lambda x: x.quantile(0.75) - x.quantile(0.25)): Interquartile range of values - (“range”, lambda x: x.max() - x.min()): Range of values\n'sum'\n\n\nwide_format\nbool\nA boolean parameter that determines whether the output should be in “wide” or “long” format. If set to True, the output will be in wide format, where each group is represented by a separate column. If set to False, the output will be in long format, where each group is represented by a separate row. The default value is False.\nFalse\n\n\nfillna\nint\nThe fillna parameter is used to specify the value to fill missing data with. By default, it is set to 0. If you want to keep missing values as NaN, you can use np.nan as the value for fillna.\n0"
  },
  {
    "objectID": "reference/summarize_by_time.html#returns",
    "href": "reference/summarize_by_time.html#returns",
    "title": "summarize_by_time",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame that is summarized by time."
  },
  {
    "objectID": "reference/summarize_by_time.html#examples",
    "href": "reference/summarize_by_time.html#examples",
    "title": "summarize_by_time",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Summarize by time with a DataFrame object\n( \n    df \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price',\n            freq         = \"MS\",\n            agg_func     = ['mean', 'sum']\n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_mean\ntotal_price_sum\n\n\n\n\n0\n2011-01-01\n4600.142857\n483015\n\n\n1\n2011-02-01\n4611.408730\n1162075\n\n\n2\n2011-03-01\n5196.653543\n659975\n\n\n3\n2011-04-01\n4533.846154\n1827140\n\n\n4\n2011-05-01\n4097.912621\n844170\n\n\n5\n2011-06-01\n4544.839228\n1413445\n\n\n6\n2011-07-01\n4976.791667\n1194430\n\n\n7\n2011-08-01\n4961.970803\n679790\n\n\n8\n2011-09-01\n4682.298851\n814720\n\n\n9\n2011-10-01\n3930.053476\n734920\n\n\n10\n2011-11-01\n4768.175355\n1006085\n\n\n11\n2011-12-01\n4186.902655\n473120\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Long Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = False, \n        )\n)\n\n\n\n\n\n\n\n\ncategory_1\norder_date\ntotal_price\n\n\n\n\n0\nMountain\n2011-01-01\n221490\n\n\n1\nMountain\n2011-02-01\n660555\n\n\n2\nMountain\n2011-03-01\n358855\n\n\n3\nMountain\n2011-04-01\n1075975\n\n\n4\nMountain\n2011-05-01\n450440\n\n\n5\nMountain\n2011-06-01\n723040\n\n\n6\nMountain\n2011-07-01\n767740\n\n\n7\nMountain\n2011-08-01\n361255\n\n\n8\nMountain\n2011-09-01\n401125\n\n\n9\nMountain\n2011-10-01\n377335\n\n\n10\nMountain\n2011-11-01\n549345\n\n\n11\nMountain\n2011-12-01\n276055\n\n\n12\nRoad\n2011-01-01\n261525\n\n\n13\nRoad\n2011-02-01\n501520\n\n\n14\nRoad\n2011-03-01\n301120\n\n\n15\nRoad\n2011-04-01\n751165\n\n\n16\nRoad\n2011-05-01\n393730\n\n\n17\nRoad\n2011-06-01\n690405\n\n\n18\nRoad\n2011-07-01\n426690\n\n\n19\nRoad\n2011-08-01\n318535\n\n\n20\nRoad\n2011-09-01\n413595\n\n\n21\nRoad\n2011-10-01\n357585\n\n\n22\nRoad\n2011-11-01\n456740\n\n\n23\nRoad\n2011-12-01\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object and multiple summaries (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            freq         = 'MS',\n            agg_func     = ['sum', 'mean', ('q25', lambda x: x.quantile(0.25)), ('q75', lambda x: x.quantile(0.75))],\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum_Mountain\ntotal_price_sum_Road\ntotal_price_mean_Mountain\ntotal_price_mean_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n4922.000000\n4358.750000\n2060.0\n1950.0\n6070.0\n5605.0\n\n\n1\n2011-02-01\n660555\n501520\n4374.536424\n4965.544554\n2060.0\n1950.0\n5330.0\n5860.0\n\n\n2\n2011-03-01\n358855\n301120\n5882.868852\n4562.424242\n2130.0\n2240.0\n6390.0\n5875.0\n\n\n3\n2011-04-01\n1075975\n751165\n4890.795455\n4104.726776\n2060.0\n1950.0\n5970.0\n4800.0\n\n\n4\n2011-05-01\n450440\n393730\n4549.898990\n3679.719626\n2010.0\n1570.0\n6020.0\n3500.0\n\n\n5\n2011-06-01\n723040\n690405\n5021.111111\n4134.161677\n1950.0\n1840.0\n5647.5\n4500.0\n\n\n6\n2011-07-01\n767740\n426690\n5444.964539\n4310.000000\n2130.0\n1895.0\n6400.0\n5330.0\n\n\n7\n2011-08-01\n361255\n318535\n5734.206349\n4304.527027\n2235.0\n1950.0\n6400.0\n4987.5\n\n\n8\n2011-09-01\n401125\n413595\n5077.531646\n4353.631579\n1620.0\n1950.0\n6390.0\n5330.0\n\n\n9\n2011-10-01\n377335\n357585\n4439.235294\n3505.735294\n2160.0\n1750.0\n6070.0\n4260.0\n\n\n10\n2011-11-01\n549345\n456740\n5282.163462\n4268.598131\n2340.0\n1950.0\n7460.0\n4370.0\n\n\n11\n2011-12-01\n276055\n197065\n5208.584906\n3284.416667\n2060.0\n1652.5\n6400.0\n3200.0"
  },
  {
    "objectID": "reference/get_pandas_frequency.html",
    "href": "reference/get_pandas_frequency.html",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "get_pandas_frequency(idx, force_regular=False)\nGet the frequency of a pandas Series or DatetimeIndex.\nThe function get_pandas_frequency takes a Pandas Series or DatetimeIndex as input and returns the inferred frequency of the index, with an option to force regular frequency.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/get_pandas_frequency.html#parameters",
    "href": "reference/get_pandas_frequency.html#parameters",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter can be either a pd.Series or a pd.DatetimeIndex. It represents the index or the time series data for which we want to determine the frequency.\nrequired\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean flag that determines whether to force the frequency to be regular. If set to True, the function will convert irregular frequencies to their regular counterparts. For example, if the inferred frequency is ‘B’ (business days), it will be converted to ‘D’ (calendar days). The default value is False.\nFalse"
  },
  {
    "objectID": "reference/get_pandas_frequency.html#returns",
    "href": "reference/get_pandas_frequency.html#returns",
    "title": "get_pandas_frequency",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nstr\nThe frequency of the given pandas series or datetime index."
  },
  {
    "objectID": "reference/get_available_datasets.html",
    "href": "reference/get_available_datasets.html",
    "title": "get_available_datasets",
    "section": "",
    "text": "datasets.get_datasets.get_available_datasets()\nGet a list of 12 datasets that can be loaded with timetk.load_dataset.\nThe get_available_datasets function returns a sorted list of available dataset names from the timetk.datasets module. The available datasets are:"
  },
  {
    "objectID": "reference/get_available_datasets.html#returns",
    "href": "reference/get_available_datasets.html#returns",
    "title": "get_available_datasets",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nThe function get_available_datasets returns a sorted list of available dataset names from the timetk.datasets module."
  },
  {
    "objectID": "reference/get_available_datasets.html#examples",
    "href": "reference/get_available_datasets.html#examples",
    "title": "get_available_datasets",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\n\ntk.get_available_datasets()\n\n['bike_sales_sample',\n 'bike_sharing_daily',\n 'm4_daily',\n 'm4_hourly',\n 'm4_monthly',\n 'm4_quarterly',\n 'm4_weekly',\n 'm4_yearly',\n 'stocks_daily',\n 'taylor_30_min',\n 'walmart_sales_weekly',\n 'wikipedia_traffic_daily']"
  },
  {
    "objectID": "reference/augment_rolling.html",
    "href": "reference/augment_rolling.html",
    "title": "augment_rolling",
    "section": "",
    "text": "augment_rolling(data, date_column, value_column, window, window_func='mean', center=False, **kwargs)\nApply one or more rolling functions and window sizes to one or more columns of a DataFrame.\nThe augment_rolling function applies multiple rolling window functions with varying window sizes to specified columns of a DataFrame, considering grouping columns and a datetime column for sorting within each group."
  },
  {
    "objectID": "reference/augment_rolling.html#parameters",
    "href": "reference/augment_rolling.html#parameters",
    "title": "augment_rolling",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe input DataFrame or GroupBy object.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is the name of the datetime column in the DataFrame by which the data should be sorted within each group.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the name of the column(s) in the DataFrame to which the rolling window function(s) should be applied. It can be a single column name or a list of column names.\nrequired\n\n\nwindow\nint or tuple or list\nThe window parameter in the augment_rolling function is used to specify the size of the rolling windows. It can be either an integer or a list of integers. - If it is an integer, the same window size will be applied to all columns specified in the value_column. - If it is a tuple, it will generate windows from the first to the second value (inclusive). - If it is a list of integers, each integer in the list will be used as the window size for the corresponding column in the value_column list.\nrequired\n\n\nwindow_func\nstr or list\nThe window_func parameter in the augment_rolling function is used to specify the function(s) to be applied to the rolling windows. It can be a string or a list of strings, where each string represents the name of the function to be applied. Alternatively, it can be a list of tuples, where each tuple contains the name of the function to be applied and the function itself. - If it is a string or a list of strings, the same function will be applied to all columns specified in the value_column. - If it is a list of tuples, each tuple in the list will be used as the function to be applied to the corresponding column in the value_column list.\n'mean'\n\n\ncenter\nbool\nThe center parameter in the augment_rolling function determines whether the rolling window is centered or not. If center is set to True, the rolling window will be centered, meaning that the alue at the center of the window will be used as the result. If False, the rolling window will not be centered, meaning that the value at the end of the window will be used as the result. The default value is False.\nFalse\n\n\n**kwargs\noptional\nAdditional keyword arguments to be passed to the pandas.DataFrame.rolling function.\n{}"
  },
  {
    "objectID": "reference/augment_rolling.html#returns",
    "href": "reference/augment_rolling.html#returns",
    "title": "augment_rolling",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe function augment_rolling returns a DataFrame with new columns for each applied function, window size, and value column."
  },
  {
    "objectID": "reference/augment_rolling.html#examples",
    "href": "reference/augment_rolling.html#examples",
    "title": "augment_rolling",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset(\"m4_daily\", parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n\n\n1\nD10\n2014-07-04\n2073.4\n\n\n2\nD10\n2014-07-05\n2048.7\n\n\n3\nD10\n2014-07-06\n2048.9\n\n\n4\nD10\n2014-07-07\n2006.4\n\n\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n\n\n9739\nD500\n2012-09-20\n9365.7\n\n\n9740\nD500\n2012-09-21\n9445.9\n\n\n9741\nD500\n2012-09-22\n9497.9\n\n\n9742\nD500\n2012-09-23\n9545.3\n\n\n\n\n9743 rows × 3 columns\n\n\n\n\n# window = [2,7] yields only 2 and 7\nrolled_df = (\n    df\n        .groupby('id')\n        .augment_rolling(\n            date_column = 'date', \n            value_column = 'value', \n            window = [2,7], \n            window_func = ['mean', ('std', lambda x: x.std())]\n        )\n)\nrolled_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_rolling_mean_win_2\nvalue_rolling_std_win_2\nvalue_rolling_mean_win_7\nvalue_rolling_std_win_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2076.20\n0.00\n2076.200000\n0.000000\n\n\n1\nD10\n2014-07-04\n2073.4\n2074.80\n1.40\n2074.800000\n1.400000\n\n\n2\nD10\n2014-07-05\n2048.7\n2061.05\n12.35\n2066.100000\n12.356645\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.80\n0.10\n2061.800000\n13.037830\n\n\n4\nD10\n2014-07-07\n2006.4\n2027.65\n21.25\n2050.720000\n25.041038\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9425.35\n6.55\n9382.071429\n74.335988\n\n\n9739\nD500\n2012-09-20\n9365.7\n9392.25\n26.55\n9396.400000\n58.431303\n\n\n9740\nD500\n2012-09-21\n9445.9\n9405.80\n40.10\n9419.114286\n39.184451\n\n\n9741\nD500\n2012-09-22\n9497.9\n9471.90\n26.00\n9438.928571\n38.945336\n\n\n9742\nD500\n2012-09-23\n9545.3\n9521.60\n23.70\n9449.028571\n53.379416\n\n\n\n\n9743 rows × 7 columns\n\n\n\n\n# window = (1,3) yields 1, 2, and 3\nrolled_df = (\n    df\n        .groupby('id')\n        .augment_rolling(\n            date_column = 'date', \n            value_column = 'value', \n            window = (1,3), \n            window_func = ['mean', ('std', lambda x: x.std())]\n        )\n)\nrolled_df \n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_rolling_mean_win_1\nvalue_rolling_std_win_1\nvalue_rolling_mean_win_2\nvalue_rolling_std_win_2\nvalue_rolling_mean_win_3\nvalue_rolling_std_win_3\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2076.2\n0.0\n2076.20\n0.00\n2076.200000\n0.000000\n\n\n1\nD10\n2014-07-04\n2073.4\n2073.4\n0.0\n2074.80\n1.40\n2074.800000\n1.400000\n\n\n2\nD10\n2014-07-05\n2048.7\n2048.7\n0.0\n2061.05\n12.35\n2066.100000\n12.356645\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.9\n0.0\n2048.80\n0.10\n2057.000000\n11.596839\n\n\n4\nD10\n2014-07-07\n2006.4\n2006.4\n0.0\n2027.65\n21.25\n2034.666667\n19.987718\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9418.8\n0.0\n9425.35\n6.55\n9429.466667\n7.905413\n\n\n9739\nD500\n2012-09-20\n9365.7\n9365.7\n0.0\n9392.25\n26.55\n9405.466667\n28.623339\n\n\n9740\nD500\n2012-09-21\n9445.9\n9445.9\n0.0\n9405.80\n40.10\n9410.133333\n33.310092\n\n\n9741\nD500\n2012-09-22\n9497.9\n9497.9\n0.0\n9471.90\n26.00\n9436.500000\n54.378182\n\n\n9742\nD500\n2012-09-23\n9545.3\n9545.3\n0.0\n9521.60\n23.70\n9496.366667\n40.594362\n\n\n\n\n9743 rows × 9 columns"
  },
  {
    "objectID": "guides/04_wrangling.html",
    "href": "guides/04_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "This section will cover data wrangling for timeseries using pytimetk. We’ll show examples for the following functions:"
  },
  {
    "objectID": "guides/04_wrangling.html#basic-example",
    "href": "guides/04_wrangling.html#basic-example",
    "title": "Data Wrangling",
    "section": "1.1 Basic Example",
    "text": "1.1 Basic Example\nThe m4_daily dataset has a daily frequency. Say we are interested in forecasting at the weekly level. We can use summarize_by_time() to aggregate to a weekly level\n\n# summarize by time: daily to weekly\nsummarized_df = m4_daily_df \\\n    .summarize_by_time(\n        date_column  = 'date',\n        value_column = 'value',\n        freq         = 'W',\n        agg_func     = 'sum'\n    )\n\nprint(summarized_df.head())\nprint('\\nLength of the full dataset:', len(summarized_df))\n\n        date     value\n0 1978-06-25  27328.12\n1 1978-07-02  63621.88\n2 1978-07-09  63334.38\n3 1978-07-16  63737.51\n4 1978-07-23  64718.76\n\nLength of the full dataset: 1977\n\n\nThe data has now been aggregated at the weekly level. Notice we now have 1977 rows, compared to full dataset which had 9743 rows."
  },
  {
    "objectID": "guides/04_wrangling.html#additional-aggregate-functions",
    "href": "guides/04_wrangling.html#additional-aggregate-functions",
    "title": "Data Wrangling",
    "section": "1.2 Additional Aggregate Functions",
    "text": "1.2 Additional Aggregate Functions\nsummarize_by_time() can take additional aggregate functions in the agg_func argument.\n\n# summarize by time with additional aggregate functions\nsummarized_multiple_agg_df = m4_daily_df \\\n    .summarize_by_time(\n        date_column  = 'date',\n        value_column = 'value',\n        freq         = 'W',\n        agg_func     = ['sum', 'min', 'max']\n    )\n\nsummarized_multiple_agg_df.head()\n\n\n\n\n\n\n\n\ndate\nvalue_sum\nvalue_min\nvalue_max\n\n\n\n\n0\n1978-06-25\n27328.12\n9103.12\n9115.62\n\n\n1\n1978-07-02\n63621.88\n9046.88\n9115.62\n\n\n2\n1978-07-09\n63334.38\n9028.12\n9096.88\n\n\n3\n1978-07-16\n63737.51\n9075.00\n9146.88\n\n\n4\n1978-07-23\n64718.76\n9171.88\n9315.62"
  },
  {
    "objectID": "guides/04_wrangling.html#summarize-by-time-with-grouped-time-series",
    "href": "guides/04_wrangling.html#summarize-by-time-with-grouped-time-series",
    "title": "Data Wrangling",
    "section": "1.3 Summarize by Time with Grouped Time Series",
    "text": "1.3 Summarize by Time with Grouped Time Series\nsummarize_by_time() also works with groups.\n\n# summarize by time with groups and additional aggregate functions\ngrouped_summarized_df = (\n    m4_daily_df\n        .groupby('id')\n        .summarize_by_time(\n            date_column  = 'date',\n            value_column = 'value',\n            freq         = 'W',\n            agg_func     = [\n                'sum',\n                'min',\n                ('q25', lambda x: np.quantile(x, 0.25)),\n                'median',\n                ('q75', lambda x: np.quantile(x, 0.75)),\n                'max'\n            ],\n        )\n)\n\ngrouped_summarized_df.head()\n\n\n\n\n\n\n\n\nid\ndate\nvalue_sum\nvalue_min\nvalue_q25\nvalue_median\nvalue_q75\nvalue_max\n\n\n\n\n0\nD10\n2014-07-06\n8247.2\n2048.7\n2048.85\n2061.15\n2074.10\n2076.2\n\n\n1\nD10\n2014-07-13\n14040.8\n1978.8\n2003.95\n2007.40\n2013.80\n2019.1\n\n\n2\nD10\n2014-07-20\n13867.6\n1943.0\n1955.30\n1988.30\n2005.60\n2014.5\n\n\n3\nD10\n2014-07-27\n13266.3\n1876.0\n1887.15\n1891.00\n1895.85\n1933.3\n\n\n4\nD10\n2014-08-03\n13471.2\n1886.2\n1914.60\n1920.00\n1939.55\n1956.7"
  },
  {
    "objectID": "guides/04_wrangling.html#basic-example-1",
    "href": "guides/04_wrangling.html#basic-example-1",
    "title": "Data Wrangling",
    "section": "2.1 Basic Example",
    "text": "2.1 Basic Example\nWe’ll continue with our use of the m4_daily_df dataset. Recall we’ve alread aggregated at the weekly level (summarized_df). Lets checkout the last week in the summarized_df:\n\n# last week in dataset\nsummarized_df \\\n    .sort_values(by = 'date', ascending = True) \\\n    .iloc[: -1] \\\n    .tail(1)\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n1975\n2016-05-01\n17959.8\n\n\n\n\n\n\n\n\n\n\n\n\n\niloc()\n\n\n\n\n\niloc[: -1] is used to filter out the last row and keep only dates that are the start of the week.\n\n\n\nWe can see that the last week is the week of 2016-05-01. Now say we wanted to forecast the next 8 weeks. We can extend the dataset beyound the week of 2016-05-01:\n\n# extend dataset by 12 weeks\nsummarized_extended_df = summarized_df \\\n    .future_frame(\n        date_column = 'date',\n        length_out  = 8\n    )\n\nsummarized_extended_df\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n1978-06-25\n27328.12\n\n\n1\n1978-07-02\n63621.88\n\n\n2\n1978-07-09\n63334.38\n\n\n3\n1978-07-16\n63737.51\n\n\n4\n1978-07-23\n64718.76\n\n\n...\n...\n...\n\n\n1980\n2016-06-05\nNaN\n\n\n1981\n2016-06-12\nNaN\n\n\n1982\n2016-06-19\nNaN\n\n\n1983\n2016-06-26\nNaN\n\n\n1984\n2016-07-03\nNaN\n\n\n\n\n1985 rows × 2 columns\n\n\n\nTo get only the future data, we can filter the dataset for where value is missing (np.nan).\n\n# get only future data\nsummarized_extended_df \\\n    .query('value.isna()')\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n1977\n2016-05-15\nNaN\n\n\n1978\n2016-05-22\nNaN\n\n\n1979\n2016-05-29\nNaN\n\n\n1980\n2016-06-05\nNaN\n\n\n1981\n2016-06-12\nNaN\n\n\n1982\n2016-06-19\nNaN\n\n\n1983\n2016-06-26\nNaN\n\n\n1984\n2016-07-03\nNaN"
  },
  {
    "objectID": "guides/04_wrangling.html#future-frame-with-grouped-time-series",
    "href": "guides/04_wrangling.html#future-frame-with-grouped-time-series",
    "title": "Data Wrangling",
    "section": "2.2 Future Frame with Grouped Time Series",
    "text": "2.2 Future Frame with Grouped Time Series\nfuture_frame() also works for grouped time series. We can see an example using our grouped summarized dataset (grouped_summarized_df) from earlier:\n\n# future frame with grouped time series\ngrouped_summarized_df[['id', 'date', 'value_sum']] \\\n    .groupby('id') \\\n    .future_frame(\n        date_column = 'date',\n        length_out  = 8\n    ) \\\n    .query('value_sum.isna()') # filtering to return only the future data\n\n\n\n\n\n\n\n\nid\ndate\nvalue_sum\n\n\n\n\n97\nD10\n2016-05-15\nNaN\n\n\n98\nD10\n2016-05-22\nNaN\n\n\n99\nD10\n2016-05-29\nNaN\n\n\n100\nD10\n2016-06-05\nNaN\n\n\n101\nD10\n2016-06-12\nNaN\n\n\n102\nD10\n2016-06-19\nNaN\n\n\n103\nD10\n2016-06-26\nNaN\n\n\n104\nD10\n2016-07-03\nNaN\n\n\n600\nD160\n2011-07-10\nNaN\n\n\n601\nD160\n2011-07-17\nNaN\n\n\n602\nD160\n2011-07-24\nNaN\n\n\n603\nD160\n2011-07-31\nNaN\n\n\n604\nD160\n2011-08-07\nNaN\n\n\n605\nD160\n2011-08-14\nNaN\n\n\n606\nD160\n2011-08-21\nNaN\n\n\n607\nD160\n2011-08-28\nNaN\n\n\n98\nD410\n1980-05-11\nNaN\n\n\n99\nD410\n1980-05-18\nNaN\n\n\n100\nD410\n1980-05-25\nNaN\n\n\n101\nD410\n1980-06-01\nNaN\n\n\n102\nD410\n1980-06-08\nNaN\n\n\n103\nD410\n1980-06-15\nNaN\n\n\n104\nD410\n1980-06-22\nNaN\n\n\n105\nD410\n1980-06-29\nNaN\n\n\n600\nD500\n2012-09-30\nNaN\n\n\n601\nD500\n2012-10-07\nNaN\n\n\n602\nD500\n2012-10-14\nNaN\n\n\n603\nD500\n2012-10-21\nNaN\n\n\n604\nD500\n2012-10-28\nNaN\n\n\n605\nD500\n2012-11-04\nNaN\n\n\n606\nD500\n2012-11-11\nNaN\n\n\n607\nD500\n2012-11-18\nNaN"
  },
  {
    "objectID": "guides/04_wrangling.html#basic-example-2",
    "href": "guides/04_wrangling.html#basic-example-2",
    "title": "Data Wrangling",
    "section": "3.1 Basic Example",
    "text": "3.1 Basic Example"
  },
  {
    "objectID": "guides/01_visualization.html",
    "href": "guides/01_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "guides/03_pandas_frequency.html",
    "href": "guides/03_pandas_frequency.html",
    "title": "Pandas Frequencies",
    "section": "",
    "text": "How this guide benefits you\n\n\n\n\n\nThis guide covers how to use the pandas frequency strings within timetk. Once you understand key frequencies, you can apply them to manipulate time series data like a pro.\n\n\n\n\n1 Pandas Frequencies\nPandas offers a variety of frequency strings, also known as offset aliases, to define the frequency of a time series. Here are some common frequency strings used in pandas:\n\n‘B’: Business Day\n‘D’: Calendar day\n‘W’: Weekly\n‘M’: Month end\n‘BM’: Business month end\n‘MS’: Month start\n‘BMS’: Business month start\n‘Q’: Quarter end\n‘BQ’: Business quarter end\n‘QS’: Quarter start\n‘BQS’: Business quarter start\n‘A’ or ‘Y’: Year end\n‘BA’ or ‘BY’: Business year end\n‘AS’ or ‘YS’: Year start\n‘BAS’ or ‘BYS’: Business year start\n‘H’: Hourly\n‘T’ or ‘min’: Minutely\n‘S’: Secondly\n‘L’ or ‘ms’: Milliseconds\n‘U’: Microseconds\n‘N’: Nanoseconds\n\n\nCustom Frequencies:\n\nYou can also create custom frequencies by combining base frequencies, like:\n\n‘2D’: Every 2 days\n‘3W’: Every 3 weeks\n‘4H’: Every 4 hours\n‘1H30T’: Every 1 hour and 30 minutes\n\n\n\n\nCompound Frequencies:\n\nYou can combine multiple frequencies by adding them together.\n\n‘1D1H’: 1 day and 1 hour\n‘1H30T’: 1 hour and 30 minutes\n\n\n\n\nExample:\n\nimport pandas as pd\n\n# Creating a date range with daily frequency\ndate_range_daily = pd.date_range(start='2023-01-01', end='2023-01-10', freq='D')\n\ndate_range_daily\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n# Creating a date range with 2 days frequency\ndate_range_two_days = pd.date_range(start='2023-01-01', end='2023-01-10', freq='2D')\n\ndate_range_two_days\n\nDatetimeIndex(['2023-01-01', '2023-01-03', '2023-01-05', '2023-01-07',\n               '2023-01-09'],\n              dtype='datetime64[ns]', freq='2D')\n\n\nThese frequency strings help in resampling, creating date ranges, and handling time-series data efficiently in pandas.\n\n\n\n2 Timetk Incorporates Pandas Frequencies\nNow that you’ve seen pandas frequencies, you’ll see them pop up in many of the timetk functions.\n\nExample: Padding Dates\nThis example shows how to use Pandas frequencies inside of timetk functions.\nWe’ll use pad_by_time to show how to use freq to fill in missing dates.\n\n# DataFrame with missing dates\nimport pandas as pd\n\ndata = {\n    # '2023-09-05' is missing\n    'datetime': ['2023-09-01', '2023-09-02', '2023-09-03', '2023-09-04', '2023-09-06'],  \n    'value': [10, 30, 40, 50, 60]\n}\n\ndf = pd.DataFrame(data)\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01\n10\n\n\n1\n2023-09-02\n30\n\n\n2\n2023-09-03\n40\n\n\n3\n2023-09-04\n50\n\n\n4\n2023-09-06\n60\n\n\n\n\n\n\n\nWe can resample to fill in the missing day using pad_by_time with freq = 'D'.\n\nimport timetk as tk\n\ndf.pad_by_time('datetime', freq = 'D')\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01\n10.0\n\n\n1\n2023-09-02\n30.0\n\n\n2\n2023-09-03\n40.0\n\n\n3\n2023-09-04\n50.0\n\n\n4\n2023-09-05\nNaN\n\n\n5\n2023-09-06\n60.0\n\n\n\n\n\n\n\nWhat about resampling every 12 hours? Just set `freq = ‘12H’.\n\nimport timetk as tk\n\ndf.pad_by_time('datetime', freq = '12H')\n\n\n\n\n\n\n\n\ndatetime\nvalue\n\n\n\n\n0\n2023-09-01 00:00:00\n10.0\n\n\n1\n2023-09-01 12:00:00\nNaN\n\n\n2\n2023-09-02 00:00:00\n30.0\n\n\n3\n2023-09-02 12:00:00\nNaN\n\n\n4\n2023-09-03 00:00:00\n40.0\n\n\n5\n2023-09-03 12:00:00\nNaN\n\n\n6\n2023-09-04 00:00:00\n50.0\n\n\n7\n2023-09-04 12:00:00\nNaN\n\n\n8\n2023-09-05 00:00:00\nNaN\n\n\n9\n2023-09-05 12:00:00\nNaN\n\n\n10\n2023-09-06 00:00:00\n60.0\n\n\n\n\n\n\n\nYou’ll see these pandas frequencies come up as the parameter freq in many timetk functions."
  },
  {
    "objectID": "getting-started/01_installation.html",
    "href": "getting-started/01_installation.html",
    "title": "Install",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/01_installation.html#installation",
    "href": "getting-started/01_installation.html#installation",
    "title": "Install",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npoetry install\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "timetk for Python ",
    "section": "",
    "text": "The Time Series Toolkit for Python\nTimetk’s Mission: To make time series analysis easier, faster, and more enjoyable in Python."
  },
  {
    "objectID": "index.html#quick-start-a-monthly-sales-analysis",
    "href": "index.html#quick-start-a-monthly-sales-analysis",
    "title": "timetk for Python ",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        freq         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "timetk for Python ",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npoetry install\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "getting-started/02_quick_start.html",
    "href": "getting-started/02_quick_start.html",
    "title": "Quick Start",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "href": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "title": "Quick Start",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        freq         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "getting-started/02_quick_start.html#more-coming-soon",
    "href": "getting-started/02_quick_start.html#more-coming-soon",
    "title": "Quick Start",
    "section": "More coming soon…",
    "text": "More coming soon…\nThere’s a lot more coming in timetk for Python. You can check out our Project Roadmap here."
  },
  {
    "objectID": "guides/02_timetk_concepts.html",
    "href": "guides/02_timetk_concepts.html",
    "title": "Timetk Basics",
    "section": "",
    "text": "Timetk has one mission: To make time series analysis simpler, easier, and faster in Python. This goal requires some opinionated ways of treating time series in Python. We will conceptually lay out how timetk can help.\nLet’s first start with how to think about time series data conceptually. Time series data has 3 core properties."
  },
  {
    "objectID": "guides/02_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "href": "guides/02_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "title": "Timetk Basics",
    "section": "2.1 Type 1: Pandas DataFrame Operations",
    "text": "2.1 Type 1: Pandas DataFrame Operations\nBefore we start using timetk, let’s make sure our data is set up properly.\n\nTimetk Data Format Compliance\n\n\n\n\n\n\n3 Core Properties Must Be Upheald\n\n\n\n\n\nA Timetk-Compliant Pandas DataFrame must have:\n\nTime Series Index: A Time Stamp column containing datetime64 values\nValue Column(s): The value column(s) containing float or int values\nGroup Column(s): Optionally for grouped time series analysis, one or more columns containg str or categorical values (shown as an object)\n\nIf these are NOT upheld, this will impact your ability to use timetk DataFrame operations.\n\n\n\n\n\n\n\n\n\nInspect the DataFrame\n\n\n\n\n\nUse Pandas info() method to check compliance.\n\n\n\nUsing pandas info() method, we can see that we have a compliant data frame with a date column containing datetime64 and a value column containing float64. For grouped analysis we have the id column containing object dtype.\n\n# Tip: Inspect for compliance with info()\nm4_daily_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9743 entries, 0 to 9742\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   id      9743 non-null   object        \n 1   date    9743 non-null   datetime64[ns]\n 2   value   9743 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 228.5+ KB\n\n\n\n\nGrouped Time Series Analysis with Summarize By Time\nFirst, inspect how the summarize_by_time function works by calling help().\n\n# Review the summarize_by_time documentation (output not shown)\nhelp(tk.summarize_by_time)\n\n\n\n\n\n\n\nHelp Doc Info: summarize_by_time()\n\n\n\n\n\n\nThe first parameter is data, indicating this is a DataFrame operation.\nThe Examples show different use cases for how to apply the function on a DataFrame\n\n\n\n\nLet’s test the summarize_by_time() DataFrame operation out using the grouped approach with method chaining. DataFrame operations can be used as Pandas methods with method-chaining, which allows us to more succinctly apply time series operations.\n\n# Grouped Summarize By Time with Method Chaining\ndf_summarized = (\n    m4_daily_df\n        .groupby('id')\n        .summarize_by_time(\n            date_column  = 'date',\n            value_column = 'value',\n            freq         = 'QS', # QS = Quarter Start\n            agg_func     = [\n                'mean', \n                'median', \n                'min',\n                ('q25', lambda x: np.quantile(x, 0.25)),\n                ('q75', lambda x: np.quantile(x, 0.75)),\n                'max',\n                ('range',lambda x: x.max() - x.min()),\n            ],\n        )\n)\n\ndf_summarized\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_q25\nvalue_q75\nvalue_max\nvalue_range\n\n\n\n\n0\nD10\n2014-07-01\n1960.078889\n1979.90\n1781.6\n1915.225\n2002.575\n2076.2\n294.6\n\n\n1\nD10\n2014-10-01\n2184.586957\n2154.05\n2022.8\n2125.075\n2274.150\n2344.9\n322.1\n\n\n2\nD10\n2015-01-01\n2309.830000\n2312.30\n2209.6\n2284.575\n2342.150\n2392.4\n182.8\n\n\n3\nD10\n2015-04-01\n2344.481319\n2333.00\n2185.1\n2301.750\n2391.000\n2499.8\n314.7\n\n\n4\nD10\n2015-07-01\n2156.754348\n2186.70\n1856.6\n1997.250\n2289.425\n2368.1\n511.5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n105\nD500\n2011-07-01\n9727.321739\n9745.55\n8964.5\n9534.125\n10003.900\n10463.9\n1499.4\n\n\n106\nD500\n2011-10-01\n8175.565217\n7897.00\n6755.0\n7669.875\n8592.575\n9860.0\n3105.0\n\n\n107\nD500\n2012-01-01\n8291.317582\n8412.60\n7471.5\n7814.800\n8677.850\n8980.7\n1509.2\n\n\n108\nD500\n2012-04-01\n8654.020879\n8471.10\n8245.6\n8389.850\n9017.250\n9349.2\n1103.6\n\n\n109\nD500\n2012-07-01\n8770.502353\n8690.50\n8348.1\n8604.400\n8846.000\n9545.3\n1197.2\n\n\n\n\n110 rows × 9 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: summarize_by_time()\n\n\n\n\n\n\nThe data must comply with the 3 core properties (date column, value column(s), and group column(s))\nThe aggregation functions were applied by combination of group (id) and resample (Quarter Start)\nThe result was a pandas DataFrame with group column, resampled date column, and summary values (mean, median, min, 25th-quantile, etc)\n\n\n\n\n\n\nAnother DataFrame Example: Creating 29 Engineered Features\nLet’s examine another DataFrame function, tk.augment_timeseries_signature(). Feel free to inspect the documentation with help(tk.augment_timeseries_signature).\n\n# Creating 29 engineered features from the date column\n# Not run: help(tk.augment_timeseries_signature)\ndf_augmented = (\n    m4_daily_df\n        .augment_timeseries_signature(date_column = 'date')\n)\n\ndf_augmented.head()\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: augment_timeseries_signature()\n\n\n\n\n\n\nThe data must comply with the 1 of the 3 core properties (date column)\nThe result was a pandas DataFrame with 29 time series features that can be used for Machine Learning and Forecasting\n\n\n\n\n\n\nMaking Future Dates with Future Frame\nA common time series task before forecasting with machine learning models is to make a future DataFrame some length_out into the future. You can do this with tk.future_frame(). Here’s how.\n\n# Preparing a time series data set for Machine Learning Forecasting\nfull_augmented_df = (\n    m4_daily_df \n        .groupby('id')\n        .future_frame('date', length_out = 365)\n        .augment_timeseries_signature('date')\n)\nfull_augmented_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4556\nD500\n2013-09-19\nNaN\n1379548800\n2013\n2013\n0\n0\n0\n2\n...\n19\n81\n262\n0\n0\n0\n0\n0\n0\nam\n\n\n4557\nD500\n2013-09-20\nNaN\n1379635200\n2013\n2013\n0\n0\n0\n2\n...\n20\n82\n263\n0\n0\n0\n0\n0\n0\nam\n\n\n4558\nD500\n2013-09-21\nNaN\n1379721600\n2013\n2013\n0\n0\n0\n2\n...\n21\n83\n264\n0\n0\n0\n0\n0\n0\nam\n\n\n4559\nD500\n2013-09-22\nNaN\n1379808000\n2013\n2013\n0\n0\n0\n2\n...\n22\n84\n265\n1\n0\n0\n0\n0\n0\nam\n\n\n4560\nD500\n2013-09-23\nNaN\n1379894400\n2013\n2013\n0\n0\n0\n2\n...\n23\n85\n266\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n11203 rows × 32 columns\n\n\n\nWe can then get the future data by keying in on the data with value column that is missing (np.nan).\n\n# Get the future data (just the observations that haven't happened yet)\nfuture_df = (\n    full_augmented_df\n        .query('value.isna()')\n)\nfuture_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n674\nD10\n2016-05-07\nNaN\n1462579200\n2016\n2016\n0\n0\n1\n1\n...\n7\n37\n128\n0\n0\n0\n0\n0\n0\nam\n\n\n675\nD10\n2016-05-08\nNaN\n1462665600\n2016\n2016\n0\n0\n1\n1\n...\n8\n38\n129\n1\n0\n0\n0\n0\n0\nam\n\n\n676\nD10\n2016-05-09\nNaN\n1462752000\n2016\n2016\n0\n0\n1\n1\n...\n9\n39\n130\n0\n0\n0\n0\n0\n0\nam\n\n\n677\nD10\n2016-05-10\nNaN\n1462838400\n2016\n2016\n0\n0\n1\n1\n...\n10\n40\n131\n0\n0\n0\n0\n0\n0\nam\n\n\n678\nD10\n2016-05-11\nNaN\n1462924800\n2016\n2016\n0\n0\n1\n1\n...\n11\n41\n132\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4556\nD500\n2013-09-19\nNaN\n1379548800\n2013\n2013\n0\n0\n0\n2\n...\n19\n81\n262\n0\n0\n0\n0\n0\n0\nam\n\n\n4557\nD500\n2013-09-20\nNaN\n1379635200\n2013\n2013\n0\n0\n0\n2\n...\n20\n82\n263\n0\n0\n0\n0\n0\n0\nam\n\n\n4558\nD500\n2013-09-21\nNaN\n1379721600\n2013\n2013\n0\n0\n0\n2\n...\n21\n83\n264\n0\n0\n0\n0\n0\n0\nam\n\n\n4559\nD500\n2013-09-22\nNaN\n1379808000\n2013\n2013\n0\n0\n0\n2\n...\n22\n84\n265\n1\n0\n0\n0\n0\n0\nam\n\n\n4560\nD500\n2013-09-23\nNaN\n1379894400\n2013\n2013\n0\n0\n0\n2\n...\n23\n85\n266\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n1460 rows × 32 columns"
  },
  {
    "objectID": "guides/02_timetk_concepts.html#type-2-pandas-series-operations",
    "href": "guides/02_timetk_concepts.html#type-2-pandas-series-operations",
    "title": "Timetk Basics",
    "section": "2.2 Type 2: Pandas Series Operations",
    "text": "2.2 Type 2: Pandas Series Operations\nThe main difference between a DataFrame operation and a Series operation is that we are operating on an array of values from typically one of the following dtypes:\n\nTimestamps (datetime64)\nNumeric (float64 or int64)\n\nThe first argument of Series operations that operate on Timestamps will always be idx.\nLet’s take a look at one shall we? We’ll start with a common action: Making future time series from an existing time series with a regular frequency.\n\nThe Make Future Time Series Function\nSay we have a monthly sequence of timestamps. What if we want to create a forecast where we predict 12 months into the future? Well, we will need to create 12 future timestamps. Here’s how.\nFirst create a pd.date_range() with dates starting at the beginning of each month.\n\n# Make a monthly date range\ndates_dt = pd.date_range(\"2023-01\", \"2024-01\", freq=\"MS\")\ndates_dt\n\nDatetimeIndex(['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01',\n               '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01',\n               '2023-09-01', '2023-10-01', '2023-11-01', '2023-12-01',\n               '2024-01-01'],\n              dtype='datetime64[ns]', freq='MS')\n\n\nNext, use tk.make_future_timeseries() to create the next 12 timestamps in the sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas Series: Future Dates\nfuture_series = pd.Series(dates_dt).make_future_timeseries(12)\nfuture_series\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\n# DateTimeIndex: Future Dates\nfuture_dt = tk.make_future_timeseries(\n    idx      = dates_dt,\n    length_out = 12\n)\nfuture_dt\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\nWe can combine the actual and future timestamps into one combined timeseries.\n\n# Combining the 2 series and resetting the index\ncombined_timeseries = (\n    pd.concat(\n        [pd.Series(dates_dt), pd.Series(future_dt)],\n        axis=0\n    )\n        .reset_index(drop = True)\n)\n\ncombined_timeseries\n\n0    2023-01-01\n1    2023-02-01\n2    2023-03-01\n3    2023-04-01\n4    2023-05-01\n5    2023-06-01\n6    2023-07-01\n7    2023-08-01\n8    2023-09-01\n9    2023-10-01\n10   2023-11-01\n11   2023-12-01\n12   2024-01-01\n13   2024-02-01\n14   2024-03-01\n15   2024-04-01\n16   2024-05-01\n17   2024-06-01\n18   2024-07-01\n19   2024-08-01\n20   2024-09-01\n21   2024-10-01\n22   2024-11-01\n23   2024-12-01\n24   2025-01-01\ndtype: datetime64[ns]\n\n\nNext, we’ll take a look at how to go from an irregular time series to a regular time series.\n\n\nFlooring Dates\nAn example is tk.floor_date, which is used to round down dates. See help(tk.floor_date).\nFlooring dates is often used as part of a strategy to go from an irregular time series to regular by combining with an aggregation. Often summarize_by_time() is used (I’ll share why shortly). But conceptually, date flooring is the secret.\n\nWith FlooringWithout Flooring\n\n\n\n# Monthly flooring rounds dates down to 1st of the month\nm4_daily_df['date'].floor_date(unit = \"M\")\n\n0      2014-07-01\n1      2014-07-01\n2      2014-07-01\n3      2014-07-01\n4      2014-07-01\n          ...    \n9738   2012-09-01\n9739   2012-09-01\n9740   2012-09-01\n9741   2012-09-01\n9742   2012-09-01\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\n# Before Flooring\nm4_daily_df['date']\n\n0      2014-07-03\n1      2014-07-04\n2      2014-07-05\n3      2014-07-06\n4      2014-07-07\n          ...    \n9738   2012-09-19\n9739   2012-09-20\n9740   2012-09-21\n9741   2012-09-22\n9742   2012-09-23\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\nThis “date flooring” operation can be useful for creating date groupings.\n\n# Adding a date group with floor_date()\ndates_grouped_by_month = (\n    m4_daily_df\n        .assign(date_group = lambda x: x['date'].floor_date(\"M\"))\n)\n\ndates_grouped_by_month\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_group\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2014-07-01\n\n\n1\nD10\n2014-07-04\n2073.4\n2014-07-01\n\n\n2\nD10\n2014-07-05\n2048.7\n2014-07-01\n\n\n3\nD10\n2014-07-06\n2048.9\n2014-07-01\n\n\n4\nD10\n2014-07-07\n2006.4\n2014-07-01\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n2012-09-01\n\n\n9739\nD500\n2012-09-20\n9365.7\n2012-09-01\n\n\n9740\nD500\n2012-09-21\n9445.9\n2012-09-01\n\n\n9741\nD500\n2012-09-22\n9497.9\n2012-09-01\n\n\n9742\nD500\n2012-09-23\n9545.3\n2012-09-01\n\n\n\n\n9743 rows × 4 columns\n\n\n\nWe can then do grouped operations.\n\n# Example of a grouped operation with floored dates\nsummary_df = (\n    dates_grouped_by_month\n        .drop('date', axis=1) \\\n        .groupby(['id', 'date_group'])\n        .mean() \\\n        .reset_index()\n)\n\nsummary_df\n\n\n\n\n\n\n\n\nid\ndate_group\nvalue\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n\n\n1\nD10\n2014-08-01\n1985.548387\n\n\n2\nD10\n2014-09-01\n1926.593333\n\n\n3\nD10\n2014-10-01\n2100.077419\n\n\n4\nD10\n2014-11-01\n2155.326667\n\n\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n\n\n319\nD500\n2012-06-01\n9124.903333\n\n\n320\nD500\n2012-07-01\n8674.551613\n\n\n321\nD500\n2012-08-01\n8666.054839\n\n\n322\nD500\n2012-09-01\n9040.604348\n\n\n\n\n323 rows × 3 columns\n\n\n\nOf course for this operation, we can do it faster with summarize_by_time() (and it’s much more flexible).\n\n# Summarize by time is less code and more flexible\n(\n    m4_daily_df \n        .groupby('id')\n        .summarize_by_time(\n            'date', 'value', \n            freq = \"MS\",\n            agg_func = ['mean', 'median', 'min', 'max']\n        )\n)\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_max\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n1978.80\n1876.0\n2076.2\n\n\n1\nD10\n2014-08-01\n1985.548387\n1995.60\n1914.7\n2027.5\n\n\n2\nD10\n2014-09-01\n1926.593333\n1920.95\n1781.6\n2023.5\n\n\n3\nD10\n2014-10-01\n2100.077419\n2107.60\n2022.8\n2154.9\n\n\n4\nD10\n2014-11-01\n2155.326667\n2149.30\n2083.5\n2245.4\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n8430.80\n8245.6\n8578.1\n\n\n319\nD500\n2012-06-01\n9124.903333\n9163.85\n8686.1\n9349.2\n\n\n320\nD500\n2012-07-01\n8674.551613\n8673.60\n8407.5\n9091.1\n\n\n321\nD500\n2012-08-01\n8666.054839\n8667.40\n8348.1\n8939.6\n\n\n322\nD500\n2012-09-01\n9040.604348\n9091.40\n8500.0\n9545.3\n\n\n\n\n323 rows × 6 columns\n\n\n\nAnd that’s the core idea behind timetk, writing less code and getting more.\nNext, let’s do one more function. The brother of augment_timeseries_signature()…\n\n\nThe Get Time Series Signature Function\nThis function takes a pandas Series or DateTimeIndex and returns a DataFrame containing the 29 engineered features.\nStart with either a DateTimeIndex…\n\ntimestamps_dt = pd.date_range(\"2023\", \"2024\", freq = \"D\")\ntimestamps_dt\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10',\n               ...\n               '2023-12-23', '2023-12-24', '2023-12-25', '2023-12-26',\n               '2023-12-27', '2023-12-28', '2023-12-29', '2023-12-30',\n               '2023-12-31', '2024-01-01'],\n              dtype='datetime64[ns]', length=366, freq='D')\n\n\n… Or a Pandas Series.\n\ntimestamps_series = pd.Series(timestamps_dt)\ntimestamps_series\n\n0     2023-01-01\n1     2023-01-02\n2     2023-01-03\n3     2023-01-04\n4     2023-01-05\n         ...    \n361   2023-12-28\n362   2023-12-29\n363   2023-12-30\n364   2023-12-31\n365   2024-01-01\nLength: 366, dtype: datetime64[ns]\n\n\nAnd you can use the pandas Series function, tk.get_timeseries_signature() to create 29 features from the date sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas series: get_timeseries_signature\ntimestamps_series.get_timeseries_signature()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns\n\n\n\n\n\n\n# DateTimeIndex: get_timeseries_signature\ntk.get_timeseries_signature(timestamps_dt)\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns"
  },
  {
    "objectID": "guides/05_augmenting.html",
    "href": "guides/05_augmenting.html",
    "title": "Adding Features (Augmenting)",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "reference/pad_by_time.html",
    "href": "reference/pad_by_time.html",
    "title": "pad_by_time",
    "section": "",
    "text": "pad_by_time(data, date_column, freq='auto', force_regular=True)\nMake irregular time series regular by padding with missing dates.\nThe pad_by_time function inserts missing dates into a Pandas DataFrame or DataFrameGroupBy object, through the process making an irregularly spaced time series regularly spaced."
  },
  {
    "objectID": "reference/pad_by_time.html#parameters",
    "href": "reference/pad_by_time.html#parameters",
    "title": "pad_by_time",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter can be either a Pandas DataFrame or a Pandas DataFrameGroupBy object. It represents the data that you want to pad with missing dates.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to determine the minimum and maximum dates in theDataFrame, and to generate the regular date range for padding.\nrequired\n\n\nfreq\nstr\nThe freq parameter specifies the frequency at which the missing timestamps should be generated. It accepts a string representing a pandas frequency alias. Automatic Frequency Detection: - \"auto\": Automatically detect the frequency of the data. This will default allow regular frequencies (i.e. no business days). This is the default value. This can be changed with the force_regular parameter. You can override this with a pandas frequency alias. Some common frequency aliases include: - S: secondly frequency - min: minute frequency - H: hourly frequency - B: business day frequency - D: daily frequency - W: weekly frequency - M: month end frequency - MS: month start frequency - BMS: Business month start - Q: quarter end frequency - QS: quarter start frequency - Y: year end frequency - YS: year start frequency\n'auto'\n\n\nforce_regular\nbool\nThe force_regular parameter is a boolean that specifies whether the frequency should be forced to be regular. This parameter is only used when the freq parameter is set to \"auto\". It has a default value of True. If force_regular is True, then the freq parameter will be forced to be a regular frequency. If force_regular is False, then the freq parameter will be allowed to be irregular (i.e. business calendars can be used).\nTrue"
  },
  {
    "objectID": "reference/pad_by_time.html#returns",
    "href": "reference/pad_by_time.html#returns",
    "title": "pad_by_time",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe function pad_by_time returns a Pandas DataFrame that has been extended with future dates."
  },
  {
    "objectID": "reference/pad_by_time.html#examples",
    "href": "reference/pad_by_time.html#examples",
    "title": "pad_by_time",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Pad Single Time Series: Fill missing dates\npadded_df = (\n    df\n        .query('symbol == \"AAPL\"')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'auto'\n        )\n        .assign(id = lambda x: x['symbol'].ffill())\n)\npadded_df \n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\nid\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\nAAPL\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\nAAPL\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\nAAPL\n\n\n3\n2013-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n4\n2013-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3910\n2023-09-17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n3911\n2023-09-18\nAAPL\n176.479996\n179.380005\n176.169998\n177.970001\n67257600.0\n177.970001\nAAPL\n\n\n3912\n2023-09-19\nAAPL\n177.520004\n179.630005\n177.130005\n179.070007\n51826900.0\n179.070007\nAAPL\n\n\n3913\n2023-09-20\nAAPL\n179.259995\n179.699997\n175.399994\n175.490005\n58436200.0\n175.490005\nAAPL\n\n\n3914\n2023-09-21\nAAPL\n174.550003\n176.300003\n173.860001\n173.929993\n63047900.0\n173.929993\nAAPL\n\n\n\n\n3915 rows × 9 columns\n\n\n\n\n# Pad Single Time Series: Fill missing dates\npadded_df = (\n    df\n        .query('symbol == \"AAPL\"')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'D'\n        )\n        .assign(id = lambda x: x['symbol'].ffill())\n)\npadded_df \n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\nid\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\nAAPL\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\nAAPL\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\nAAPL\n\n\n3\n2013-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n4\n2013-01-06\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3910\n2023-09-17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAAPL\n\n\n3911\n2023-09-18\nAAPL\n176.479996\n179.380005\n176.169998\n177.970001\n67257600.0\n177.970001\nAAPL\n\n\n3912\n2023-09-19\nAAPL\n177.520004\n179.630005\n177.130005\n179.070007\n51826900.0\n179.070007\nAAPL\n\n\n3913\n2023-09-20\nAAPL\n179.259995\n179.699997\n175.399994\n175.490005\n58436200.0\n175.490005\nAAPL\n\n\n3914\n2023-09-21\nAAPL\n174.550003\n176.300003\n173.860001\n173.929993\n63047900.0\n173.929993\nAAPL\n\n\n\n\n3915 rows × 9 columns\n\n\n\n\n# Pad by Group: Pad each group with missing dates\npadded_df = (\n    df\n        .groupby('symbol')\n        .pad_by_time(\n            date_column = 'date',\n            freq        = 'D'\n        )\n)\npadded_df\n\n\n\n\n\n\n\n\ndate\nsymbol\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\n2013-01-02\nAAPL\n19.779285\n19.821428\n19.343929\n19.608213\n560518000.0\n16.791180\n\n\n1\n2013-01-03\nAAPL\n19.567142\n19.631071\n19.321428\n19.360714\n352965200.0\n16.579241\n\n\n2\n2013-01-04\nAAPL\n19.177500\n19.236786\n18.779642\n18.821428\n594333600.0\n16.117437\n\n\n3\n2013-01-05\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2013-01-06\nAAPL\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n23485\n2023-09-17\nNVDA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n23486\n2023-09-18\nNVDA\n427.480011\n442.420013\n420.000000\n439.660004\n50027100.0\n439.660004\n\n\n23487\n2023-09-19\nNVDA\n438.329987\n439.660004\n430.019989\n435.200012\n37306400.0\n435.200012\n\n\n23488\n2023-09-20\nNVDA\n436.000000\n439.029999\n422.230011\n422.390015\n36710800.0\n422.390015\n\n\n23489\n2023-09-21\nNVDA\n415.829987\n421.000000\n409.799988\n410.170013\n44893000.0\n410.170013\n\n\n\n\n23490 rows × 8 columns"
  },
  {
    "objectID": "reference/is_holiday.html",
    "href": "reference/is_holiday.html",
    "title": "is_holiday",
    "section": "",
    "text": "is_holiday(idx, country_name='UnitedStates', country=None)\nCheck if a given list of dates are holidays for a specified country.\nNote: This function requires the holidays package to be installed."
  },
  {
    "objectID": "reference/is_holiday.html#parameters",
    "href": "reference/is_holiday.html#parameters",
    "title": "is_holiday",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\nUnion[str, datetime, List[Union[str, datetime]], pd.DatetimeIndex, pd.Series]\nThe dates to check for holiday status.\nrequired\n\n\ncountry_name\nstr\nThe name of the country for which to check the holiday status. Defaults to ‘UnitedStates’ if not specified.\n'UnitedStates'\n\n\ncountry\nstr\nAn alternative parameter to specify the country for holiday checking, overriding country_name.\nNone"
  },
  {
    "objectID": "reference/is_holiday.html#returns",
    "href": "reference/is_holiday.html#returns",
    "title": "is_holiday",
    "section": "Returns:",
    "text": "Returns:\npd.Series: Series containing True if the date is a holiday, False otherwise."
  },
  {
    "objectID": "reference/is_holiday.html#raises",
    "href": "reference/is_holiday.html#raises",
    "title": "is_holiday",
    "section": "Raises:",
    "text": "Raises:\nValueError: If the specified country is not found in the holidays package."
  },
  {
    "objectID": "reference/is_holiday.html#examples",
    "href": "reference/is_holiday.html#examples",
    "title": "is_holiday",
    "section": "Examples:",
    "text": "Examples:\n\nimport pandas as pd\nimport timetk as tk\n\ntk.is_holiday('2023-01-01', country_name='UnitedStates')\n\n0    True\nName: is_holiday, dtype: bool\n\n\n\n# List of dates\ntk.is_holiday(['2023-01-01', '2023-01-02', '2023-01-03'], country_name='UnitedStates')\n\n0     True\n1     True\n2    False\nName: is_holiday, dtype: bool\n\n\n\n# DatetimeIndex\ntk.is_holiday(pd.date_range(\"2023-01-01\", \"2023-01-03\"), country_name='UnitedStates')\n\n0     True\n1     True\n2    False\nName: is_holiday, dtype: bool\n\n\n\n# Pandas Series Method\n( \n    pd.Series(pd.date_range(\"2023-01-01\", \"2023-01-03\"))\n        .is_holiday(country_name='UnitedStates')\n)\n\n0     True\n1     True\n2    False\nName: is_holiday, dtype: bool"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Bend time series data to your will.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\npad_by_time\nMake irregular time series regular by padding with missing dates.\n\n\nfuture_frame\nExtend a DataFrame or GroupBy object with future dates.\n\n\n\n\n\n\nAdd one or more feature columns to time series data.\n\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame.\n\n\naugment_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries.\n\n\naugment_lags\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_leads\nAdds leads to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_rolling\nApply one or more rolling functions and window sizes to one or more columns of a DataFrame.\n\n\n\n\n\n\nPython implementation of the R package tsfeatures.\n\n\n\nts_features\nExtracts aggregated time series features from a DataFrame or DataFrameGroupBy object using the tsfeatures package.\n\n\n\n\n\n\nTime series functions that generate / manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nmake_weekday_sequence\nGenerate a sequence of weekday dates within a specified date range, optionally excluding weekends and holidays.\n\n\nmake_weekend_sequence\nGenerate a sequence of weekend dates within a specified date range, optionally excluding holidays.\n\n\n\n\n\n\nHelper functions to make your life easier.\n\n\n\nget_pandas_frequency\nGet the frequency of a pandas Series or DatetimeIndex.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nget_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries.\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nis_holiday\nCheck if a given list of dates are holidays for a specified country.\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month.\n\n\n\n\n\n\nPractice timetk with 12 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 12 datasets that can be loaded with timetk.load_dataset.\n\n\nload_dataset\nLoad one of 12 Time Series Datasets."
  },
  {
    "objectID": "reference/index.html#wrangling-pandas-time-series-dataframes",
    "href": "reference/index.html#wrangling-pandas-time-series-dataframes",
    "title": "Function reference",
    "section": "",
    "text": "Bend time series data to your will.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\npad_by_time\nMake irregular time series regular by padding with missing dates.\n\n\nfuture_frame\nExtend a DataFrame or GroupBy object with future dates."
  },
  {
    "objectID": "reference/index.html#adding-features-to-time-series-dataframes-augmenting",
    "href": "reference/index.html#adding-features-to-time-series-dataframes-augmenting",
    "title": "Function reference",
    "section": "",
    "text": "Add one or more feature columns to time series data.\n\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame.\n\n\naugment_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries.\n\n\naugment_lags\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_leads\nAdds leads to a Pandas DataFrame or DataFrameGroupBy object.\n\n\naugment_rolling\nApply one or more rolling functions and window sizes to one or more columns of a DataFrame."
  },
  {
    "objectID": "reference/index.html#ts-features",
    "href": "reference/index.html#ts-features",
    "title": "Function reference",
    "section": "",
    "text": "Python implementation of the R package tsfeatures.\n\n\n\nts_features\nExtracts aggregated time series features from a DataFrame or DataFrameGroupBy object using the tsfeatures package."
  },
  {
    "objectID": "reference/index.html#time-series-for-pandas-series",
    "href": "reference/index.html#time-series-for-pandas-series",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that generate / manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nmake_weekday_sequence\nGenerate a sequence of weekday dates within a specified date range, optionally excluding weekends and holidays.\n\n\nmake_weekend_sequence\nGenerate a sequence of weekend dates within a specified date range, optionally excluding holidays."
  },
  {
    "objectID": "reference/index.html#utilities",
    "href": "reference/index.html#utilities",
    "title": "Function reference",
    "section": "",
    "text": "Helper functions to make your life easier.\n\n\n\nget_pandas_frequency\nGet the frequency of a pandas Series or DatetimeIndex.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nget_holiday_signature\nEngineers 4 different holiday features from a single datetime for 80+ countries.\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nis_holiday\nCheck if a given list of dates are holidays for a specified country.\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month."
  },
  {
    "objectID": "reference/index.html#datasets",
    "href": "reference/index.html#datasets",
    "title": "Function reference",
    "section": "",
    "text": "Practice timetk with 12 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 12 datasets that can be loaded with timetk.load_dataset.\n\n\nload_dataset\nLoad one of 12 Time Series Datasets."
  },
  {
    "objectID": "reference/week_of_month.html",
    "href": "reference/week_of_month.html",
    "title": "week_of_month",
    "section": "",
    "text": "week_of_month(idx)\nThe “week_of_month” function calculates the week number of a given date within its month."
  },
  {
    "objectID": "reference/week_of_month.html#parameters",
    "href": "reference/week_of_month.html#parameters",
    "title": "week_of_month",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe parameter “idx” is a pandas Series object that represents a specific date for which you want to determine the week of the month.\nrequired"
  },
  {
    "objectID": "reference/week_of_month.html#returns",
    "href": "reference/week_of_month.html#returns",
    "title": "week_of_month",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe week of the month for a given date."
  },
  {
    "objectID": "reference/week_of_month.html#examples",
    "href": "reference/week_of_month.html#examples",
    "title": "week_of_month",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"1D\")\ndates\n\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10', '2020-01-11', '2020-01-12',\n               '2020-01-13', '2020-01-14', '2020-01-15', '2020-01-16',\n               '2020-01-17', '2020-01-18', '2020-01-19', '2020-01-20',\n               '2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24',\n               '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28',\n               '2020-01-29', '2020-01-30', '2020-01-31', '2020-02-01',\n               '2020-02-02', '2020-02-03', '2020-02-04', '2020-02-05',\n               '2020-02-06', '2020-02-07', '2020-02-08', '2020-02-09',\n               '2020-02-10', '2020-02-11', '2020-02-12', '2020-02-13',\n               '2020-02-14', '2020-02-15', '2020-02-16', '2020-02-17',\n               '2020-02-18', '2020-02-19', '2020-02-20', '2020-02-21',\n               '2020-02-22', '2020-02-23', '2020-02-24', '2020-02-25',\n               '2020-02-26', '2020-02-27', '2020-02-28'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n# Works on DateTimeIndex\ntk.week_of_month(dates)\n\n0     1\n1     1\n2     1\n3     1\n4     1\n5     1\n6     1\n7     2\n8     2\n9     2\n10    2\n11    2\n12    2\n13    2\n14    3\n15    3\n16    3\n17    3\n18    3\n19    3\n20    3\n21    4\n22    4\n23    4\n24    4\n25    4\n26    4\n27    4\n28    5\n29    5\n30    5\n31    1\n32    1\n33    1\n34    1\n35    1\n36    1\n37    1\n38    2\n39    2\n40    2\n41    2\n42    2\n43    2\n44    2\n45    3\n46    3\n47    3\n48    3\n49    3\n50    3\n51    3\n52    4\n53    4\n54    4\n55    4\n56    4\n57    4\n58    4\nName: week_of_month, dtype: int32\n\n\n\n# Works on Pandas Series\ndates.to_series().week_of_month()\n\n2020-01-01    1\n2020-01-02    1\n2020-01-03    1\n2020-01-04    1\n2020-01-05    1\n2020-01-06    1\n2020-01-07    1\n2020-01-08    2\n2020-01-09    2\n2020-01-10    2\n2020-01-11    2\n2020-01-12    2\n2020-01-13    2\n2020-01-14    2\n2020-01-15    3\n2020-01-16    3\n2020-01-17    3\n2020-01-18    3\n2020-01-19    3\n2020-01-20    3\n2020-01-21    3\n2020-01-22    4\n2020-01-23    4\n2020-01-24    4\n2020-01-25    4\n2020-01-26    4\n2020-01-27    4\n2020-01-28    4\n2020-01-29    5\n2020-01-30    5\n2020-01-31    5\n2020-02-01    1\n2020-02-02    1\n2020-02-03    1\n2020-02-04    1\n2020-02-05    1\n2020-02-06    1\n2020-02-07    1\n2020-02-08    2\n2020-02-09    2\n2020-02-10    2\n2020-02-11    2\n2020-02-12    2\n2020-02-13    2\n2020-02-14    2\n2020-02-15    3\n2020-02-16    3\n2020-02-17    3\n2020-02-18    3\n2020-02-19    3\n2020-02-20    3\n2020-02-21    3\n2020-02-22    4\n2020-02-23    4\n2020-02-24    4\n2020-02-25    4\n2020-02-26    4\n2020-02-27    4\n2020-02-28    4\nFreq: D, Name: week_of_month, dtype: int32"
  },
  {
    "objectID": "reference/make_weekday_sequence.html",
    "href": "reference/make_weekday_sequence.html",
    "title": "make_weekday_sequence",
    "section": "",
    "text": "make_weekday_sequence(start_date, end_date, sunday_to_thursday=False, remove_holidays=False, country=None)\nGenerate a sequence of weekday dates within a specified date range, optionally excluding weekends and holidays."
  },
  {
    "objectID": "reference/make_weekday_sequence.html#parameters",
    "href": "reference/make_weekday_sequence.html#parameters",
    "title": "make_weekday_sequence",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nstr or datetime or pd.DatetimeIndex\nThe start date of the date range.\nrequired\n\n\nend_date\nstr or datetime or pd.DatetimeIndex\nThe end date of the date range.\nrequired\n\n\nsunday_to_thursday\nbool\nIf True, generates a sequence with Sunday to Thursday weekdays (excluding Friday and Saturday). If False (default), generates a sequence with Monday to Friday weekdays.\nFalse\n\n\nremove_holidays\n(bool, optional)\nIf True, excludes holidays (based on the specified country) from the generated sequence. If False (default), includes holidays in the sequence.\nFalse\n\n\ncountry\nstr\nThe name of the country for which to generate holiday-specific sequences. Defaults to None, which uses the United States as the default country.\nNone"
  },
  {
    "objectID": "reference/make_weekday_sequence.html#returns",
    "href": "reference/make_weekday_sequence.html#returns",
    "title": "make_weekday_sequence",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA Series containing the generated weekday dates."
  },
  {
    "objectID": "reference/make_weekday_sequence.html#examples",
    "href": "reference/make_weekday_sequence.html#examples",
    "title": "make_weekday_sequence",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\n# United States has Monday to Friday as weekdays (excluding Saturday and Sunday and holidays)\ntk.make_weekday_sequence(\"2023-01-01\", \"2023-01-15\", sunday_to_thursday=False, remove_holidays=True, country='UnitedStates')\n\n0   2023-01-03\n1   2023-01-04\n2   2023-01-05\n3   2023-01-06\n4   2023-01-09\n5   2023-01-10\n6   2023-01-11\n7   2023-01-12\n8   2023-01-13\nName: Weekday Dates, dtype: datetime64[ns]\n\n\n\n# Israel has Sunday to Thursday as weekdays (excluding Friday and Saturday and Israel holidays)\ntk.make_weekday_sequence(\"2023-01-01\", \"2023-01-15\", sunday_to_thursday=True, remove_holidays=True, country='Israel')\n\n0    2023-01-01\n1    2023-01-02\n2    2023-01-03\n3    2023-01-04\n4    2023-01-05\n5    2023-01-08\n6    2023-01-09\n7    2023-01-10\n8    2023-01-11\n9    2023-01-12\n10   2023-01-15\nName: Weekday Dates, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "datasets.get_datasets.load_dataset(name='m4_daily', verbose=False, **kwargs)\nLoad one of 12 Time Series Datasets.\nThe load_dataset function is used to load various time series datasets by name, with options to print the available datasets and pass additional arguments to pandas.read_csv. The available datasets are:\nThe datasets can be loaded with timetk.load_dataset(name), where name is the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned above."
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name parameter is used to specify the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned in the function’s docstring.\n'm4_daily'\n\n\nverbose\nbool\nThe verbose parameter is a boolean flag that determines whether or not to print the names of the available datasets. If verbose is set to True, the function will print the names of the available datasets. If verbose is set to False, the function will not print anything.\nFalse\n\n\n**kwargs\n\nThe **kwargs parameter is used to pass additional arguments to pandas.read_csv.\n{}"
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe load_dataset function returns the requested dataset as a pandas DataFrame."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\n\n# Stocks Daily Dataset: META, APPL, AMZN, NFLX, NVDA, GOOG\ndf = tk.load_dataset('stocks_daily', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\n0\nMETA\n2013-01-02\n27.440001\n28.180000\n27.420000\n28.000000\n69846400\n28.000000\n\n\n1\nMETA\n2013-01-03\n27.879999\n28.469999\n27.590000\n27.770000\n63140600\n27.770000\n\n\n2\nMETA\n2013-01-04\n28.010000\n28.930000\n27.830000\n28.760000\n72715400\n28.760000\n\n\n3\nMETA\n2013-01-07\n28.690001\n29.790001\n28.650000\n29.420000\n83781800\n29.420000\n\n\n4\nMETA\n2013-01-08\n29.510000\n29.600000\n28.860001\n29.059999\n45871300\n29.059999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16189\nGOOG\n2023-09-15\n138.800003\n139.360001\n137.179993\n138.300003\n48947600\n138.300003\n\n\n16190\nGOOG\n2023-09-18\n137.630005\n139.929993\n137.630005\n138.960007\n16233600\n138.960007\n\n\n16191\nGOOG\n2023-09-19\n138.250000\n139.175003\n137.500000\n138.830002\n15479100\n138.830002\n\n\n16192\nGOOG\n2023-09-20\n138.830002\n138.839996\n134.520004\n134.589996\n21473500\n134.589996\n\n\n16193\nGOOG\n2023-09-21\n132.389999\n133.190002\n131.089996\n131.360001\n22042700\n131.360001\n\n\n\n\n16194 rows × 8 columns\n\n\n\n\n# Bike Sales CRM Sample Dataset\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Taylor 30-Minute Power Demand Dataset\ndf = tk.load_dataset('taylor_30_min', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2000-06-05 00:00:00+00:00\n22262\n\n\n1\n2000-06-05 00:30:00+00:00\n21756\n\n\n2\n2000-06-05 01:00:00+00:00\n22247\n\n\n3\n2000-06-05 01:30:00+00:00\n22759\n\n\n4\n2000-06-05 02:00:00+00:00\n22549\n\n\n...\n...\n...\n\n\n4027\n2000-08-27 21:30:00+00:00\n27946\n\n\n4028\n2000-08-27 22:00:00+00:00\n27133\n\n\n4029\n2000-08-27 22:30:00+00:00\n25996\n\n\n4030\n2000-08-27 23:00:00+00:00\n24610\n\n\n4031\n2000-08-27 23:30:00+00:00\n23132\n\n\n\n\n4032 rows × 2 columns"
  },
  {
    "objectID": "reference/make_weekend_sequence.html",
    "href": "reference/make_weekend_sequence.html",
    "title": "make_weekend_sequence",
    "section": "",
    "text": "make_weekend_sequence(start_date, end_date, friday_saturday=False, remove_holidays=False, country=None)\nGenerate a sequence of weekend dates within a specified date range, optionally excluding holidays."
  },
  {
    "objectID": "reference/make_weekend_sequence.html#parameters",
    "href": "reference/make_weekend_sequence.html#parameters",
    "title": "make_weekend_sequence",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart_date\nstr or datetime or pd.DatetimeIndex\nThe start date of the date range.\nrequired\n\n\nend_date\nstr or datetime or pd.DatetimeIndex\nThe end date of the date range.\nrequired\n\n\nfriday_saturday\nbool\nIf True, generates a sequence with Friday and Saturday as weekends.If False (default), generates a sequence with Saturday and Sunday as weekends.\nFalse\n\n\nremove_holidays\nbool\nIf True, excludes holidays (based on the specified country) from the generated sequence. If False (default), includes holidays in the sequence.\nFalse\n\n\ncountry\nstr\nThe name of the country for which to generate holiday-specific sequences. Defaults to None, which uses the United States as the default country.\nNone"
  },
  {
    "objectID": "reference/make_weekend_sequence.html#returns",
    "href": "reference/make_weekend_sequence.html#returns",
    "title": "make_weekend_sequence",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA Series containing the generated weekday dates."
  },
  {
    "objectID": "reference/make_weekend_sequence.html#examples",
    "href": "reference/make_weekend_sequence.html#examples",
    "title": "make_weekend_sequence",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\n# United States has Saturday and Sunday as weekends\ntk.make_weekend_sequence(\"2023-01-01\", \"2023-01-31\", friday_saturday=False, remove_holidays=True, country='UnitedStates')\n\n0   2023-01-07\n1   2023-01-08\n2   2023-01-14\n3   2023-01-15\n4   2023-01-21\n5   2023-01-22\n6   2023-01-28\n7   2023-01-29\nName: Weekend Dates, dtype: datetime64[ns]\n\n\n\n# Saudi Arabia has Friday and Saturday as weekends\ntk.make_weekend_sequence(\"2023-01-01\", \"2023-01-31\", friday_saturday=True, remove_holidays=True, country='SaudiArabia')\n\n0   2023-01-06\n1   2023-01-07\n2   2023-01-13\n3   2023-01-14\n4   2023-01-20\n5   2023-01-21\n6   2023-01-27\n7   2023-01-28\nName: Weekend Dates, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html",
    "href": "reference/augment_timeseries_signature.html",
    "title": "augment_timeseries_signature",
    "section": "",
    "text": "augment_timeseries_signature(data, date_column)\nAdd 29 time series features to a DataFrame.\nThe function augment_timeseries_signature takes a DataFrame and a date column as input and returns the original DataFrame with the 29 different date and time based features added as new columns:"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#parameters",
    "href": "reference/augment_timeseries_signature.html#parameters",
    "title": "augment_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe data parameter is a pandas DataFrame that contains the time series data.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that represents the name of the date column in the data DataFrame.\nrequired"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#returns",
    "href": "reference/augment_timeseries_signature.html#returns",
    "title": "augment_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA pandas DataFrame that is the concatenation of the original data DataFrame and the ts_signature_df DataFrame."
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#examples",
    "href": "reference/augment_timeseries_signature.html#examples",
    "title": "augment_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\n# Adds 29 new time series features as columns to the original DataFrame\n( \n    tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n        .augment_timeseries_signature(date_column = 'order_date')\n        .head()\n)\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\norder_date_leapyear\norder_date_half\norder_date_quarter\norder_date_quarteryear\norder_date_quarterstart\norder_date_quarterend\norder_date_month\norder_date_month_lbl\norder_date_monthstart\norder_date_monthend\norder_date_yweek\norder_date_mweek\norder_date_wday\norder_date_wday_lbl\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/augment_lags.html",
    "href": "reference/augment_lags.html",
    "title": "augment_lags",
    "section": "",
    "text": "augment_lags(data, date_column, value_column, lags=1)\nAdds lags to a Pandas DataFrame or DataFrameGroupBy object.\nThe augment_lags function takes a Pandas DataFrame or GroupBy object, a date column, a value column or list of value columns, and a lag or list of lags, and adds lagged versions of the value columns to the DataFrame."
  },
  {
    "objectID": "reference/augment_lags.html#parameters",
    "href": "reference/augment_lags.html#parameters",
    "title": "augment_lags",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nThe data parameter is the input DataFrame or DataFrameGroupBy object that you want to add lagged columns to.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that specifies the name of the column in the DataFrame that contains the dates. This column will be used to sort the data before adding the lagged values.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the column(s) in the DataFrame that you want to add lagged values for. It can be either a single column name (string) or a list of column names.\nrequired\n\n\nlags\nint or tuple or list\nThe lags parameter is an integer, tuple, or list that specifies the number of lagged values to add to the DataFrame. - If it is an integer, the function will add that number of lagged values for each column specified in the value_column parameter. - If it is a tuple, it will generate lags from the first to the second value (inclusive). - If it is a list, it will generate lags based on the values in the list.\n1"
  },
  {
    "objectID": "reference/augment_lags.html#returns",
    "href": "reference/augment_lags.html#returns",
    "title": "augment_lags",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame with lagged columns added to it."
  },
  {
    "objectID": "reference/augment_lags.html#examples",
    "href": "reference/augment_lags.html#examples",
    "title": "augment_lags",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndf = tk.load_dataset('m4_daily', parse_dates=['date'])\ndf\n\n\n\n\n\n\n\n\nid\ndate\nvalue\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n\n\n1\nD10\n2014-07-04\n2073.4\n\n\n2\nD10\n2014-07-05\n2048.7\n\n\n3\nD10\n2014-07-06\n2048.9\n\n\n4\nD10\n2014-07-07\n2006.4\n\n\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n\n\n9739\nD500\n2012-09-20\n9365.7\n\n\n9740\nD500\n2012-09-21\n9445.9\n\n\n9741\nD500\n2012-09-22\n9497.9\n\n\n9742\nD500\n2012-09-23\n9545.3\n\n\n\n\n9743 rows × 3 columns\n\n\n\n\n# Add a lagged value of 2 for each grouped time series\nlagged_df = (\n    df \n        .groupby('id')\n        .augment_lags(\n            date_column='date',\n            value_column='value',\n            lags=2\n        )\n)\nlagged_df\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_2\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2076.2\n\n\n3\nD10\n2014-07-06\n2048.9\n2073.4\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.7\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n9437.7\n\n\n9739\nD500\n2012-09-20\n9365.7\n9431.9\n\n\n9740\nD500\n2012-09-21\n9445.9\n9418.8\n\n\n9741\nD500\n2012-09-22\n9497.9\n9365.7\n\n\n9742\nD500\n2012-09-23\n9545.3\n9445.9\n\n\n\n\n9743 rows × 4 columns\n\n\n\n\n# Add 7 lagged values for a single time series\nlagged_df_single = (\n    df \n        .query('id == \"D10\"')\n        .augment_lags(\n            date_column='date',\n            value_column='value',\n            lags=(1, 7)\n        )\n)\nlagged_df_single\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_1\nvalue_lag_2\nvalue_lag_3\nvalue_lag_4\nvalue_lag_5\nvalue_lag_6\nvalue_lag_7\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nD10\n2014-07-06\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\nNaN\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.9\n2048.7\n2073.4\n2076.2\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n2542.0\n2534.2\n\n\n670\nD10\n2016-05-03\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n2542.0\n\n\n671\nD10\n2016-05-04\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n2585.8\n\n\n672\nD10\n2016-05-05\n2622.5\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n2579.9\n\n\n673\nD10\n2016-05-06\n2620.1\n2622.5\n2631.8\n2649.3\n2630.7\n2601.0\n2572.9\n2544.0\n\n\n\n\n674 rows × 10 columns\n\n\n\n\n# Add 2 lagged values, 2 and 4, for a single time series\nlagged_df_single_two = (\n    df \n        .query('id == \"D10\"')\n        .augment_lags(\n            date_column='date',\n            value_column='value',\n            lags=[2, 4]\n        )\n)\nlagged_df_single_two\n\n\n\n\n\n\n\n\nid\ndate\nvalue\nvalue_lag_2\nvalue_lag_4\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\nNaN\nNaN\n\n\n1\nD10\n2014-07-04\n2073.4\nNaN\nNaN\n\n\n2\nD10\n2014-07-05\n2048.7\n2076.2\nNaN\n\n\n3\nD10\n2014-07-06\n2048.9\n2073.4\nNaN\n\n\n4\nD10\n2014-07-07\n2006.4\n2048.7\n2076.2\n\n\n...\n...\n...\n...\n...\n...\n\n\n669\nD10\n2016-05-02\n2630.7\n2572.9\n2579.9\n\n\n670\nD10\n2016-05-03\n2649.3\n2601.0\n2544.0\n\n\n671\nD10\n2016-05-04\n2631.8\n2630.7\n2572.9\n\n\n672\nD10\n2016-05-05\n2622.5\n2649.3\n2601.0\n\n\n673\nD10\n2016-05-06\n2620.1\n2631.8\n2630.7\n\n\n\n\n674 rows × 5 columns"
  },
  {
    "objectID": "reference/floor_date.html",
    "href": "reference/floor_date.html",
    "title": "floor_date",
    "section": "",
    "text": "floor_date(idx, unit='D')\nRound a date down to the specified unit (e.g. Flooring).\nThe floor_date function takes a pandas Series of dates and returns a new Series with the dates rounded down to the specified unit."
  },
  {
    "objectID": "reference/floor_date.html#parameters",
    "href": "reference/floor_date.html#parameters",
    "title": "floor_date",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter is a pandas Series or pandas DatetimeIndex object that contains datetime values. It represents the dates that you want to round down.\nrequired\n\n\nunit\nstr\nThe unit parameter in the floor_date function is a string that specifies the time unit to which the dates in the idx series should be rounded down. It has a default value of “D”, which stands for day. Other possible values for the unit parameter could be\n'D'"
  },
  {
    "objectID": "reference/floor_date.html#returns",
    "href": "reference/floor_date.html#returns",
    "title": "floor_date",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe floor_date function returns a pandas Series object containing datetime64[ns] values."
  },
  {
    "objectID": "reference/floor_date.html#examples",
    "href": "reference/floor_date.html#examples",
    "title": "floor_date",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-01-10\", freq=\"1H\")\ndates\n\nDatetimeIndex(['2020-01-01 00:00:00', '2020-01-01 01:00:00',\n               '2020-01-01 02:00:00', '2020-01-01 03:00:00',\n               '2020-01-01 04:00:00', '2020-01-01 05:00:00',\n               '2020-01-01 06:00:00', '2020-01-01 07:00:00',\n               '2020-01-01 08:00:00', '2020-01-01 09:00:00',\n               ...\n               '2020-01-09 15:00:00', '2020-01-09 16:00:00',\n               '2020-01-09 17:00:00', '2020-01-09 18:00:00',\n               '2020-01-09 19:00:00', '2020-01-09 20:00:00',\n               '2020-01-09 21:00:00', '2020-01-09 22:00:00',\n               '2020-01-09 23:00:00', '2020-01-10 00:00:00'],\n              dtype='datetime64[ns]', length=217, freq='H')\n\n\n\n# Works on DateTimeIndex\ntk.floor_date(dates, unit=\"D\")\n\n0     2020-01-01\n1     2020-01-01\n2     2020-01-01\n3     2020-01-01\n4     2020-01-01\n         ...    \n212   2020-01-09\n213   2020-01-09\n214   2020-01-09\n215   2020-01-09\n216   2020-01-10\nName: idx, Length: 217, dtype: datetime64[ns]\n\n\n\n# Works on Pandas Series\ndates.to_series().floor_date(unit=\"D\")\n\n2020-01-01 00:00:00   2020-01-01\n2020-01-01 01:00:00   2020-01-01\n2020-01-01 02:00:00   2020-01-01\n2020-01-01 03:00:00   2020-01-01\n2020-01-01 04:00:00   2020-01-01\n                         ...    \n2020-01-09 20:00:00   2020-01-09\n2020-01-09 21:00:00   2020-01-09\n2020-01-09 22:00:00   2020-01-09\n2020-01-09 23:00:00   2020-01-09\n2020-01-10 00:00:00   2020-01-10\nFreq: H, Length: 217, dtype: datetime64[ns]"
  }
]