[
  {
    "objectID": "reference/floor_date.html",
    "href": "reference/floor_date.html",
    "title": "floor_date",
    "section": "",
    "text": "floor_date(idx, unit='D')\nThe floor_date function takes a pandas Series of dates and returns a new Series with the dates rounded down to the specified unit."
  },
  {
    "objectID": "reference/floor_date.html#parameters",
    "href": "reference/floor_date.html#parameters",
    "title": "floor_date",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter is a pandas Series or pandas DatetimeIndex object that contains datetime values. It represents the dates that you want to round down.\nrequired\n\n\nunit\nstr\nThe unit parameter in the floor_date function is a string that specifies the time unit to which the dates in the idx series should be rounded down. It has a default value of “D”, which stands for day. Other possible values for the unit parameter could be\n'D'"
  },
  {
    "objectID": "reference/floor_date.html#returns",
    "href": "reference/floor_date.html#returns",
    "title": "floor_date",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe floor_date function returns a pandas Series object."
  },
  {
    "objectID": "reference/floor_date.html#examples",
    "href": "reference/floor_date.html#examples",
    "title": "floor_date",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-01-10\", freq=\"1H\")\ndates\n\nDatetimeIndex(['2020-01-01 00:00:00', '2020-01-01 01:00:00',\n               '2020-01-01 02:00:00', '2020-01-01 03:00:00',\n               '2020-01-01 04:00:00', '2020-01-01 05:00:00',\n               '2020-01-01 06:00:00', '2020-01-01 07:00:00',\n               '2020-01-01 08:00:00', '2020-01-01 09:00:00',\n               ...\n               '2020-01-09 15:00:00', '2020-01-09 16:00:00',\n               '2020-01-09 17:00:00', '2020-01-09 18:00:00',\n               '2020-01-09 19:00:00', '2020-01-09 20:00:00',\n               '2020-01-09 21:00:00', '2020-01-09 22:00:00',\n               '2020-01-09 23:00:00', '2020-01-10 00:00:00'],\n              dtype='datetime64[ns]', length=217, freq='H')\n\n\n\n# Works on DateTimeIndex\ntk.floor_date(dates, unit=\"D\")\n\n0     2020-01-01\n1     2020-01-01\n2     2020-01-01\n3     2020-01-01\n4     2020-01-01\n         ...    \n212   2020-01-09\n213   2020-01-09\n214   2020-01-09\n215   2020-01-09\n216   2020-01-10\nName: idx, Length: 217, dtype: datetime64[ns]\n\n\n\n# Works on Pandas Series\ndates.to_series().floor_date(unit=\"D\")\n\n2020-01-01 00:00:00   2020-01-01\n2020-01-01 01:00:00   2020-01-01\n2020-01-01 02:00:00   2020-01-01\n2020-01-01 03:00:00   2020-01-01\n2020-01-01 04:00:00   2020-01-01\n                         ...    \n2020-01-09 20:00:00   2020-01-09\n2020-01-09 21:00:00   2020-01-09\n2020-01-09 22:00:00   2020-01-09\n2020-01-09 23:00:00   2020-01-09\n2020-01-10 00:00:00   2020-01-10\nFreq: H, Length: 217, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html",
    "href": "reference/augment_timeseries_signature.html",
    "title": "augment_timeseries_signature",
    "section": "",
    "text": "augment_timeseries_signature(data, date_column)\nThe function augment_timeseries_signature takes a DataFrame and a date column as input and returns the original DataFrame with the 29 different date and time based features added as new columns:"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#parameters",
    "href": "reference/augment_timeseries_signature.html#parameters",
    "title": "augment_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe data parameter is a pandas DataFrame that contains the time series data.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that represents the name of the date column in the data DataFrame.\nrequired"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#returns",
    "href": "reference/augment_timeseries_signature.html#returns",
    "title": "augment_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA pandas DataFrame that is the concatenation of the original data DataFrame and the ts_signature_df DataFrame."
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#examples",
    "href": "reference/augment_timeseries_signature.html#examples",
    "title": "augment_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\n# Adds 29 new time series features as columns to the original DataFrame\n( \n    tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n        .augment_timeseries_signature(date_column = 'order_date')\n        .head()\n)\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\norder_date_leapyear\norder_date_half\norder_date_quarter\norder_date_quarteryear\norder_date_quarterstart\norder_date_quarterend\norder_date_month\norder_date_month_lbl\norder_date_monthstart\norder_date_monthend\norder_date_yweek\norder_date_mweek\norder_date_wday\norder_date_wday_lbl\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/summarize_by_time.html",
    "href": "reference/summarize_by_time.html",
    "title": "summarize_by_time",
    "section": "",
    "text": "summarize_by_time"
  },
  {
    "objectID": "reference/summarize_by_time.html#functions",
    "href": "reference/summarize_by_time.html#functions",
    "title": "summarize_by_time",
    "section": "Functions",
    "text": "Functions\n\n\n\nName\nDescription\n\n\n\n\nsummarize_by_time\nThe summarize_by_time function aggregates data by a specified time period and one or more numeric columns, allowing for grouping and customization of the aggregation.\n\n\n\n\nsummarize_by_time\nsummarize_by_time.summarize_by_time(data, date_column, value_column, groups=None, rule='D', agg_func='sum', kind='timestamp', wide_format=False, fillna=0, flatten_column_names=True, reset_index=True, *args, **kwargs)\nThe summarize_by_time function aggregates data by a specified time period and one or more numeric columns, allowing for grouping and customization of the aggregation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nA pandas DataFrame or a pandas GroupBy object. This is the data that you want to summarize by time.\nrequired\n\n\ndate_column\nstr\nThe name of the column in the data frame that contains the dates or timestamps to be aggregated by. This column must be of type datetime64.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the name of one or more columns in the DataFrame that you want to aggregate by. It can be either a string representing a single column name, or a list of strings representing multiple column names.\nrequired\n\n\ngroups\nstr or list or None\nThe groups parameter is an optional parameter that allows you to specify one or more column names representing groups to aggregate by. If you want to aggregate the data by specific groups, you can pass the column names as a string or a list to the groups parameter. If you want to aggregate the data without grouping, you can set groups to None. The default value is None.\nNone\n\n\nrule\nstr\nThe rule parameter specifies the frequency at which the data should be aggregated. It accepts a string representing a pandas frequency offset, such as “D” for daily or “MS” for month start. The default value is “D”, which means the data will be aggregated on a daily basis.\n'D'\n\n\nagg_func\nlist\nThe agg_func parameter is used to specify one or more aggregating functions to apply to the value column(s) during the summarization process. It can be a single function or a list of functions. The default value is \"sum\", which represents the sum function.\n'sum'\n\n\nkind\nstr\nThe kind parameter specifies whether the time series data is represented as a “timestamp” or a “period”. If kind is set to “timestamp”, the data is treated as a continuous time series with specific timestamps. If kind is set to “period”, the data is treated as a discrete time series with specific periods. The default value is “timestamp”.\n'timestamp'\n\n\nwide_format\nbool\nA boolean parameter that determines whether the output should be in “wide” or “long” format. If set to True, the output will be in wide format, where each group is represented by a separate column. If set to False, the output will be in long format, where each group is represented by a separate row. The default value is False.\nFalse\n\n\nfillna\nint\nThe fillna parameter is used to specify the value to fill missing data with. By default, it is set to 0. If you want to keep missing values as NaN, you can use np.nan as the value for fillna.\n0\n\n\nflatten_column_names\nbool\nA boolean parameter that determines whether or not to flatten the multiindex column names. If set to True, the multiindex column names will be flattened. If set to False, the multiindex column names will be preserved. The default value is True.\nTrue\n\n\nreset_index\nbool\nA boolean parameter that determines whether or not to reset the index of the resulting DataFrame. If set to True, the index will be reset to the default integer index. If set to False, the index will not be reset. The default value is True.\nTrue\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\na Pandas DataFrame that is summarized by time.\n\n\n\n\n\n\nExamples\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Summarize by time with a DataFrame object\n( \n    df \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price',\n            groups       = \"category_2\",\n            rule         = \"MS\",\n            agg_func     = ['mean', 'sum']\n        )\n)\n\n\n\n\n\n\n\n\ncategory_2\norder_date\ntotal_price_mean\ntotal_price_sum\n\n\n\n\n0\nCross Country Race\n2011-01-01\n7561.052632\n143660\n\n\n1\nCross Country Race\n2011-02-01\n5406.666667\n324400\n\n\n2\nCross Country Race\n2011-03-01\n7888.888889\n142000\n\n\n3\nCross Country Race\n2011-04-01\n6475.064935\n498580\n\n\n4\nCross Country Race\n2011-05-01\n6676.060606\n220310\n\n\n...\n...\n...\n...\n...\n\n\n102\nTriathalon\n2011-08-01\n5512.500000\n44100\n\n\n103\nTriathalon\n2011-09-01\n3143.750000\n25150\n\n\n104\nTriathalon\n2011-10-01\n5775.000000\n57750\n\n\n105\nTriathalon\n2011-11-01\n4892.307692\n63600\n\n\n106\nTriathalon\n2011-12-01\n4992.857143\n34950\n\n\n\n\n107 rows × 4 columns\n\n\n\n\n# Summarize by time with a GroupBy object\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            rule         = 'MS',\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065"
  },
  {
    "objectID": "reference/get_available_datasets.html",
    "href": "reference/get_available_datasets.html",
    "title": "get_available_datasets",
    "section": "",
    "text": "datasets.get_datasets.get_available_datasets()\nThe get_available_datasets function returns a sorted list of available dataset names from the timetk.datasets module."
  },
  {
    "objectID": "reference/get_available_datasets.html#returns",
    "href": "reference/get_available_datasets.html#returns",
    "title": "get_available_datasets",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function get_available_datasets returns a sorted list of available dataset names from the timetk.datasets module."
  },
  {
    "objectID": "reference/get_available_datasets.html#examples",
    "href": "reference/get_available_datasets.html#examples",
    "title": "get_available_datasets",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\n\ntk.get_available_datasets()\n\n['bike_sales_sample',\n 'bike_sharing_daily',\n 'm4_daily',\n 'm4_hourly',\n 'm4_monthly',\n 'm4_quarterly',\n 'm4_weekly',\n 'm4_yearly',\n 'taylor_30_min',\n 'walmart_sales_weekly',\n 'wikipedia_traffic_daily']"
  },
  {
    "objectID": "getting-started/02_quick_start.html",
    "href": "getting-started/02_quick_start.html",
    "title": "Quick Start",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "href": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "title": "Quick Start",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        rule         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "getting-started/02_quick_start.html#more-coming-soon",
    "href": "getting-started/02_quick_start.html#more-coming-soon",
    "title": "Quick Start",
    "section": "More coming soon…",
    "text": "More coming soon…\nThere’s a lot more coming in timetk for Python. You can check out our Project Roadmap here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "timetk for Python ",
    "section": "",
    "text": "The Time Series Toolkit for Python\nTimetk’s Mission: To make time series analysis easier, faster, and more enjoyable in Python."
  },
  {
    "objectID": "index.html#quick-start-a-monthly-sales-analysis",
    "href": "index.html#quick-start-a-monthly-sales-analysis",
    "title": "timetk for Python ",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        rule         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "timetk for Python ",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npip install poetry\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "getting-started/01_installation.html",
    "href": "getting-started/01_installation.html",
    "title": "Install",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/01_installation.html#installation",
    "href": "getting-started/01_installation.html#installation",
    "title": "Install",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npip install poetry\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that manipulate DataFrames.\n\n\n\nsummarize_by_time\n\n\n\naugment_timeseries_signature\nThe function augment_timeseries_signature takes a DataFrame and a date column as input and returns the original DataFrame with the 29 different date and time based features added as new columns:\n\n\n\n\n\n\nTime series functions that manipulate Pandas Series.\n\n\n\nget_timeseries_signature\nThe function tk_get_timeseries_signature engineers 29 different date and time based features from a single datetime index idx:\n\n\nfloor_date\nThe floor_date function takes a pandas Series of dates and returns a new Series with the dates\n\n\n\n\n\n\nPractice timetk with our complementary time series datasets.\n\n\n\nget_available_datasets\nThe get_available_datasets function returns a sorted list of available dataset names from the timetk.datasets module.\n\n\nload_dataset\nThe load_dataset function is used to load various time series datasets by name, with options to print the available datasets and pass additional arguments to pandas.read_csv. The available datasets are:"
  },
  {
    "objectID": "reference/index.html#time-series-for-pandas-dataframes",
    "href": "reference/index.html#time-series-for-pandas-dataframes",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that manipulate DataFrames.\n\n\n\nsummarize_by_time\n\n\n\naugment_timeseries_signature\nThe function augment_timeseries_signature takes a DataFrame and a date column as input and returns the original DataFrame with the 29 different date and time based features added as new columns:"
  },
  {
    "objectID": "reference/index.html#time-series-for-pandas-series",
    "href": "reference/index.html#time-series-for-pandas-series",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that manipulate Pandas Series.\n\n\n\nget_timeseries_signature\nThe function tk_get_timeseries_signature engineers 29 different date and time based features from a single datetime index idx:\n\n\nfloor_date\nThe floor_date function takes a pandas Series of dates and returns a new Series with the dates"
  },
  {
    "objectID": "reference/index.html#datasets",
    "href": "reference/index.html#datasets",
    "title": "Function reference",
    "section": "",
    "text": "Practice timetk with our complementary time series datasets.\n\n\n\nget_available_datasets\nThe get_available_datasets function returns a sorted list of available dataset names from the timetk.datasets module.\n\n\nload_dataset\nThe load_dataset function is used to load various time series datasets by name, with options to print the available datasets and pass additional arguments to pandas.read_csv. The available datasets are:"
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "datasets.get_datasets.load_dataset(name='m4_daily', verbose=False, **kwargs)\nThe load_dataset function is used to load various time series datasets by name, with options to print the available datasets and pass additional arguments to pandas.read_csv. The available datasets are:"
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name parameter is used to specify the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned in the function’s docstring.\n'm4_daily'\n\n\nverbose\nbool\nThe verbose parameter is a boolean flag that determines whether or not to print the names of the available datasets. If verbose is set to True, the function will print the names of the available datasets. If verbose is set to False, the function will not print anything.\nFalse\n\n\n**kwargs\n\nThe **kwargs parameter is used to pass additional arguments to pandas.read_csv.\n{}"
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe load_dataset function returns the requested dataset as a pandas DataFrame."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\n\n# Bike Sales Sample Dataset\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Taylor 30-Minute Dataset\ndf = tk.load_dataset('taylor_30_min', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2000-06-05 00:00:00+00:00\n22262\n\n\n1\n2000-06-05 00:30:00+00:00\n21756\n\n\n2\n2000-06-05 01:00:00+00:00\n22247\n\n\n3\n2000-06-05 01:30:00+00:00\n22759\n\n\n4\n2000-06-05 02:00:00+00:00\n22549\n\n\n...\n...\n...\n\n\n4027\n2000-08-27 21:30:00+00:00\n27946\n\n\n4028\n2000-08-27 22:00:00+00:00\n27133\n\n\n4029\n2000-08-27 22:30:00+00:00\n25996\n\n\n4030\n2000-08-27 23:00:00+00:00\n24610\n\n\n4031\n2000-08-27 23:30:00+00:00\n23132\n\n\n\n\n4032 rows × 2 columns"
  },
  {
    "objectID": "reference/get_timeseries_signature.html",
    "href": "reference/get_timeseries_signature.html",
    "title": "get_timeseries_signature",
    "section": "",
    "text": "get_timeseries_signature(idx)\nThe function tk_get_timeseries_signature engineers 29 different date and time based features from a single datetime index idx:"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#parameters",
    "href": "reference/get_timeseries_signature.html#parameters",
    "title": "get_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nidx is a pandas Series object containing datetime values. Alternatively a pd.DatetimeIndex can be passed.\nrequired"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#returns",
    "href": "reference/get_timeseries_signature.html#returns",
    "title": "get_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function tk_get_timeseries_signature returns a pandas DataFrame that contains 29 different date and time based features derived from a single datetime column."
  },
  {
    "objectID": "reference/get_timeseries_signature.html#examples",
    "href": "reference/get_timeseries_signature.html#examples",
    "title": "get_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\ndates = pd.date_range(start = '2019-01', end = '2019-03', freq = 'D')\n\n# Makes 29 new time series features from the dates\ntk.get_timeseries_signature(dates).head()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\nquarterend\nmonth\nmonth_lbl\nmonthstart\nmonthend\nyweek\nmweek\nwday\nwday_lbl\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1546300800\n2019\n2019\n1\n0\n0\n1\n1\n2019Q1\n1\n0\n1\nJanuary\n1\n0\n1\n1\n2\nTuesday\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1546387200\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n3\nWednesday\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1546473600\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n4\nThursday\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1546560000\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1546646400\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n6\nSaturday\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam"
  }
]