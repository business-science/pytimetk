[
  {
    "objectID": "reference/floor_date.html",
    "href": "reference/floor_date.html",
    "title": "floor_date",
    "section": "",
    "text": "floor_date(idx, unit='D')\nRound a date down to the specified unit (e.g. Flooring).\nThe floor_date function takes a pandas Series of dates and returns a new Series with the dates rounded down to the specified unit."
  },
  {
    "objectID": "reference/floor_date.html#parameters",
    "href": "reference/floor_date.html#parameters",
    "title": "floor_date",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe idx parameter is a pandas Series or pandas DatetimeIndex object that contains datetime values. It represents the dates that you want to round down.\nrequired\n\n\nunit\nstr\nThe unit parameter in the floor_date function is a string that specifies the time unit to which the dates in the idx series should be rounded down. It has a default value of “D”, which stands for day. Other possible values for the unit parameter could be\n'D'"
  },
  {
    "objectID": "reference/floor_date.html#returns",
    "href": "reference/floor_date.html#returns",
    "title": "floor_date",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe floor_date function returns a pandas Series object containing datetime64[ns] values."
  },
  {
    "objectID": "reference/floor_date.html#examples",
    "href": "reference/floor_date.html#examples",
    "title": "floor_date",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-01-10\", freq=\"1H\")\ndates\n\nDatetimeIndex(['2020-01-01 00:00:00', '2020-01-01 01:00:00',\n               '2020-01-01 02:00:00', '2020-01-01 03:00:00',\n               '2020-01-01 04:00:00', '2020-01-01 05:00:00',\n               '2020-01-01 06:00:00', '2020-01-01 07:00:00',\n               '2020-01-01 08:00:00', '2020-01-01 09:00:00',\n               ...\n               '2020-01-09 15:00:00', '2020-01-09 16:00:00',\n               '2020-01-09 17:00:00', '2020-01-09 18:00:00',\n               '2020-01-09 19:00:00', '2020-01-09 20:00:00',\n               '2020-01-09 21:00:00', '2020-01-09 22:00:00',\n               '2020-01-09 23:00:00', '2020-01-10 00:00:00'],\n              dtype='datetime64[ns]', length=217, freq='H')\n\n\n\n# Works on DateTimeIndex\ntk.floor_date(dates, unit=\"D\")\n\n0     2020-01-01\n1     2020-01-01\n2     2020-01-01\n3     2020-01-01\n4     2020-01-01\n         ...    \n212   2020-01-09\n213   2020-01-09\n214   2020-01-09\n215   2020-01-09\n216   2020-01-10\nName: idx, Length: 217, dtype: datetime64[ns]\n\n\n\n# Works on Pandas Series\ndates.to_series().floor_date(unit=\"D\")\n\n2020-01-01 00:00:00   2020-01-01\n2020-01-01 01:00:00   2020-01-01\n2020-01-01 02:00:00   2020-01-01\n2020-01-01 03:00:00   2020-01-01\n2020-01-01 04:00:00   2020-01-01\n                         ...    \n2020-01-09 20:00:00   2020-01-09\n2020-01-09 21:00:00   2020-01-09\n2020-01-09 22:00:00   2020-01-09\n2020-01-09 23:00:00   2020-01-09\n2020-01-10 00:00:00   2020-01-10\nFreq: H, Length: 217, dtype: datetime64[ns]"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html",
    "href": "reference/augment_timeseries_signature.html",
    "title": "augment_timeseries_signature",
    "section": "",
    "text": "augment_timeseries_signature(data, date_column)\nAdd 29 time series features to a DataFrame.\nThe function augment_timeseries_signature takes a DataFrame and a date column as input and returns the original DataFrame with the 29 different date and time based features added as new columns:"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#parameters",
    "href": "reference/augment_timeseries_signature.html#parameters",
    "title": "augment_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nThe data parameter is a pandas DataFrame that contains the time series data.\nrequired\n\n\ndate_column\nstr\nThe date_column parameter is a string that represents the name of the date column in the data DataFrame.\nrequired"
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#returns",
    "href": "reference/augment_timeseries_signature.html#returns",
    "title": "augment_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA pandas DataFrame that is the concatenation of the original data DataFrame and the ts_signature_df DataFrame."
  },
  {
    "objectID": "reference/augment_timeseries_signature.html#examples",
    "href": "reference/augment_timeseries_signature.html#examples",
    "title": "augment_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\n# Adds 29 new time series features as columns to the original DataFrame\n( \n    tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n        .augment_timeseries_signature(date_column = 'order_date')\n        .head()\n)\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\norder_date_index_num\norder_date_year\norder_date_year_iso\norder_date_yearstart\norder_date_yearend\norder_date_leapyear\norder_date_half\norder_date_quarter\norder_date_quarteryear\norder_date_quarterstart\norder_date_quarterend\norder_date_month\norder_date_month_lbl\norder_date_monthstart\norder_date_monthend\norder_date_yweek\norder_date_mweek\norder_date_wday\norder_date_wday_lbl\norder_date_mday\norder_date_qday\norder_date_yday\norder_date_weekend\norder_date_hour\norder_date_minute\norder_date_second\norder_date_msecond\norder_date_nsecond\norder_date_am_pm\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n1294358400\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n7\n7\n7\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n1294617600\n2011\n2011\n0\n0\n0\n1\n1\n2011Q1\n0\n0\n1\nJanuary\n0\n0\n2\n2\n1\nMonday\n10\n10\n10\n0\n0\n0\n0\n0\n0\nam"
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "datasets.get_datasets.load_dataset(name='m4_daily', verbose=False, **kwargs)\nLoad one of 11 Time Series Datasets.\nThe load_dataset function is used to load various time series datasets by name, with options to print the available datasets and pass additional arguments to pandas.read_csv. The available datasets are:\nThe datasets can be loaded with timetk.load_dataset(name), where name is the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned above."
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name parameter is used to specify the name of the dataset that you want to load. The default value is set to “m4_daily”, which is the M4 daily dataset. However, you can choose from a list of available datasets mentioned in the function’s docstring.\n'm4_daily'\n\n\nverbose\nbool\nThe verbose parameter is a boolean flag that determines whether or not to print the names of the available datasets. If verbose is set to True, the function will print the names of the available datasets. If verbose is set to False, the function will not print anything.\nFalse\n\n\n**kwargs\n\nThe **kwargs parameter is used to pass additional arguments to pandas.read_csv.\n{}"
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe load_dataset function returns the requested dataset as a pandas DataFrame."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\n\n# Bike Sales Sample Dataset\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Taylor 30-Minute Dataset\ndf = tk.load_dataset('taylor_30_min', parse_dates = ['date'])\n\ndf\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2000-06-05 00:00:00+00:00\n22262\n\n\n1\n2000-06-05 00:30:00+00:00\n21756\n\n\n2\n2000-06-05 01:00:00+00:00\n22247\n\n\n3\n2000-06-05 01:30:00+00:00\n22759\n\n\n4\n2000-06-05 02:00:00+00:00\n22549\n\n\n...\n...\n...\n\n\n4027\n2000-08-27 21:30:00+00:00\n27946\n\n\n4028\n2000-08-27 22:00:00+00:00\n27133\n\n\n4029\n2000-08-27 22:30:00+00:00\n25996\n\n\n4030\n2000-08-27 23:00:00+00:00\n24610\n\n\n4031\n2000-08-27 23:30:00+00:00\n23132\n\n\n\n\n4032 rows × 2 columns"
  },
  {
    "objectID": "reference/summarize_by_time.html",
    "href": "reference/summarize_by_time.html",
    "title": "summarize_by_time",
    "section": "",
    "text": "summarize_by_time(data, date_column, value_column, rule='D', agg_func='sum', kind='timestamp', wide_format=False, fillna=0, flatten_column_names=True, reset_index=True, *args, **kwargs)\nSummarize a DataFrame or GroupBy object by time.\nThe summarize_by_time function aggregates data by a specified time period and one or more numeric columns, allowing for grouping and customization of the time-based aggregation."
  },
  {
    "objectID": "reference/summarize_by_time.html#parameters",
    "href": "reference/summarize_by_time.html#parameters",
    "title": "summarize_by_time",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame or pd.core.groupby.generic.DataFrameGroupBy\nA pandas DataFrame or a pandas GroupBy object. This is the data that you want to summarize by time.\nrequired\n\n\ndate_column\nstr\nThe name of the column in the data frame that contains the dates or timestamps to be aggregated by. This column must be of type datetime64.\nrequired\n\n\nvalue_column\nstr or list\nThe value_column parameter is the name of one or more columns in the DataFrame that you want to aggregate by. It can be either a string representing a single column name, or a list of strings representing multiple column names.\nrequired\n\n\nrule\nstr\nThe rule parameter specifies the frequency at which the data should be aggregated. It accepts a string representing a pandas frequency offset, such as “D” for daily or “MS” for month start. The default value is “D”, which means the data will be aggregated on a daily basis.\n'D'\n\n\nagg_func\nlist\nThe agg_func parameter is used to specify one or more aggregating functions to apply to the value column(s) during the summarization process. It can be a single function or a list of functions. The default value is \"sum\", which represents the sum function.\n'sum'\n\n\nkind\nstr\nThe kind parameter specifies whether the time series data is represented as a “timestamp” or a “period”. If kind is set to “timestamp”, the data is treated as a continuous time series with specific timestamps. If kind is set to “period”, the data is treated as a discrete time series with specific periods. The default value is “timestamp”.\n'timestamp'\n\n\nwide_format\nbool\nA boolean parameter that determines whether the output should be in “wide” or “long” format. If set to True, the output will be in wide format, where each group is represented by a separate column. If set to False, the output will be in long format, where each group is represented by a separate row. The default value is False.\nFalse\n\n\nfillna\nint\nThe fillna parameter is used to specify the value to fill missing data with. By default, it is set to 0. If you want to keep missing values as NaN, you can use np.nan as the value for fillna.\n0\n\n\nflatten_column_names\nbool\nA boolean parameter that determines whether or not to flatten the multiindex column names. If set to True, the multiindex column names will be flattened. If set to False, the multiindex column names will be preserved. The default value is True.\nTrue\n\n\nreset_index\nbool\nA boolean parameter that determines whether or not to reset the index of the resulting DataFrame. If set to True, the index will be reset to the default integer index. If set to False, the index will not be reset. The default value is True.\nTrue"
  },
  {
    "objectID": "reference/summarize_by_time.html#returns",
    "href": "reference/summarize_by_time.html#returns",
    "title": "summarize_by_time",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nA Pandas DataFrame that is summarized by time."
  },
  {
    "objectID": "reference/summarize_by_time.html#examples",
    "href": "reference/summarize_by_time.html#examples",
    "title": "summarize_by_time",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample', parse_dates = ['order_date'])\n\ndf\n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n# Summarize by time with a DataFrame object\n( \n    df \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price',\n            rule         = \"MS\",\n            agg_func     = ['mean', 'sum']\n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_mean\ntotal_price_sum\n\n\n\n\n0\n2011-01-01\n4600.142857\n483015\n\n\n1\n2011-02-01\n4611.408730\n1162075\n\n\n2\n2011-03-01\n5196.653543\n659975\n\n\n3\n2011-04-01\n4533.846154\n1827140\n\n\n4\n2011-05-01\n4097.912621\n844170\n\n\n5\n2011-06-01\n4544.839228\n1413445\n\n\n6\n2011-07-01\n4976.791667\n1194430\n\n\n7\n2011-08-01\n4961.970803\n679790\n\n\n8\n2011-09-01\n4682.298851\n814720\n\n\n9\n2011-10-01\n3930.053476\n734920\n\n\n10\n2011-11-01\n4768.175355\n1006085\n\n\n11\n2011-12-01\n4186.902655\n473120\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Long Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            rule         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = False, \n        )\n)\n\n\n\n\n\n\n\n\ncategory_1\norder_date\ntotal_price\n\n\n\n\n0\nMountain\n2011-01-01\n221490\n\n\n1\nMountain\n2011-02-01\n660555\n\n\n2\nMountain\n2011-03-01\n358855\n\n\n3\nMountain\n2011-04-01\n1075975\n\n\n4\nMountain\n2011-05-01\n450440\n\n\n5\nMountain\n2011-06-01\n723040\n\n\n6\nMountain\n2011-07-01\n767740\n\n\n7\nMountain\n2011-08-01\n361255\n\n\n8\nMountain\n2011-09-01\n401125\n\n\n9\nMountain\n2011-10-01\n377335\n\n\n10\nMountain\n2011-11-01\n549345\n\n\n11\nMountain\n2011-12-01\n276055\n\n\n12\nRoad\n2011-01-01\n261525\n\n\n13\nRoad\n2011-02-01\n501520\n\n\n14\nRoad\n2011-03-01\n301120\n\n\n15\nRoad\n2011-04-01\n751165\n\n\n16\nRoad\n2011-05-01\n393730\n\n\n17\nRoad\n2011-06-01\n690405\n\n\n18\nRoad\n2011-07-01\n426690\n\n\n19\nRoad\n2011-08-01\n318535\n\n\n20\nRoad\n2011-09-01\n413595\n\n\n21\nRoad\n2011-10-01\n357585\n\n\n22\nRoad\n2011-11-01\n456740\n\n\n23\nRoad\n2011-12-01\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            rule         = 'MS',\n            agg_func     = 'sum',\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n# Summarize by time with a GroupBy object and multiple summaries (Wide Format)\n(\n    df \n        .groupby('category_1') \n        .summarize_by_time(\n            date_column  = 'order_date', \n            value_column = 'total_price', \n            rule         = 'MS',\n            agg_func     = ['sum', 'mean', ('q25', lambda x: x.quantile(0.25)), ('q75', lambda x: x.quantile(0.75))],\n            wide_format  = True, \n        )\n)\n\n\n\n\n\n\n\n\norder_date\ntotal_price_sum_Mountain\ntotal_price_sum_Road\ntotal_price_mean_Mountain\ntotal_price_mean_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\ntotal_price_q25_Mountain\ntotal_price_q75_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n4922.000000\n4358.750000\n2060.0\n1950.0\n6070.0\n5605.0\n\n\n1\n2011-02-01\n660555\n501520\n4374.536424\n4965.544554\n2060.0\n1950.0\n5330.0\n5860.0\n\n\n2\n2011-03-01\n358855\n301120\n5882.868852\n4562.424242\n2130.0\n2240.0\n6390.0\n5875.0\n\n\n3\n2011-04-01\n1075975\n751165\n4890.795455\n4104.726776\n2060.0\n1950.0\n5970.0\n4800.0\n\n\n4\n2011-05-01\n450440\n393730\n4549.898990\n3679.719626\n2010.0\n1570.0\n6020.0\n3500.0\n\n\n5\n2011-06-01\n723040\n690405\n5021.111111\n4134.161677\n1950.0\n1840.0\n5647.5\n4500.0\n\n\n6\n2011-07-01\n767740\n426690\n5444.964539\n4310.000000\n2130.0\n1895.0\n6400.0\n5330.0\n\n\n7\n2011-08-01\n361255\n318535\n5734.206349\n4304.527027\n2235.0\n1950.0\n6400.0\n4987.5\n\n\n8\n2011-09-01\n401125\n413595\n5077.531646\n4353.631579\n1620.0\n1950.0\n6390.0\n5330.0\n\n\n9\n2011-10-01\n377335\n357585\n4439.235294\n3505.735294\n2160.0\n1750.0\n6070.0\n4260.0\n\n\n10\n2011-11-01\n549345\n456740\n5282.163462\n4268.598131\n2340.0\n1950.0\n7460.0\n4370.0\n\n\n11\n2011-12-01\n276055\n197065\n5208.584906\n3284.416667\n2060.0\n1652.5\n6400.0\n3200.0"
  },
  {
    "objectID": "reference/get_available_datasets.html",
    "href": "reference/get_available_datasets.html",
    "title": "get_available_datasets",
    "section": "",
    "text": "datasets.get_datasets.get_available_datasets()\nGet a list of 11 datasets that can be loaded with timetk.load_dataset.\nThe get_available_datasets function returns a sorted list of available dataset names from the timetk.datasets module. The available datasets are:"
  },
  {
    "objectID": "reference/get_available_datasets.html#returns",
    "href": "reference/get_available_datasets.html#returns",
    "title": "get_available_datasets",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nThe function get_available_datasets returns a sorted list of available dataset names from the timetk.datasets module."
  },
  {
    "objectID": "reference/get_available_datasets.html#examples",
    "href": "reference/get_available_datasets.html#examples",
    "title": "get_available_datasets",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\n\ntk.get_available_datasets()\n\n['bike_sales_sample',\n 'bike_sharing_daily',\n 'm4_daily',\n 'm4_hourly',\n 'm4_monthly',\n 'm4_quarterly',\n 'm4_weekly',\n 'm4_yearly',\n 'taylor_30_min',\n 'walmart_sales_weekly',\n 'wikipedia_traffic_daily']"
  },
  {
    "objectID": "guides/01_timetk_concepts.html",
    "href": "guides/01_timetk_concepts.html",
    "title": "Timetk Basics",
    "section": "",
    "text": "Timetk has one mission: To make time series analysis simpler, easier, and faster in Python. This goal requires some opinionated ways of treating time series in Python. We will conceptually lay out how timetk can help.\nLet’s first start with how to think about time series data conceptually. Time series data has 3 core properties."
  },
  {
    "objectID": "guides/01_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "href": "guides/01_timetk_concepts.html#type-1-pandas-dataframe-operations",
    "title": "Timetk Basics",
    "section": "2.1 Type 1: Pandas DataFrame Operations",
    "text": "2.1 Type 1: Pandas DataFrame Operations\nBefore we start using timetk, let’s make sure our data is set up properly.\n\nTimetk Data Format Compliance\n\n\n\n\n\n\n3 Core Properties Must Be Upheald\n\n\n\n\n\nA Timetk-Compliant Pandas DataFrame must have:\n\nTime Series Index: A Time Stamp column containing datetime64 values\nValue Column(s): The value column(s) containing float or int values\nGroup Column(s): Optionally for grouped time series analysis, one or more columns containg str or categorical values (shown as an object)\n\nIf these are NOT upheld, this will impact your ability to use timetk DataFrame operations.\n\n\n\n\n\n\n\n\n\nInspect the DataFrame\n\n\n\n\n\nUse Pandas info() method to check compliance.\n\n\n\nUsing pandas info() method, we can see that we have a compliant data frame with a date column containing datetime64 and a value column containing float64. For grouped analysis we have the id column containing object dtype.\n\n# Tip: Inspect for compliance with info()\nm4_daily_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9743 entries, 0 to 9742\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   id      9743 non-null   object        \n 1   date    9743 non-null   datetime64[ns]\n 2   value   9743 non-null   float64       \ndtypes: datetime64[ns](1), float64(1), object(1)\nmemory usage: 228.5+ KB\n\n\n\n\nGrouped Time Series Analysis with Summarize By Time\nFirst, inspect how the summarize_by_time function works by calling help().\n\n# Review the summarize_by_time documentation (output not shown)\nhelp(tk.summarize_by_time)\n\n\n\n\n\n\n\nHelp Doc Info: summarize_by_time()\n\n\n\n\n\n\nThe first parameter is data, indicating this is a DataFrame operation.\nThe Examples show different use cases for how to apply the function on a DataFrame\n\n\n\n\nLet’s test the summarize_by_time() DataFrame operation out using the grouped approach with method chaining. DataFrame operations can be used as Pandas methods with method-chaining, which allows us to more succinctly apply time series operations.\n\n# Grouped Summarize By Time with Method Chaining\ndf_summarized = (\n    m4_daily_df\n        .groupby('id')\n        .summarize_by_time(\n            date_column  = 'date',\n            value_column = 'value',\n            rule         = 'QS', # QS = Quarter Start\n            agg_func     = [\n                'mean', \n                'median', \n                'min',\n                ('q25', lambda x: np.quantile(x, 0.25)),\n                ('q75', lambda x: np.quantile(x, 0.75)),\n                'max',\n                ('range',lambda x: x.max() - x.min()),\n            ],\n        )\n)\n\ndf_summarized\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_q25\nvalue_q75\nvalue_max\nvalue_range\n\n\n\n\n0\nD10\n2014-07-01\n1960.078889\n1979.90\n1781.6\n1915.225\n2002.575\n2076.2\n294.6\n\n\n1\nD10\n2014-10-01\n2184.586957\n2154.05\n2022.8\n2125.075\n2274.150\n2344.9\n322.1\n\n\n2\nD10\n2015-01-01\n2309.830000\n2312.30\n2209.6\n2284.575\n2342.150\n2392.4\n182.8\n\n\n3\nD10\n2015-04-01\n2344.481319\n2333.00\n2185.1\n2301.750\n2391.000\n2499.8\n314.7\n\n\n4\nD10\n2015-07-01\n2156.754348\n2186.70\n1856.6\n1997.250\n2289.425\n2368.1\n511.5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n105\nD500\n2011-07-01\n9727.321739\n9745.55\n8964.5\n9534.125\n10003.900\n10463.9\n1499.4\n\n\n106\nD500\n2011-10-01\n8175.565217\n7897.00\n6755.0\n7669.875\n8592.575\n9860.0\n3105.0\n\n\n107\nD500\n2012-01-01\n8291.317582\n8412.60\n7471.5\n7814.800\n8677.850\n8980.7\n1509.2\n\n\n108\nD500\n2012-04-01\n8654.020879\n8471.10\n8245.6\n8389.850\n9017.250\n9349.2\n1103.6\n\n\n109\nD500\n2012-07-01\n8770.502353\n8690.50\n8348.1\n8604.400\n8846.000\n9545.3\n1197.2\n\n\n\n\n110 rows × 9 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: summarize_by_time()\n\n\n\n\n\n\nThe data must comply with the 3 core properties (date column, value column(s), and group column(s))\nThe aggregation functions were applied by combination of group (id) and resample (Quarter Start)\nThe result was a pandas DataFrame with group column, resampled date column, and summary values (mean, median, min, 25th-quantile, etc)\n\n\n\n\n\n\nAnother DataFrame Example: Creating 29 Engineered Features\nLet’s examine another DataFrame function, tk.augment_timeseries_signature(). Feel free to inspect the documentation with help(tk.augment_timeseries_signature).\n\n# Creating 29 engineered features from the date column\n# Not run: help(tk.augment_timeseries_signature)\ndf_augmented = (\n    m4_daily_df\n        .augment_timeseries_signature(date_column = 'date')\n)\n\ndf_augmented.head()\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_index_num\ndate_year\ndate_year_iso\ndate_yearstart\ndate_yearend\ndate_leapyear\ndate_half\n...\ndate_mday\ndate_qday\ndate_yday\ndate_weekend\ndate_hour\ndate_minute\ndate_second\ndate_msecond\ndate_nsecond\ndate_am_pm\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n1404345600\n2014\n2014\n0\n0\n0\n2\n...\n3\n3\n184\n0\n0\n0\n0\n0\n0\nam\n\n\n1\nD10\n2014-07-04\n2073.4\n1404432000\n2014\n2014\n0\n0\n0\n2\n...\n4\n4\n185\n0\n0\n0\n0\n0\n0\nam\n\n\n2\nD10\n2014-07-05\n2048.7\n1404518400\n2014\n2014\n0\n0\n0\n2\n...\n5\n5\n186\n0\n0\n0\n0\n0\n0\nam\n\n\n3\nD10\n2014-07-06\n2048.9\n1404604800\n2014\n2014\n0\n0\n0\n2\n...\n6\n6\n187\n1\n0\n0\n0\n0\n0\nam\n\n\n4\nD10\n2014-07-07\n2006.4\n1404691200\n2014\n2014\n0\n0\n0\n2\n...\n7\n7\n188\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\n\n\n\n\nKey Takeaways: augment_timeseries_signature()\n\n\n\n\n\n\nThe data must comply with the 1 of the 3 core properties (date column)\nThe result was a pandas DataFrame with 29 time series features that can be used for Machine Learning and Forecasting"
  },
  {
    "objectID": "guides/01_timetk_concepts.html#type-2-pandas-series-operations",
    "href": "guides/01_timetk_concepts.html#type-2-pandas-series-operations",
    "title": "Timetk Basics",
    "section": "2.2 Type 2: Pandas Series Operations",
    "text": "2.2 Type 2: Pandas Series Operations\nThe main difference between a DataFrame operation and a Series operation is that we are operating on an array of values from typically one of the following dtypes:\n\nTimestamps (datetime64)\nNumeric (float64 or int64)\n\nThe first argument of Series operations that operate on Timestamps will always be idx.\nLet’s take a look at one shall we? We’ll start with a common action: Making future time series from an existing time series with a regular frequency.\n\nThe Make Future Time Series Function\nSay we have a monthly sequence of timestamps. What if we want to create a forecast where we predict 12 months into the future? Well, we will need to create 12 future timestamps. Here’s how.\nFirst create a pd.date_range() with dates starting at the beginning of each month.\n\n# Make a monthly date range\ndates_dt = pd.date_range(\"2023-01\", \"2024-01\", freq=\"MS\")\ndates_dt\n\nDatetimeIndex(['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01',\n               '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01',\n               '2023-09-01', '2023-10-01', '2023-11-01', '2023-12-01',\n               '2024-01-01'],\n              dtype='datetime64[ns]', freq='MS')\n\n\nNext, use tk.make_future_timeseries() to create the next 12 timestamps in the sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas Series: Future Dates\nfuture_series = pd.Series(dates_dt).make_future_timeseries(12)\nfuture_series\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\n# DateTimeIndex: Future Dates\nfuture_dt = tk.make_future_timeseries(\n    idx      = dates_dt,\n    length_out = 12\n)\nfuture_dt\n\n0    2024-02-01\n1    2024-03-01\n2    2024-04-01\n3    2024-05-01\n4    2024-06-01\n5    2024-07-01\n6    2024-08-01\n7    2024-09-01\n8    2024-10-01\n9    2024-11-01\n10   2024-12-01\n11   2025-01-01\ndtype: datetime64[ns]\n\n\n\n\n\nWe can combine the actual and future timestamps into one combined timeseries.\n\n# Combining the 2 series and resetting the index\ncombined_timeseries = (\n    pd.concat(\n        [pd.Series(dates_dt), pd.Series(future_dt)],\n        axis=0\n    )\n        .reset_index(drop = True)\n)\n\ncombined_timeseries\n\n0    2023-01-01\n1    2023-02-01\n2    2023-03-01\n3    2023-04-01\n4    2023-05-01\n5    2023-06-01\n6    2023-07-01\n7    2023-08-01\n8    2023-09-01\n9    2023-10-01\n10   2023-11-01\n11   2023-12-01\n12   2024-01-01\n13   2024-02-01\n14   2024-03-01\n15   2024-04-01\n16   2024-05-01\n17   2024-06-01\n18   2024-07-01\n19   2024-08-01\n20   2024-09-01\n21   2024-10-01\n22   2024-11-01\n23   2024-12-01\n24   2025-01-01\ndtype: datetime64[ns]\n\n\nNext, we’ll take a look at how to go from an irregular time series to a regular time series.\n\n\nFlooring Dates\nAn example is tk.floor_date, which is used to round down dates. See help(tk.floor_date).\nFlooring dates is often used as part of a strategy to go from an irregular time series to regular by combining with an aggregation. Often summarize_by_time() is used (I’ll share why shortly). But conceptually, date flooring is the secret.\n\nWith FlooringWithout Flooring\n\n\n\n# Monthly flooring rounds dates down to 1st of the month\nm4_daily_df['date'].floor_date(unit = \"M\")\n\n0      2014-07-01\n1      2014-07-01\n2      2014-07-01\n3      2014-07-01\n4      2014-07-01\n          ...    \n9738   2012-09-01\n9739   2012-09-01\n9740   2012-09-01\n9741   2012-09-01\n9742   2012-09-01\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\n# Before Flooring\nm4_daily_df['date']\n\n0      2014-07-03\n1      2014-07-04\n2      2014-07-05\n3      2014-07-06\n4      2014-07-07\n          ...    \n9738   2012-09-19\n9739   2012-09-20\n9740   2012-09-21\n9741   2012-09-22\n9742   2012-09-23\nName: date, Length: 9743, dtype: datetime64[ns]\n\n\n\n\n\nThis “date flooring” operation can be useful for creating date groupings.\n\n# Adding a date group with floor_date()\ndates_grouped_by_month = (\n    m4_daily_df\n        .assign(date_group = lambda x: x['date'].floor_date(\"M\"))\n)\n\ndates_grouped_by_month\n\n\n\n\n\n\n\n\nid\ndate\nvalue\ndate_group\n\n\n\n\n0\nD10\n2014-07-03\n2076.2\n2014-07-01\n\n\n1\nD10\n2014-07-04\n2073.4\n2014-07-01\n\n\n2\nD10\n2014-07-05\n2048.7\n2014-07-01\n\n\n3\nD10\n2014-07-06\n2048.9\n2014-07-01\n\n\n4\nD10\n2014-07-07\n2006.4\n2014-07-01\n\n\n...\n...\n...\n...\n...\n\n\n9738\nD500\n2012-09-19\n9418.8\n2012-09-01\n\n\n9739\nD500\n2012-09-20\n9365.7\n2012-09-01\n\n\n9740\nD500\n2012-09-21\n9445.9\n2012-09-01\n\n\n9741\nD500\n2012-09-22\n9497.9\n2012-09-01\n\n\n9742\nD500\n2012-09-23\n9545.3\n2012-09-01\n\n\n\n\n9743 rows × 4 columns\n\n\n\nWe can then do grouped operations.\n\n# Example of a grouped operation with floored dates\nsummary_df = (\n    dates_grouped_by_month\n        .drop('date', axis=1) \\\n        .groupby(['id', 'date_group'])\n        .mean() \\\n        .reset_index()\n)\n\nsummary_df\n\n\n\n\n\n\n\n\nid\ndate_group\nvalue\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n\n\n1\nD10\n2014-08-01\n1985.548387\n\n\n2\nD10\n2014-09-01\n1926.593333\n\n\n3\nD10\n2014-10-01\n2100.077419\n\n\n4\nD10\n2014-11-01\n2155.326667\n\n\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n\n\n319\nD500\n2012-06-01\n9124.903333\n\n\n320\nD500\n2012-07-01\n8674.551613\n\n\n321\nD500\n2012-08-01\n8666.054839\n\n\n322\nD500\n2012-09-01\n9040.604348\n\n\n\n\n323 rows × 3 columns\n\n\n\nOf course for this operation, we can do it faster with summarize_by_time() (and it’s much more flexible).\n\n# Summarize by time is less code and more flexible\n(\n    m4_daily_df \n        .groupby('id')\n        .summarize_by_time(\n            'date', 'value', \n            rule = \"MS\",\n            agg_func = ['mean', 'median', 'min', 'max']\n        )\n)\n\n\n\n\n\n\n\n\nid\ndate\nvalue_mean\nvalue_median\nvalue_min\nvalue_max\n\n\n\n\n0\nD10\n2014-07-01\n1967.493103\n1978.80\n1876.0\n2076.2\n\n\n1\nD10\n2014-08-01\n1985.548387\n1995.60\n1914.7\n2027.5\n\n\n2\nD10\n2014-09-01\n1926.593333\n1920.95\n1781.6\n2023.5\n\n\n3\nD10\n2014-10-01\n2100.077419\n2107.60\n2022.8\n2154.9\n\n\n4\nD10\n2014-11-01\n2155.326667\n2149.30\n2083.5\n2245.4\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n318\nD500\n2012-05-01\n8407.096774\n8430.80\n8245.6\n8578.1\n\n\n319\nD500\n2012-06-01\n9124.903333\n9163.85\n8686.1\n9349.2\n\n\n320\nD500\n2012-07-01\n8674.551613\n8673.60\n8407.5\n9091.1\n\n\n321\nD500\n2012-08-01\n8666.054839\n8667.40\n8348.1\n8939.6\n\n\n322\nD500\n2012-09-01\n9040.604348\n9091.40\n8500.0\n9545.3\n\n\n\n\n323 rows × 6 columns\n\n\n\nAnd that’s the core idea behind timetk, writing less code and getting more.\nNext, let’s do one more function. The brother of augment_timeseries_signature()…\n\n\nThe Get Time Series Signature Function\nThis function takes a pandas Series or DateTimeIndex and returns a DataFrame containing the 29 engineered features.\nStart with either a DateTimeIndex…\n\ntimestamps_dt = pd.date_range(\"2023\", \"2024\", freq = \"D\")\ntimestamps_dt\n\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n               '2023-01-05', '2023-01-06', '2023-01-07', '2023-01-08',\n               '2023-01-09', '2023-01-10',\n               ...\n               '2023-12-23', '2023-12-24', '2023-12-25', '2023-12-26',\n               '2023-12-27', '2023-12-28', '2023-12-29', '2023-12-30',\n               '2023-12-31', '2024-01-01'],\n              dtype='datetime64[ns]', length=366, freq='D')\n\n\n… Or a Pandas Series.\n\ntimestamps_series = pd.Series(timestamps_dt)\ntimestamps_series\n\n0     2023-01-01\n1     2023-01-02\n2     2023-01-03\n3     2023-01-04\n4     2023-01-05\n         ...    \n361   2023-12-28\n362   2023-12-29\n363   2023-12-30\n364   2023-12-31\n365   2024-01-01\nLength: 366, dtype: datetime64[ns]\n\n\nAnd you can use the pandas Series function, tk.get_timeseries_signature() to create 29 features from the date sequence.\n\nPandas SeriesDateTimeIndex\n\n\n\n# Pandas series: get_timeseries_signature\ntimestamps_series.get_timeseries_signature()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns\n\n\n\n\n\n\n# DateTimeIndex: get_timeseries_signature\ntk.get_timeseries_signature(timestamps_dt)\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\n...\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1672531200\n2023\n2022\n1\n0\n0\n1\n1\n2023Q1\n1\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\nam\n\n\n1\n1672617600\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1672704000\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1672790400\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1672876800\n2023\n2023\n0\n0\n0\n1\n1\n2023Q1\n0\n...\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n1703721600\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n28\n89\n362\n0\n0\n0\n0\n0\n0\nam\n\n\n362\n1703808000\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n29\n90\n363\n0\n0\n0\n0\n0\n0\nam\n\n\n363\n1703894400\n2023\n2023\n0\n0\n0\n2\n4\n2023Q4\n0\n...\n30\n91\n364\n0\n0\n0\n0\n0\n0\nam\n\n\n364\n1703980800\n2023\n2023\n0\n1\n0\n2\n4\n2023Q4\n0\n...\n31\n92\n365\n1\n0\n0\n0\n0\n0\nam\n\n\n365\n1704067200\n2024\n2024\n1\n0\n1\n1\n1\n2024Q1\n1\n...\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n\n\n366 rows × 29 columns"
  },
  {
    "objectID": "getting-started/01_installation.html",
    "href": "getting-started/01_installation.html",
    "title": "Install",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/01_installation.html#installation",
    "href": "getting-started/01_installation.html#installation",
    "title": "Install",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npip install poetry\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "timetk for Python ",
    "section": "",
    "text": "The Time Series Toolkit for Python\nTimetk’s Mission: To make time series analysis easier, faster, and more enjoyable in Python."
  },
  {
    "objectID": "index.html#quick-start-a-monthly-sales-analysis",
    "href": "index.html#quick-start-a-monthly-sales-analysis",
    "title": "timetk for Python ",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        rule         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "timetk for Python ",
    "section": "Installation",
    "text": "Installation\nTo install timetk using Poetry, follow these steps:\n\n1. Prerequisites\nMake sure you have Python 3.9 or later installed on your system.\n\n\n2. Install Poetry\nTo install Poetry, you can use the official installer provided by Poetry. Do not use pip.\n\n\n3. Clone the Repository\nClone the timetk repository from GitHub:\ngit clone https://github.com/business-science/pytimetk\n\n\n4. Install Dependencies\nUse Poetry to install the package and its dependencies:\npip install poetry\nor you can create a virtualenv with poetry and install the dependencies\npoetry shell\npoetry install"
  },
  {
    "objectID": "getting-started/02_quick_start.html",
    "href": "getting-started/02_quick_start.html",
    "title": "Quick Start",
    "section": "",
    "text": "Under Development\n\n\n\n\n\nThis library is currently under development and is not intended for general usage yet. Functionality is experimental until release 0.1.0."
  },
  {
    "objectID": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "href": "getting-started/02_quick_start.html#quick-start-a-monthly-sales-analysis",
    "title": "Quick Start",
    "section": "Quick Start: A Monthly Sales Analysis",
    "text": "Quick Start: A Monthly Sales Analysis\nThis is a simple exercise to showcase the power of summarize_by_time():\n\nImport Libraries & Data\nFirst, import timetk as tk. This gets you access to the most important functions. Use tk.load_dataset() to load the “bike_sales_sample” dataset.\n\n\n\n\n\n\nAbout the Bike Sales Sample Dataset\n\n\n\n\n\nThis dataset contains “orderlines” for orders recieved. The order_date column contains timestamps. We can use this column to peform sales aggregations (e.g. total revenue).\n\n\n\n\nimport timetk as tk\nimport pandas as pd\n\ndf = tk.load_dataset('bike_sales_sample')\ndf['order_date'] = pd.to_datetime(df['order_date'])\n\ndf   \n\n\n\n\n\n\n\n\norder_id\norder_line\norder_date\nquantity\nprice\ntotal_price\nmodel\ncategory_1\ncategory_2\nframe_material\nbikeshop_name\ncity\nstate\n\n\n\n\n0\n1\n1\n2011-01-07\n1\n6070\n6070\nJekyll Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n1\n1\n2\n2011-01-07\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nIthaca Mountain Climbers\nIthaca\nNY\n\n\n2\n2\n1\n2011-01-10\n1\n2770\n2770\nBeast of the East 1\nMountain\nTrail\nAluminum\nKansas City 29ers\nKansas City\nKS\n\n\n3\n2\n2\n2011-01-10\n1\n5970\n5970\nTrigger Carbon 2\nMountain\nOver Mountain\nCarbon\nKansas City 29ers\nKansas City\nKS\n\n\n4\n3\n1\n2011-01-10\n1\n10660\n10660\nSupersix Evo Hi-Mod Team\nRoad\nElite Road\nCarbon\nLouisville Race Equipment\nLouisville\nKY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2461\n321\n3\n2011-12-22\n1\n1410\n1410\nCAAD8 105\nRoad\nElite Road\nAluminum\nMiami Race Equipment\nMiami\nFL\n\n\n2462\n322\n1\n2011-12-28\n1\n1250\n1250\nSynapse Disc Tiagra\nRoad\nEndurance Road\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2463\n322\n2\n2011-12-28\n1\n2660\n2660\nBad Habit 2\nMountain\nTrail\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2464\n322\n3\n2011-12-28\n1\n2340\n2340\nF-Si 1\nMountain\nCross Country Race\nAluminum\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n2465\n322\n4\n2011-12-28\n1\n5860\n5860\nSynapse Hi-Mod Dura Ace\nRoad\nEndurance Road\nCarbon\nPhoenix Bi-peds\nPhoenix\nAZ\n\n\n\n\n2466 rows × 13 columns\n\n\n\n\n\nUsing summarize_by_time() for a Sales Analysis\nYour company might be interested in sales patterns for various categories of bicycles. We can obtain a grouped monthly sales aggregation by category_1 in two lines of code:\n\nFirst use pandas’s groupby() method to group the DataFrame on category_1\nNext, use timetk’s summarize_by_time() method to apply the sum function my month start (“MS”) and use wide_format to return the dataframe in wide format.\n\nThe result is the total revenue for Mountain and Road bikes by month.\n\nsummary_category_1_df = df \\\n    .groupby(\"category_1\") \\\n    .summarize_by_time(\n        date_column  = 'order_date', \n        value_column = 'total_price',\n        rule         = \"MS\",\n        agg_func     = 'sum',\n        wide_format  = True\n    )\n\nsummary_category_1_df\n\n\n\n\n\n\n\n\norder_date\ntotal_price_Mountain\ntotal_price_Road\n\n\n\n\n0\n2011-01-01\n221490\n261525\n\n\n1\n2011-02-01\n660555\n501520\n\n\n2\n2011-03-01\n358855\n301120\n\n\n3\n2011-04-01\n1075975\n751165\n\n\n4\n2011-05-01\n450440\n393730\n\n\n5\n2011-06-01\n723040\n690405\n\n\n6\n2011-07-01\n767740\n426690\n\n\n7\n2011-08-01\n361255\n318535\n\n\n8\n2011-09-01\n401125\n413595\n\n\n9\n2011-10-01\n377335\n357585\n\n\n10\n2011-11-01\n549345\n456740\n\n\n11\n2011-12-01\n276055\n197065\n\n\n\n\n\n\n\n\n\nVisualizing Sales Patterns\n\n\n\n\n\n\nComing soon: plot_timeseries().\n\n\n\n\n\nWe are working on an even easier and more attractive plotting solution specifically designed for Time Series Analysis. It’s coming soon.\n\n\n\nWe can visualize with plotly.\n\nimport plotly.express as px\n\npx.line(\n    summary_category_1_df, \n    x = 'order_date', \n    y = ['total_price_Mountain', 'total_price_Road'],\n    template = \"plotly_dark\",    \n    title = \"Monthly Sales of Mountain and Road Bicycles\",\n    width = 900\n)"
  },
  {
    "objectID": "getting-started/02_quick_start.html#more-coming-soon",
    "href": "getting-started/02_quick_start.html#more-coming-soon",
    "title": "Quick Start",
    "section": "More coming soon…",
    "text": "More coming soon…\nThere’s a lot more coming in timetk for Python. You can check out our Project Roadmap here."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that manipulate DataFrames.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame.\n\n\n\n\n\n\nTime series functions that manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month.\n\n\n\n\n\n\nPractice timetk with 11 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 11 datasets that can be loaded with timetk.load_dataset.\n\n\nload_dataset\nLoad one of 11 Time Series Datasets."
  },
  {
    "objectID": "reference/index.html#time-series-for-pandas-dataframes",
    "href": "reference/index.html#time-series-for-pandas-dataframes",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that manipulate DataFrames.\n\n\n\nsummarize_by_time\nSummarize a DataFrame or GroupBy object by time.\n\n\naugment_timeseries_signature\nAdd 29 time series features to a DataFrame."
  },
  {
    "objectID": "reference/index.html#time-series-for-pandas-series",
    "href": "reference/index.html#time-series-for-pandas-series",
    "title": "Function reference",
    "section": "",
    "text": "Time series functions that manipulate Pandas Series.\n\n\n\nmake_future_timeseries\nMake future dates for a time series.\n\n\nget_timeseries_signature\nConvert a timestamp to a set of 29 time series features.\n\n\nfloor_date\nRound a date down to the specified unit (e.g. Flooring).\n\n\nweek_of_month\nThe “week_of_month” function calculates the week number of a given date within its month."
  },
  {
    "objectID": "reference/index.html#datasets",
    "href": "reference/index.html#datasets",
    "title": "Function reference",
    "section": "",
    "text": "Practice timetk with 11 complementary time series datasets.\n\n\n\nget_available_datasets\nGet a list of 11 datasets that can be loaded with timetk.load_dataset.\n\n\nload_dataset\nLoad one of 11 Time Series Datasets."
  },
  {
    "objectID": "reference/week_of_month.html",
    "href": "reference/week_of_month.html",
    "title": "week_of_month",
    "section": "",
    "text": "week_of_month(idx)\nThe “week_of_month” function calculates the week number of a given date within its month."
  },
  {
    "objectID": "reference/week_of_month.html#parameters",
    "href": "reference/week_of_month.html#parameters",
    "title": "week_of_month",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nThe parameter “idx” is a pandas Series object that represents a specific date for which you want to determine the week of the month.\nrequired"
  },
  {
    "objectID": "reference/week_of_month.html#returns",
    "href": "reference/week_of_month.html#returns",
    "title": "week_of_month",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nThe week of the month for a given date."
  },
  {
    "objectID": "reference/week_of_month.html#examples",
    "href": "reference/week_of_month.html#examples",
    "title": "week_of_month",
    "section": "Examples",
    "text": "Examples\n\nimport timetk as tk\nimport pandas as pd\n\ndates = pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"1D\")\ndates\n\nDatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10', '2020-01-11', '2020-01-12',\n               '2020-01-13', '2020-01-14', '2020-01-15', '2020-01-16',\n               '2020-01-17', '2020-01-18', '2020-01-19', '2020-01-20',\n               '2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24',\n               '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28',\n               '2020-01-29', '2020-01-30', '2020-01-31', '2020-02-01',\n               '2020-02-02', '2020-02-03', '2020-02-04', '2020-02-05',\n               '2020-02-06', '2020-02-07', '2020-02-08', '2020-02-09',\n               '2020-02-10', '2020-02-11', '2020-02-12', '2020-02-13',\n               '2020-02-14', '2020-02-15', '2020-02-16', '2020-02-17',\n               '2020-02-18', '2020-02-19', '2020-02-20', '2020-02-21',\n               '2020-02-22', '2020-02-23', '2020-02-24', '2020-02-25',\n               '2020-02-26', '2020-02-27', '2020-02-28'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\n# Works on DateTimeIndex\ntk.week_of_month(dates)\n\n0     1\n1     1\n2     1\n3     1\n4     1\n5     1\n6     1\n7     2\n8     2\n9     2\n10    2\n11    2\n12    2\n13    2\n14    3\n15    3\n16    3\n17    3\n18    3\n19    3\n20    3\n21    4\n22    4\n23    4\n24    4\n25    4\n26    4\n27    4\n28    5\n29    5\n30    5\n31    1\n32    1\n33    1\n34    1\n35    1\n36    1\n37    1\n38    2\n39    2\n40    2\n41    2\n42    2\n43    2\n44    2\n45    3\n46    3\n47    3\n48    3\n49    3\n50    3\n51    3\n52    4\n53    4\n54    4\n55    4\n56    4\n57    4\n58    4\nName: week_of_month, dtype: int32\n\n\n\n# Works on Pandas Series\ndates.to_series().week_of_month()\n\n2020-01-01    1\n2020-01-02    1\n2020-01-03    1\n2020-01-04    1\n2020-01-05    1\n2020-01-06    1\n2020-01-07    1\n2020-01-08    2\n2020-01-09    2\n2020-01-10    2\n2020-01-11    2\n2020-01-12    2\n2020-01-13    2\n2020-01-14    2\n2020-01-15    3\n2020-01-16    3\n2020-01-17    3\n2020-01-18    3\n2020-01-19    3\n2020-01-20    3\n2020-01-21    3\n2020-01-22    4\n2020-01-23    4\n2020-01-24    4\n2020-01-25    4\n2020-01-26    4\n2020-01-27    4\n2020-01-28    4\n2020-01-29    5\n2020-01-30    5\n2020-01-31    5\n2020-02-01    1\n2020-02-02    1\n2020-02-03    1\n2020-02-04    1\n2020-02-05    1\n2020-02-06    1\n2020-02-07    1\n2020-02-08    2\n2020-02-09    2\n2020-02-10    2\n2020-02-11    2\n2020-02-12    2\n2020-02-13    2\n2020-02-14    2\n2020-02-15    3\n2020-02-16    3\n2020-02-17    3\n2020-02-18    3\n2020-02-19    3\n2020-02-20    3\n2020-02-21    3\n2020-02-22    4\n2020-02-23    4\n2020-02-24    4\n2020-02-25    4\n2020-02-26    4\n2020-02-27    4\n2020-02-28    4\nFreq: D, Name: week_of_month, dtype: int32"
  },
  {
    "objectID": "reference/make_future_timeseries.html",
    "href": "reference/make_future_timeseries.html",
    "title": "make_future_timeseries",
    "section": "",
    "text": "make_future_timeseries(idx, length_out)\nMake future dates for a time series.\nThe function make_future_timeseries takes a pandas Series or DateTimeIndex and generates a future sequence of dates based on the frequency of the input series."
  },
  {
    "objectID": "reference/make_future_timeseries.html#parameters",
    "href": "reference/make_future_timeseries.html#parameters",
    "title": "make_future_timeseries",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DateTimeIndex\nThe idx parameter is the input time series data. It can be either a pandas Series or a pandas DateTimeIndex. It represents the existing dates in the time series.\nrequired\n\n\nlength_out\nint\nThe parameter length_out is an integer that represents the number of future dates to generate in the time series.\nrequired"
  },
  {
    "objectID": "reference/make_future_timeseries.html#returns",
    "href": "reference/make_future_timeseries.html#returns",
    "title": "make_future_timeseries",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\npd.Series\nA pandas Series object containing future dates."
  },
  {
    "objectID": "reference/make_future_timeseries.html#examples",
    "href": "reference/make_future_timeseries.html#examples",
    "title": "make_future_timeseries",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\ndates = pd.Series(pd.to_datetime(['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04']))\ndates\n\n0   2022-01-01\n1   2022-01-02\n2   2022-01-03\n3   2022-01-04\ndtype: datetime64[ns]\n\n\n\n# DateTimeIndex: Generate 5 future dates\nfuture_dates_dt = tk.make_future_timeseries(dates, 5)\nfuture_dates_dt\n\n0   2022-01-05\n1   2022-01-06\n2   2022-01-07\n3   2022-01-08\n4   2022-01-09\ndtype: datetime64[ns]\n\n\n\n# Series: Generate 5 future dates\npd.Series(future_dates_dt).make_future_timeseries(5)\n\n0   2022-01-10\n1   2022-01-11\n2   2022-01-12\n3   2022-01-13\n4   2022-01-14\ndtype: datetime64[ns]\n\n\n\ntimestamps = [\"2023-01-01 01:00\", \"2023-01-01 02:00\", \"2023-01-01 03:00\", \"2023-01-01 04:00\", \"2023-01-01 05:00\"]\n\ndates = pd.to_datetime(timestamps)\n\ntk.make_future_timeseries(dates, 5)\n\n0   2023-01-01 06:00:00\n1   2023-01-01 07:00:00\n2   2023-01-01 08:00:00\n3   2023-01-01 09:00:00\n4   2023-01-01 10:00:00\ndtype: datetime64[ns]\n\n\n\n# Monthly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-02-01\", \"2021-03-01\", \"2021-04-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2021-05-01\n1   2021-06-01\n2   2021-07-01\n3   2021-08-01\ndtype: datetime64[ns]\n\n\n\n# Quarterly Frequency: Generate 4 future dates\ndates = pd.to_datetime([\"2021-01-01\", \"2021-04-01\", \"2021-07-01\", \"2021-10-01\"])\n\ntk.make_future_timeseries(dates, 4)\n\n0   2022-01-01\n1   2022-04-01\n2   2022-07-01\n3   2022-10-01\ndtype: datetime64[ns]"
  },
  {
    "objectID": "reference/get_timeseries_signature.html",
    "href": "reference/get_timeseries_signature.html",
    "title": "get_timeseries_signature",
    "section": "",
    "text": "get_timeseries_signature(idx)\nConvert a timestamp to a set of 29 time series features.\nThe function tk_get_timeseries_signature engineers 29 different date and time based features from a single datetime index idx:"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#parameters",
    "href": "reference/get_timeseries_signature.html#parameters",
    "title": "get_timeseries_signature",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nidx\npd.Series or pd.DatetimeIndex\nidx is a pandas Series object containing datetime values. Alternatively a pd.DatetimeIndex can be passed.\nrequired"
  },
  {
    "objectID": "reference/get_timeseries_signature.html#returns",
    "href": "reference/get_timeseries_signature.html#returns",
    "title": "get_timeseries_signature",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nThe function tk_get_timeseries_signature returns a pandas DataFrame that contains 29 different date and time based features derived from a single datetime column."
  },
  {
    "objectID": "reference/get_timeseries_signature.html#examples",
    "href": "reference/get_timeseries_signature.html#examples",
    "title": "get_timeseries_signature",
    "section": "Examples",
    "text": "Examples\n\nimport pandas as pd\nimport timetk as tk\n\npd.set_option('display.max_columns', None)\n\ndates = pd.date_range(start = '2019-01', end = '2019-03', freq = 'D')\n\n# Makes 29 new time series features from the dates\ntk.get_timeseries_signature(dates).head()\n\n\n\n\n\n\n\n\nindex_num\nyear\nyear_iso\nyearstart\nyearend\nleapyear\nhalf\nquarter\nquarteryear\nquarterstart\nquarterend\nmonth\nmonth_lbl\nmonthstart\nmonthend\nyweek\nmweek\nwday\nwday_lbl\nmday\nqday\nyday\nweekend\nhour\nminute\nsecond\nmsecond\nnsecond\nam_pm\n\n\n\n\n0\n1546300800\n2019\n2019\n1\n0\n0\n1\n1\n2019Q1\n1\n0\n1\nJanuary\n1\n0\n1\n1\n2\nTuesday\n1\n1\n1\n0\n0\n0\n0\n0\n0\nam\n\n\n1\n1546387200\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n3\nWednesday\n2\n2\n2\n0\n0\n0\n0\n0\n0\nam\n\n\n2\n1546473600\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n4\nThursday\n3\n3\n3\n0\n0\n0\n0\n0\n0\nam\n\n\n3\n1546560000\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n5\nFriday\n4\n4\n4\n0\n0\n0\n0\n0\n0\nam\n\n\n4\n1546646400\n2019\n2019\n0\n0\n0\n1\n1\n2019Q1\n0\n0\n1\nJanuary\n0\n0\n1\n1\n6\nSaturday\n5\n5\n5\n0\n0\n0\n0\n0\n0\nam"
  }
]